{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alisavin/AgenticADMET/openr1/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-26 00:20:00 __init__.py:190] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 00:20:01,082\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvladvin111\u001b[0m (\u001b[33mvladvin-org\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from dataset import load_polaris_dataset, validate_dataset\n",
    "from train import get_dataset\n",
    "import numpy as np\n",
    "from latex2sympy2_extended import NormalizationConfig\n",
    "from math_verify import LatexExtractionConfig, parse\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    PreTrainedTokenizer\n",
    ")\n",
    "import torch\n",
    "from peft import PeftModel\n",
    "from peft import prepare_model_for_kbit_training\n",
    "from trl import ModelConfig\n",
    "from munch import Munch\n",
    "import json\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "import hashlib\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_CHAT_TEMPLATE = \"{% for message in messages %}\\n{% if message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'system' %}\\n{{ '<|system|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>\\n'  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '<|assistant|>' }}\\n{% endif %}\\n{% endfor %}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mae(completions, ground_truth=None, log_normalize=False, model_name=\"pred\", **kwargs):\n",
    "    \n",
    "    smiles = kwargs.get(\"smiles\")\n",
    "    num_generations = len(completions) / len(set(smiles))\n",
    "    \n",
    "    Path(f\"./test/completions/{model_name}_{num_generations}/\").mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    contents = [completion[0][\"content\"] for completion in completions]\n",
    "    rewards = []\n",
    "\n",
    "    solutions = kwargs.get(\"solution\") # Get solutions from kwargs\n",
    "    \n",
    "\n",
    "    if solutions is None:\n",
    "        return [0.5] * len(completions) # Return neutral reward if no solution\n",
    "    smiles2conts = defaultdict(list)\n",
    "    #TODO: not parsed empty list\n",
    "    #TODO: mae mean per group, take meadian and calculate\n",
    "    #TODO: make more generations, save separately\n",
    "    for content, gold_val, smiles_i in zip(contents, solutions, smiles):\n",
    "        \n",
    "        if gold_val is not None:  # Check if parsing was successful\n",
    "            # Parse the model's answer with relaxed normalization\n",
    "            answer_parsed = parse(\n",
    "                content,\n",
    "                extraction_config=[\n",
    "                    LatexExtractionConfig(\n",
    "                        normalization_config=NormalizationConfig(\n",
    "                            nits=False,\n",
    "                            malformed_operators=False,\n",
    "                            basic_latex=True,\n",
    "                            equations=True,\n",
    "                            boxed=\"all\",\n",
    "                            units=True,\n",
    "                        ),\n",
    "                        boxed_match_priority=0,\n",
    "                        try_extract_without_anchor=False,\n",
    "                    )\n",
    "                ],\n",
    "                extraction_mode=\"first_match\",\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                # if len(answer_parsed) == 0:\n",
    "                #     raise Exception(\"Parsed values is empty\")\n",
    "                answer_val = float(answer_parsed[0])\n",
    "                mae = np.mean(np.abs(gold_val - answer_val))\n",
    "                reward = np.clip(1-(1/6)*mae, 0, 1)\n",
    "                # print(content)\n",
    "                print(\"parsed correctly\", answer_val, gold_val)\n",
    "            except Exception as e:\n",
    "                answer_val = None\n",
    "                reward = 0\n",
    "                mae = None\n",
    "                if len(answer_parsed) > 0:\n",
    "                    print(e, answer_parsed)\n",
    "        else:\n",
    "            # If ground truth cannot be parsed, assign neutral reward (0.5)\n",
    "            reward = 0.5\n",
    "            answer_val = None\n",
    "            mae = None\n",
    "            print(\"Warning: Gold solution is None:\", gold_val)\n",
    "        if answer_val is not None:\n",
    "            post = \"parsed_\"\n",
    "        else:\n",
    "            post = \"\"\n",
    "        smiles_hash = hashlib.blake2b(smiles_i.encode('utf-8'), digest_size=4).hexdigest()\n",
    "        rewards.append(reward)\n",
    "        smiles2conts[smiles_hash].append({\"completion\": content, \n",
    "                       \"gold_val\": str(gold_val), \n",
    "                       \"answer_parsed\": str(answer_parsed), \n",
    "                       \"smiles\": smiles_i,\n",
    "                       \"answer_val\": answer_val,\n",
    "                       \"reward\": reward,\n",
    "                       \"mae\": mae\n",
    "                       }) \n",
    "    for k, v in smiles2conts.items():\n",
    "        with open(f\"./test/completions/{model_name}_{num_generations}/{post}{k}.json\", \"w\") as f:\n",
    "                answers_g = [v_i[\"answer_val\"] for v_i in v]\n",
    "                answers_g = [float(v_i) for v_i in answers_g if v_i is not None]\n",
    "                answer_median = np.median(answers_g)\n",
    "                mae_median = np.median(np.abs(float(v[0][\"gold_val\"]) - answer_median))\n",
    "                json.dump({\"completion\": [v_i[\"completion\"] for v_i in v], \n",
    "                        \"gold_val\": v[0][\"gold_val\"], \n",
    "                        \"answer_parsed\": [v_i[\"answer_parsed\"] for v_i in v], \n",
    "                        \"smiles\": v[0][\"smiles\"],\n",
    "                        \"answer_val\": [v_i[\"answer_val\"] for v_i in v],\n",
    "                        \"reward\": [v_i[\"reward\"] for v_i in v],\n",
    "                        \"mae\": [v_i[\"mae\"] for v_i in v],\n",
    "                        \"mae_median\": str(mae_median)\n",
    "                        }, f, indent=2)\n",
    "    return rewards\n",
    "\n",
    "def get_tokenizer(\n",
    "    model_args: ModelConfig, training_args, auto_set_chat_template: bool = True\n",
    ") -> PreTrainedTokenizer:\n",
    "    \"\"\"Get the tokenizer for the model.\"\"\"\n",
    "    # https://github.com/huggingface/open-r1/blob/eeca246b078457bc0f69ba2e8297b799df0e2bda/src/open_r1/utils/model_utils.py#L11\n",
    "    print(\"loading tokenizer\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        revision=model_args.model_revision,\n",
    "        trust_remote_code=False, # model_args.trust_remote_code\n",
    "    )\n",
    "    print(\"tokenizer loaded\")\n",
    "\n",
    "    if training_args.chat_template is not None:\n",
    "        tokenizer.chat_template = training_args.chat_template\n",
    "    elif auto_set_chat_template and tokenizer.get_chat_template() is None:\n",
    "        tokenizer.chat_template = DEFAULT_CHAT_TEMPLATE\n",
    "    print(\"chat template\")\n",
    "    # if processing_class is None:\n",
    "    #     processing_class = AutoTokenizer.from_pretrained(model.config._name_or_path, padding_side=\"left\")\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 206/206 [00:00<00:00, 11582.75 examples/s]\n",
      "Map: 100%|██████████| 41/41 [00:00<00:00, 6923.52 examples/s]\n",
      "Map: 100%|██████████| 48/48 [00:00<00:00, 7581.78 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 206\n",
      "Test set size: 41\n",
      "\n",
      "Validating train split:\n",
      "✓ All required fields present\n",
      "✓ Prompt format is correct\n",
      "\n",
      "Validating test split:\n",
      "✓ All required fields present\n",
      "✓ Prompt format is correct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = get_dataset(params=[\"LogD\"], subset_train=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alisavin/AgenticADMET/openr1/lib/python3.11/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/alisavin/AgenticADMET/openr1/lib/python3.11/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/alisavin/AgenticADMET/openr1/lib/python3.11/site-packages/transformers/training_args.py:2077: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer loaded\n",
      "chat template\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alisavin/AgenticADMET/openr1/lib/python3.11/site-packages/trl/trainer/grpo_trainer.py:390: UserWarning: The requested device cuda:0 is also being used for training. For higher throughput and to avoid out-of-memory errors, it is recommended to use a dedicated device for vLLM. If this is intentional, you may ignore this warning but should adjust `vllm_gpu_memory_utilization` accordingly.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-26 00:20:16 config.py:542] This model supports multiple tasks: {'reward', 'embed', 'classify', 'generate', 'score'}. Defaulting to 'generate'.\n",
      "INFO 02-26 00:20:16 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='deepseek-ai/DeepSeek-R1-Distill-Qwen-7B', speculative_config=None, tokenizer='deepseek-ai/DeepSeek-R1-Distill-Qwen-7B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda:0, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=deepseek-ai/DeepSeek-R1-Distill-Qwen-7B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 02-26 00:20:17 cuda.py:230] Using Flash Attention backend.\n",
      "INFO 02-26 00:20:17 model_runner.py:1110] Starting to load model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B...\n",
      "INFO 02-26 00:20:18 weight_utils.py:252] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.75s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:04<00:00,  2.19s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:04<00:00,  2.12s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-26 00:20:22 model_runner.py:1115] Loading model weights took 14.2712 GB\n",
      "INFO 02-26 00:20:24 worker.py:267] Memory profiling takes 0.91 seconds\n",
      "INFO 02-26 00:20:24 worker.py:267] the current vLLM instance can use total_gpu_memory (79.14GiB) x gpu_memory_utilization (0.25) = 19.78GiB\n",
      "INFO 02-26 00:20:24 worker.py:267] model weights take 14.27GiB; non_torch_memory takes 0.02GiB; PyTorch activation peak memory takes 1.40GiB; the rest of the memory reserved for KV Cache is 4.09GiB.\n",
      "INFO 02-26 00:20:24 executor_base.py:110] # CUDA blocks: 4791, # CPU blocks: 4681\n",
      "INFO 02-26 00:20:24 executor_base.py:115] Maximum concurrency for 2048 tokens per request: 37.43x\n",
      "INFO 02-26 00:20:28 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-26 00:20:50 model_runner.py:1562] Graph capturing finished in 23 secs, took 0.22 GiB\n",
      "INFO 02-26 00:20:50 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 27.80 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\"\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained()\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    trust_remote_code=True,    # if required\n",
    "    torch_dtype=torch.bfloat16,  # if you used bf16\n",
    "    device_map=\"auto\"           # or \"cuda:0\", depending on your environment\n",
    ")\n",
    "\n",
    "# # 2) Load LoRA adapter weights onto the base model\n",
    "model = PeftModel.from_pretrained(base_model, \"/home/alisavin/AgenticADMET/outputs/2025-02-25/20-38-55/checkpoint-60/\")\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "# model = base_model\n",
    "model.eval()\n",
    "\n",
    "model_args_i = Munch.fromDict({\n",
    "        \"model_name_or_path\": MODEL_NAME,\n",
    "        \"model_revision\": \"main\",\n",
    "        \"trust_remote_code\": False # TODO: everyboudy sets to True and default is True\n",
    "        })\n",
    "\n",
    "training_args_i = Munch.fromDict({\"chat_template\": \"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='') %}{%- for message in messages %}{%- if message['role'] == 'system' %}{% set ns.system_prompt = message['content'] %}{%- endif %}{%- endfor %}{{bos_token}}{{ns.system_prompt}}{%- for message in messages %}{%- if message['role'] == 'user' %}{%- set ns.is_tool = false -%}{{'<｜User｜>' + message['content']}}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is none %}{%- set ns.is_tool = false -%}{%- for tool in message['tool_calls']%}{%- if not ns.is_first %}{{'<｜Assistant｜><｜tool▁calls▁begin｜><｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\\\n' + '```json' + '\\\\n' + tool['function']['arguments'] + '\\\\n' + '```' + '<｜tool▁call▁end｜>'}}{%- set ns.is_first = true -%}{%- else %}{{'\\\\n' + '<｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\\\n' + '```json' + '\\\\n' + tool['function']['arguments'] + '\\\\n' + '```' + '<｜tool▁call▁end｜>'}}{{'<｜tool▁calls▁end｜><｜end▁of▁sentence｜>'}}{%- endif %}{%- endfor %}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is not none %}{%- if ns.is_tool %}{{'<｜tool▁outputs▁end｜>' + message['content'] + '<｜end▁of▁sentence｜>'}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message['content'] %}{{'<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>'}}{%- endif %}{%- endif %}{%- if message['role'] == 'tool' %}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{'<｜tool▁outputs▁begin｜><｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- set ns.is_output_first = false %}{%- else %}{{'\\\\n<｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{'<｜tool▁outputs▁end｜>'}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{'<｜Assistant｜>'}}{% endif %}\"})\n",
    "\n",
    "tokenizer = get_tokenizer(model_args_i, training_args_i)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# # 4) Generate text from a prompt\n",
    "# prompt = \"Explain the concept of molecular solubility in simple terms.\"\n",
    "\n",
    "# input = {\n",
    "#         \"ground_truth\": example[\"solution\"],\n",
    "#         \"prompt\": [\n",
    "#             {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "#             {\"role\": \"user\", \"content\": example[\"problem\"]},\n",
    "#         ],\n",
    "#     }\n",
    "# inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     outputs = model.generate(\n",
    "#         **inputs,\n",
    "#         max_new_tokens=128,\n",
    "#         do_sample=True,\n",
    "#         temperature=0.7,\n",
    "#         top_p=0.9\n",
    "#     )\n",
    "\n",
    "# generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "# print(\"Generated:\", generated_text)\n",
    "from train import GRPOTrainer2\n",
    "import os\n",
    "from trl import (\n",
    "    GRPOConfig, \n",
    "    GRPOTrainer,\n",
    "    get_peft_config\n",
    ")\n",
    "from dataclasses import field, dataclass\n",
    "\n",
    "def get_reward_functions(script_args, model_name):\n",
    "    \"\"\"\n",
    "    Returns a list of reward functions based on the script arguments.\n",
    "    \"\"\"\n",
    "    reward_funcs_list = []\n",
    "\n",
    "    fnc = partial(compute_mae, model_name=model_name)\n",
    "    fnc.__name__ = compute_mae.__name__\n",
    "    reward_funcs_registry = {\n",
    "        \"mae\": fnc,  # Assuming accuracy_reward is defined in previous steps\n",
    "    }\n",
    "\n",
    "    for func_name in script_args.reward_funcs:\n",
    "        if func_name not in reward_funcs_registry:\n",
    "            raise ValueError(f\"Reward function '{func_name}' not found in registry.\")\n",
    "        reward_funcs_list.append(reward_funcs_registry[func_name])\n",
    "\n",
    "    return reward_funcs_list\n",
    "\n",
    "@dataclass\n",
    "class GRPOScriptArguments:\n",
    "    \"\"\"\n",
    "    Script arguments for GRPO training, specifically related to reward functions.\n",
    "    \"\"\"\n",
    "\n",
    "    reward_funcs: list[str] = field(\n",
    "        default_factory=lambda: [\"mae\"], \n",
    "        metadata={\n",
    "            \"help\": \"List of reward functions. Possible values: 'accuracy', 'format', 'reasoning_steps', 'repetition_penalty'\"        },\n",
    "    )\n",
    "\n",
    "    repetition_n_grams: int = field(\n",
    "        default=3,\n",
    "        metadata={\"help\": \"Number of n-grams for repetition penalty reward\"},\n",
    "    )\n",
    "    repetition_max_penalty: float = field(\n",
    "        default=-0.1,\n",
    "        metadata={\"help\": \"Maximum (negative) penalty for for repetition penalty reward\"},\n",
    "    )\n",
    "    \n",
    "script_args = GRPOScriptArguments()\n",
    "\n",
    "reward_functions = get_reward_functions(script_args, model_name=\"tuned_v2\") #TODO: check trl they had someshere gpro example and used different rewards including lenght reward\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    logging_dir=\"./logs/wandb/\",\n",
    "    num_train_epochs=10,             # Total number of training epochs\n",
    "    per_device_train_batch_size=16,  # Batch size per device during training\n",
    "    per_device_eval_batch_size=16,   # Batch size for evaluation TODO: why it says this   File \"/home/alisavin/AgenticADMET/train.py\", line 534, in <module>\n",
    "    gradient_accumulation_steps=4,  # Accumulate gradients to simulate larger batch size\n",
    "    learning_rate=1e-6,            # Initial learning rate for AdamW optimizer\n",
    "    warmup_ratio=0.1,              # Linear warmup over warmup_ratio fraction of training steps\n",
    "    weight_decay=0.01,             # Apply weight decay to all layers except bias and LayerNorm weights\n",
    "    logging_steps=1,              # Log every X updates steps\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_first_step=True,\n",
    "    evaluation_strategy=\"epoch\",    # Evaluate every `eval_steps`\n",
    "    save_strategy=\"epoch\",      # Disables regular checkpoints\n",
    "    save_total_limit=1,      # Makes sure no checkpoints are kept\n",
    "    load_best_model_at_end=False,  # Disables saving the best model\n",
    "    dataloader_num_workers=4,      # Number of subprocesses to use for data loading\n",
    "    seed=42,                       # Random seed for reproducibility\n",
    "    bf16=True,                     # Use mixed precision BFP16 training #TODO: ??????\n",
    "    push_to_hub=False,             # Whether to push the final model to Hugging Face Hub\n",
    "    report_to=[\"wandb\"],              # Reporting to no one\n",
    "    run_name=\"test\",\n",
    "    do_train=False,\n",
    "    disable_tqdm=False,\n",
    "    gradient_checkpointing=True,   # Enable gradient checkpointing        \n",
    "    remove_unused_columns=False,\n",
    "    do_eval=False, #TODO: use\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False}, # TODO: use\n",
    "    lr_scheduler_type=\"cosine_with_min_lr\",\n",
    "    lr_scheduler_kwargs={\"min_lr_rate\": 0.1},\n",
    "    max_steps=-1, #TODO: change to -1\n",
    "    resume_from_checkpoint=\"/home/alisavin/AgenticADMET/outputs/2025-02-25/20-38-55/checkpoint-60/\"\n",
    ")\n",
    "\n",
    "grpo_config = GRPOConfig(\n",
    "    **training_args.to_dict(), # Convert TrainingArguments to dictionary and unpack\n",
    "    **{ \n",
    "    # REMOVED model_init_kwargs here \n",
    "    # We are passing the instantiated 'model' object, so GRPOTrainer doesn't need model_init_kwargs\n",
    "    },\n",
    "    num_generations=16, #TODO: 16\n",
    "    use_vllm=True, #TODO: use True\n",
    "    vllm_device=\"cuda:0\",\n",
    "    vllm_gpu_memory_utilization=0.25, # TODO: 0.25 0.7\n",
    "    vllm_max_model_len=2048, #TODO: 2048\n",
    "    max_prompt_length=800, #TODO: 800+\n",
    "    max_completion_length=1024, #TODO: 1024+ (better 2048/4048 and more)\n",
    "    temperature=0.7,\n",
    "    reward_weights=[1.0]\n",
    "    )\n",
    "\n",
    "model_args = ModelConfig(model_name_or_path=MODEL_NAME, use_peft=False)\n",
    "\n",
    "grpo_trainer = GRPOTrainer2(\n",
    "    model=model,                      # Our initialized Qwen model\n",
    "    reward_funcs=reward_functions,    # List of reward functions from previous step\n",
    "    args=grpo_config,                # GRPOConfig (created from TrainingArguments)\n",
    "    train_dataset=dataset['train'],   # Training dataset\n",
    "    eval_dataset=dataset['validation'],    # Evaluation dataset\n",
    "    processing_class=tokenizer, #TODO: check callback from config\n",
    "    # peft_config=get_peft_config(model_args) #TODO: check # label_names\n",
    "    peft_config=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active adapter: default\n",
      "Adapter names: dict_keys(['default'])\n"
     ]
    }
   ],
   "source": [
    "# Before creating the trainer\n",
    "if hasattr(model, \"active_adapter\"):\n",
    "    print(f\"Active adapter: {grpo_trainer.model.active_adapter}\")\n",
    "    print(\"Adapter names:\", grpo_trainer.model.peft_config.keys())\n",
    "else:\n",
    "    print(\"No adapter is active. This might be just the base model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models have different parameter structures\n",
      "Models are identical: False\n"
     ]
    }
   ],
   "source": [
    "def are_models_identical(model1, model2):\n",
    "    # Get named parameters for both models\n",
    "    params1 = dict(model1.named_parameters())\n",
    "    params2 = dict(model2.named_parameters())\n",
    "    \n",
    "    # Check if they have the same parameter names\n",
    "    if params1.keys() != params2.keys():\n",
    "        print(\"Models have different parameter structures\")\n",
    "        return False\n",
    "    \n",
    "    # Check if parameter values are identical\n",
    "    all_equal = True\n",
    "    for name in params1.keys():\n",
    "        if not torch.allclose(params1[name], params2[name], atol=1e-5):\n",
    "            print(f\"Parameters differ at: {name}\")\n",
    "            all_equal = False\n",
    "            # Optional: print some details about the differing parameters\n",
    "            print(f\"  Model1: min={params1[name].min()}, max={params1[name].max()}, mean={params1[name].mean()}\")\n",
    "            print(f\"  Model2: min={params2[name].min()}, max={params2[name].max()}, mean={params2[name].mean()}\")\n",
    "            # Only show a few differences to avoid overwhelming output\n",
    "            if not all_equal:\n",
    "                break\n",
    "                \n",
    "    return all_equal\n",
    "\n",
    "# Usage:\n",
    "are_identical = are_models_identical(base_model, model)\n",
    "print(f\"Models are identical: {are_identical}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "equations is deprecated, as it handled by the parser now\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsed correctly 3.8 2.4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='48' max='48' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [48/48 12:51]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "equations is deprecated, as it handled by the parser now\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsed correctly 5.2 1.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alisavin/AgenticADMET/openr1/lib/python3.11/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/alisavin/AgenticADMET/openr1/lib/python3.11/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "equations is deprecated, as it handled by the parser now\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsed correctly 4.2 -0.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "equations is deprecated, as it handled by the parser now\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsed correctly 5.2 2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "equations is deprecated, as it handled by the parser now\n",
      "equations is deprecated, as it handled by the parser now\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsed correctly 4.5 0.4\n",
      "parsed correctly 4.8 0.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "equations is deprecated, as it handled by the parser now\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "could not convert string to float: 'LogD value is high due to aromatic rings , chlorine, but precise value requires a LogD calculator.' ['LogD value is high due to aromatic rings , chlorine, but precise value requires a LogD calculator.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "equations is deprecated, as it handled by the parser now\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsed correctly 2.5 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "equations is deprecated, as it handled by the parser now\n",
      "equations is deprecated, as it handled by the parser now\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsed correctly 3.2 0.7\n",
      "parsed correctly 3.0 0.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "equations is deprecated, as it handled by the parser now\n",
      "equations is deprecated, as it handled by the parser now\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsed correctly -2.5 2.8\n",
      "parsed correctly 2.7 2.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "equations is deprecated, as it handled by the parser now\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsed correctly 4.2 2.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "equations is deprecated, as it handled by the parser now\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsed correctly 5.2 4.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "equations is deprecated, as it handled by the parser now\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsed correctly -0.5 1.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "equations is deprecated, as it handled by the parser now\n",
      "equations is deprecated, as it handled by the parser now\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsed correctly 2.7 0.6\n",
      "parsed correctly 4.8 0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "equations is deprecated, as it handled by the parser now\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsed correctly 2.8 3.8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/alisavin/AgenticADMET/notebooks/wandb/run-20250226_003359-z9ek8pgp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vladvin-org/huggingface/runs/z9ek8pgp' target=\"_blank\">test</a></strong> to <a href='https://wandb.ai/vladvin-org/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/vladvin-org/huggingface' target=\"_blank\">https://wandb.ai/vladvin-org/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/vladvin-org/huggingface/runs/z9ek8pgp' target=\"_blank\">https://wandb.ai/vladvin-org/huggingface/runs/z9ek8pgp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_result = grpo_trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alisavin/AgenticADMET/openr1/lib/python3.11/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/alisavin/AgenticADMET/openr1/lib/python3.11/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/12 06:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "equations is deprecated, as it handled by the parser now\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsed correctly 4.2 2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "equations is deprecated, as it handled by the parser now\n",
      "equations is deprecated, as it handled by the parser now\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsed correctly 5.8 0.0\n",
      "parsed correctly 4.2 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "equations is deprecated, as it handled by the parser now\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsed correctly 4.2 2.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "equations is deprecated, as it handled by the parser now\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsed correctly 4.0 4.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "equations is deprecated, as it handled by the parser now\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsed correctly 4.7 1.8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/alisavin/AgenticADMET/notebooks/wandb/run-20250226_001102-jiw8osvj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vladvin-org/huggingface/runs/jiw8osvj' target=\"_blank\">test</a></strong> to <a href='https://wandb.ai/vladvin-org/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/vladvin-org/huggingface' target=\"_blank\">https://wandb.ai/vladvin-org/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/vladvin-org/huggingface/runs/jiw8osvj' target=\"_blank\">https://wandb.ai/vladvin-org/huggingface/runs/jiw8osvj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_result = grpo_trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean mae tuned - 9.888541666666667, mean mae - 9.888541666666667\n",
      "median: mean mae tuned - 2.4400000000000004, mean mae - 2.4400000000000004\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "pred_path = \"test/completions/tuned_8.0/*.json\"\n",
    "num_generations = Path(pred_path).parts[-2].split(\"_\")[-1]\n",
    "pths_1 = glob.glob(pred_path)\n",
    "# pth_2 = glob.glob(\"test/not_tuned_pred/*json\")\n",
    "\n",
    "dict_all = {}\n",
    "\n",
    "mean_mae_1 = []\n",
    "mean_mae_2 = []\n",
    "\n",
    "mean_mae_1_median = []\n",
    "mean_mae_2_median = []\n",
    "\n",
    "\n",
    "for pth_i in pths_1:\n",
    "    with open(pth_i, \"r\") as f:\n",
    "        dict_i = json.load(f)\n",
    "    smiles = dict_i[\"smiles\"]\n",
    "    smiles_hash = hashlib.blake2b(smiles.encode('utf-8'), digest_size=4).hexdigest()\n",
    "\n",
    "    pth = f\"test/completions/init_{num_generations}/parsed_{smiles_hash}.json\" if Path(f\"test/completions/init_{num_generations}/parsed_{smiles_hash}.json\").exists() else f\"test/completions/init_{num_generations}/{smiles_hash}.json\"\n",
    "    with open(pth, \"r\") as f:\n",
    "        dict_i_2 = json.load(f)\n",
    "    \n",
    "    dict_all[smiles] = {\n",
    "        \"7BQwen\": {\n",
    "            \"completion\": dict_i_2[\"completion\"], \n",
    "            \"answer_val\": dict_i_2[\"answer_val\"],\n",
    "            \"mae\": dict_i_2[\"mae\"],\n",
    "            \"mae_median\": dict_i_2[\"mae_median\"]\n",
    "        },\n",
    "        \"7BQwenTuned\": {\n",
    "            \"completion\": dict_i[\"completion\"], \n",
    "            \"answer_val\": dict_i[\"answer_val\"],\n",
    "            \"mae\": dict_i[\"mae\"],\n",
    "            \"mae_median\": dict_i[\"mae_median\"]\n",
    "        },\n",
    "        \"gold_val\": dict_i_2[\"gold_val\"],\n",
    "    }\n",
    "    if dict_i_2[\"answer_val\"] is None:\n",
    "        dict_all[smiles][\"7BQwen\"][\"answer_parsed\"] = dict_i_2[\"answer_parsed\"]\n",
    "    if dict_i[\"answer_val\"] is None:\n",
    "        dict_all[smiles][\"7BQwenTuned\"][\"answer_parsed\"] = dict_i_2[\"answer_parsed\"]\n",
    "\n",
    "    if dict_i[\"mae\"] is not None:\n",
    "        mean_mae_1.extend([float(v_i) if v_i is not None else 10 for v_i in dict_i[\"mae\"]])\n",
    "    if dict_i_2[\"mae\"] is not None:\n",
    "        mean_mae_2.append([float(v_i) if v_i is not None else 10 for v_i in dict_i_2[\"mae\"]])\n",
    "\n",
    "    if dict_i[\"mae_median\"] is not None:\n",
    "        mean_mae_1_median.append(float(dict_i[\"mae_median\"]))\n",
    "    if dict_i_2[\"mae_median\"] is not None:\n",
    "        mean_mae_2_median.append(float(dict_i_2[\"mae_median\"]))\n",
    "print(f\"mean mae tuned - {np.mean(mean_mae_1)}, mean mae - {np.mean(mean_mae_2)}\")\n",
    "print(f\"median: mean mae tuned - {np.mean([v_i for v_i in mean_mae_1_median if not np.isnan(v_i)])}, mean mae - {np.mean([v_i for v_i in mean_mae_2_median if not np.isnan(v_i)])}\")\n",
    "\n",
    "with open(f\"./test/completions/all_results_{num_generations}.json\", \"w\") as f:\n",
    "    json.dump(dict_all, f, indent=2)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renamed: parsed3afaf4aa.json → parsed_afaf4aa.json\n",
      "Renamed: parsedbeff7dfa.json → parsed_eff7dfa.json\n",
      "Renamed: parsed159fc2ec.json → parsed_59fc2ec.json\n",
      "Renamed: parsed1a554c5c.json → parsed_a554c5c.json\n",
      "Renamed: parsedc94a83f1.json → parsed_94a83f1.json\n",
      "Renamed: parsed189835d9.json → parsed_89835d9.json\n",
      "Renamed: parsed2bed14c1.json → parsed_bed14c1.json\n",
      "Renamed: parsed16bad6bb.json → parsed_6bad6bb.json\n",
      "Renamed: parsedb940e7a8.json → parsed_940e7a8.json\n",
      "Renamed: parsed21a08268.json → parsed_1a08268.json\n",
      "Renamed: parsedec93f253.json → parsed_c93f253.json\n",
      "Renamed: parsed9ef3031d.json → parsed_ef3031d.json\n",
      "Renamed: parsedfc6cfa4d.json → parsed_c6cfa4d.json\n",
      "Renamed: parsede8858049.json → parsed_8858049.json\n",
      "Renamed: parsed03476906.json → parsed_3476906.json\n",
      "Renamed: parsedd26c74eb.json → parsed_26c74eb.json\n",
      "Renamed: parsed9698e723.json → parsed_698e723.json\n",
      "Renamed: parsedbb9f2209.json → parsed_b9f2209.json\n",
      "Renamed: parsed822656fc.json → parsed_22656fc.json\n",
      "Renamed: parsedf5f8aded.json → parsed_5f8aded.json\n",
      "Renamed: parsed778dbbe6.json → parsed_78dbbe6.json\n",
      "Renamed: parsed52196b7d.json → parsed_2196b7d.json\n",
      "Renamed: parsed89599bec.json → parsed_9599bec.json\n",
      "Renamed: parsed1275f45f.json → parsed_275f45f.json\n",
      "Renamed: parsedc1ed104f.json → parsed_1ed104f.json\n",
      "Renamed: parsed08d0433c.json → parsed_8d0433c.json\n",
      "Renamed: parsed36278a35.json → parsed_6278a35.json\n",
      "Renamed: parsed0ae45c73.json → parsed_ae45c73.json\n",
      "Renamed: parseda85986fc.json → parsed_85986fc.json\n",
      "Renamed: parsed62251fa8.json → parsed_2251fa8.json\n",
      "Renamed: parsed39d7ff38.json → parsed_9d7ff38.json\n",
      "Renamed: parsed8c1682c4.json → parsed_c1682c4.json\n",
      "Renamed: parsed266c53c9.json → parsed_66c53c9.json\n",
      "Renamed: parsedb43d0ed3.json → parsed_43d0ed3.json\n",
      "Renamed: parsed928a7833.json → parsed_28a7833.json\n",
      "Renamed: parsedc2e5d971.json → parsed_2e5d971.json\n",
      "Renamed: parsed23facd0c.json → parsed_3facd0c.json\n",
      "Renamed: parsed9fbb21ac.json → parsed_fbb21ac.json\n",
      "Renamed: parsed85b81b64.json → parsed_5b81b64.json\n",
      "Renamed: parsed0ea366e4.json → parsed_ea366e4.json\n",
      "Renamed: parsed1edc635b.json → parsed_edc635b.json\n",
      "Renamed: parsed92ce0070.json → parsed_2ce0070.json\n",
      "Renamed: parsedeed33aa9.json → parsed_ed33aa9.json\n",
      "Renamed: parsedd8142a3b.json → parsed_8142a3b.json\n",
      "Renamed: parseda0410400.json → parsed_0410400.json\n",
      "Renamed: parsedaed9f7e2.json → parsed_ed9f7e2.json\n",
      "Renamed: parsed51926692.json → parsed_1926692.json\n",
      "Renamed: parsed5da49738.json → parsed_da49738.json\n",
      "Renamed: parsed90d1569f.json → parsed_0d1569f.json\n",
      "Renamed: parsed9c6bca43.json → parsed_c6bca43.json\n",
      "Renamed: parsed877d316e.json → parsed_77d316e.json\n",
      "Renamed: parsed6277bed9.json → parsed_277bed9.json\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "\n",
    "# directory = \"./test/completions/pred/\"  # Replace with your directory path\n",
    "\n",
    "# for filename in os.listdir(directory):\n",
    "#     if filename.startswith(\"parsed\"):\n",
    "#         # Remove the underscore and add \"parsed_\" if needed\n",
    "#         new_name = \"parsed_\" + filename[7:]\n",
    "        \n",
    "#         # Full paths for renaming\n",
    "#         old_path = os.path.join(directory, filename)\n",
    "#         new_path = os.path.join(directory, new_name)\n",
    "        \n",
    "#         # Rename the file\n",
    "#         os.rename(old_path, new_path)\n",
    "#         print(f\"Renamed: {filename} → {new_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openr1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
