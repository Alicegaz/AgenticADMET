{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "from lightning import pytorch as pl\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "from chemprop import data, featurizers, models, nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORKERS = 0 # number of workers for dataloader. 0 means using main process for data loading\n",
    "smiles_column = 'smiles_std'\n",
    "TARGET_COLUMNS = ['LogHLM', 'LogMLM', 'LogD', 'LogKSOL', 'LogMDR1-MDCKII']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(input_df):\n",
    "    train_data, val_data = [], []\n",
    "    for _, row in input_df.iterrows():\n",
    "        dp = data.MoleculeDatapoint.from_smi(row[smiles_column], row[TARGET_COLUMNS].values)\n",
    "        if row['split'] == 'train':\n",
    "            train_data.append(dp)\n",
    "        elif row['split'] == 'val':\n",
    "            val_data.append(dp)\n",
    "\n",
    "    pred_data = []\n",
    "    for _, row in input_df.iterrows():\n",
    "        dp = data.MoleculeDatapoint.from_smi(row[smiles_column], row[TARGET_COLUMNS].values)\n",
    "        pred_data.append(dp)\n",
    "\n",
    "    featurizer = featurizers.SimpleMoleculeMolGraphFeaturizer()\n",
    "\n",
    "    train_dset = data.MoleculeDataset(train_data, featurizer)\n",
    "    # scaler = train_dset.normalize_targets()\n",
    "\n",
    "    val_dset = data.MoleculeDataset(val_data, featurizer)\n",
    "    # val_dset.normalize_targets(scaler)\n",
    "\n",
    "    pred_dset = data.MoleculeDataset(pred_data, featurizer)\n",
    "\n",
    "    return train_dset, val_dset, pred_dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_model(config, train_dset, val_dset, num_workers, scaler):\n",
    "def train_model(config, train_dset, val_dset, num_workers, save_dir):\n",
    "    # config is a dictionary containing hyperparameters used for the trial\n",
    "    depth = int(config[\"depth\"])\n",
    "    ffn_hidden_dim = int(config[\"ffn_hidden_dim\"])\n",
    "    ffn_num_layers = int(config[\"ffn_num_layers\"])\n",
    "    message_hidden_dim = int(config[\"message_hidden_dim\"])\n",
    "    dropout = 0.2\n",
    "\n",
    "    train_loader = data.build_dataloader(train_dset, num_workers=num_workers, shuffle=True)\n",
    "    val_loader = data.build_dataloader(val_dset, num_workers=num_workers, shuffle=False)\n",
    "\n",
    "    mp = nn.BondMessagePassing(d_h=message_hidden_dim, depth=depth, dropout=dropout)\n",
    "    agg = nn.MeanAggregation()\n",
    "    # output_transform = nn.UnscaleTransform.from_standard_scaler(scaler)\n",
    "    # ffn = nn.RegressionFFN(\n",
    "    #     n_tasks=len(target_columns),\n",
    "    #     output_transform=output_transform, input_dim=message_hidden_dim, hidden_dim=ffn_hidden_dim, n_layers=ffn_num_layers,\n",
    "    #     dropout=dropout\n",
    "    # )\n",
    "    ffn = nn.RegressionFFN(\n",
    "        n_tasks=len(TARGET_COLUMNS),\n",
    "        output_transform=None, input_dim=message_hidden_dim, hidden_dim=ffn_hidden_dim, n_layers=ffn_num_layers,\n",
    "        dropout=dropout\n",
    "    )\n",
    "    batch_norm = True\n",
    "    metric_list = [nn.metrics.MAE(), nn.metrics.R2Score()]\n",
    "    model = models.MPNN(mp, agg, ffn, batch_norm, metric_list)\n",
    "\n",
    "    ckpt_callback = ModelCheckpoint(\n",
    "        save_top_k=0,\n",
    "        save_last=True\n",
    "    )\n",
    "\n",
    "    exp_name = f\"chemprop_run_{RUN_IDX}\"\n",
    "    logger = WandbLogger(\n",
    "        project=\"admet-challenge\",\n",
    "        name=exp_name,\n",
    "        prefix=f\"{save_dir.stem}\",\n",
    "        save_dir=f\"../wandb/{exp_name}\"\n",
    "    )\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        accelerator=\"auto\",\n",
    "        devices=1,\n",
    "        max_epochs=200, # number of epochs to train for\n",
    "        # below are needed for Ray and Lightning integration\n",
    "        enable_progress_bar=True,\n",
    "        callbacks=[ckpt_callback],\n",
    "        default_root_dir=save_dir,\n",
    "        logger=logger\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        trainer.fit(model, train_loader, val_loader)\n",
    "    except Exception as e:\n",
    "        logger.finalize(\"failed\")\n",
    "        wandb.finish()\n",
    "        raise e\n",
    "    else:\n",
    "        logger.finalize(\"success\")\n",
    "\n",
    "    return model\n",
    "\n",
    "def predict(model, pred_dset, num_workers):\n",
    "    pred_loader = data.build_dataloader(pred_dset, num_workers=num_workers, shuffle=False)\n",
    "    \n",
    "    trainer = pl.Trainer(\n",
    "        accelerator=\"auto\",\n",
    "        devices=1,\n",
    "        enable_progress_bar=True\n",
    "    )\n",
    "\n",
    "    model.eval()\n",
    "    preds = trainer.predict(model, pred_loader, return_predictions=True)\n",
    "    preds = torch.cat(preds)\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CONFIG = {\n",
    "    \"depth\": 5,\n",
    "    \"ffn_hidden_dim\": 1500,\n",
    "    \"ffn_num_layers\": 2,\n",
    "    \"message_hidden_dim\": 300\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copypaste from https://github.com/asapdiscovery/asap-polaris-blind-challenge-examples/blob/a613051bac57060f686d9993e201ecaa15e51009/evaulation.py\n",
    "# with a log-transform fix according to this issue https://github.com/asapdiscovery/asap-polaris-blind-challenge-examples/issues/14\n",
    "\n",
    "from collections import defaultdict\n",
    "from typing import Tuple\n",
    "\n",
    "def mask_nan(y_true, y_pred):\n",
    "    mask = ~np.isnan(y_true)\n",
    "    y_true = np.array(y_true)[mask]\n",
    "    y_pred = np.array(y_pred)[mask]\n",
    "    return y_true, y_pred\n",
    "\n",
    "def eval_admet(preds: dict[str, list], refs: dict[str, list]) -> Tuple[dict[str, float], np.ndarray]:\n",
    "    \"\"\"\n",
    "    Eval ADMET targets with MAE for pre-log10 transformed targets (LogD) and MALE  (MAE on log10 transformed dataset) on non-transformed data\n",
    "\n",
    "    This provides a \"relative\" error metric that will not be as sensitive to the large outliers with huge errors. This is sometimes known as MALE.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    preds : dict[str, list]\n",
    "        Dictionary of predicted ADMET values.\n",
    "    refs : dict[str, list]\n",
    "        Dictionary of reference ADMET values.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict[str, float]\n",
    "        Returns a dictonary of summary statistics\n",
    "    \"\"\"\n",
    "    keys = {\n",
    "        \"MLM\",\n",
    "        \"HLM\",\n",
    "        \"KSOL\",\n",
    "        \"LogD\",\n",
    "        \"MDR1-MDCKII\",\n",
    "    }\n",
    "    # will be treated as is\n",
    "    logscale_endpts = {\"LogD\"}\n",
    "\n",
    "    collect = defaultdict(dict)\n",
    "\n",
    "    for k in keys:\n",
    "        if k not in preds.keys() or k not in refs.keys():\n",
    "            raise ValueError(\"required key not present\")\n",
    "\n",
    "        ref, pred = mask_nan(refs[k], preds[k])\n",
    "\n",
    "        if k in logscale_endpts:\n",
    "            # already log10scaled\n",
    "            mae = mean_absolute_error(ref, pred)\n",
    "            r2 = r2_score(ref, pred)\n",
    "        else:\n",
    "            # clip to a detection limit\n",
    "            # epsilon = 1e-8\n",
    "            # pred = np.clip(pred, a_min=epsilon, a_max=None)\n",
    "            # ref = np.clip(ref, a_min=epsilon, a_max=None)\n",
    "\n",
    "            # transform both log10scale\n",
    "            pred_log10s = np.log10(pred + 1.)\n",
    "            ref_log10s = np.log10(ref + 1.)\n",
    "\n",
    "            # compute MALE and R2 in log space\n",
    "            mae = mean_absolute_error(ref_log10s, pred_log10s)\n",
    "            r2 = r2_score(ref_log10s, pred_log10s)\n",
    "\n",
    "        collect[k][\"mean_absolute_error\"] = mae\n",
    "        collect[k][\"r2\"] = r2\n",
    "\n",
    "    # compute macro average MAE\n",
    "    macro_mae = np.mean([collect[k][\"mean_absolute_error\"] for k in keys])\n",
    "    collect[\"aggregated\"][\"macro_mean_absolute_error\"] = macro_mae\n",
    "\n",
    "    # compute macro average R2\n",
    "    macro_r2 = np.mean([collect[k][\"r2\"] for k in keys])\n",
    "    collect[\"aggregated\"][\"macro_r2\"] = macro_r2\n",
    "\n",
    "    return collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_preds(preds: pd.DataFrame):\n",
    "    preds_dict = {}\n",
    "    for t in TARGET_COLUMNS:\n",
    "        if t in [\"LogHLM\", \"LogMLM\", \"LogKSOL\", \"LogMDR1-MDCKII\"]:\n",
    "            # transform back to non-log scale\n",
    "            preds_dict[t[3:]] = np.power(10, preds.iloc[:, preds.columns.get_loc(f\"pred_{t}\")].values) - 1.\n",
    "        else:\n",
    "            preds_dict[t] = preds.iloc[:, preds.columns.get_loc(f\"pred_{t}\")].values\n",
    "    \n",
    "    return preds_dict\n",
    "\n",
    "def extract_refs(refs: pd.DataFrame):\n",
    "    refs_dict = {}\n",
    "    for t in TARGET_COLUMNS:\n",
    "        if t in [\"LogHLM\", \"LogMLM\", \"LogKSOL\", \"LogMDR1-MDCKII\"]:\n",
    "            refs_dict[t[3:]] = refs.iloc[:, refs.columns.get_loc(t[3:])].values\n",
    "        else:\n",
    "            refs_dict[t] = refs.iloc[:, refs.columns.get_loc(t)].values\n",
    "    \n",
    "    return refs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval(input_paths, save_dirs):\n",
    "    for input_path, save_dir in zip(input_paths, save_dirs):\n",
    "        print(f\"Training and predicting on {input_path}\")\n",
    "        input_df = pd.read_csv(input_path)\n",
    "        train_dset, val_dset, pred_dset = prepare_data(input_df)\n",
    "        model = train_model(MODEL_CONFIG, train_dset, val_dset, NUM_WORKERS, save_dir)\n",
    "        preds = predict(model, pred_dset, NUM_WORKERS)\n",
    "\n",
    "        output_df = input_df.copy()\n",
    "        output_df[[\"pred_\" + t for t in TARGET_COLUMNS]] = preds\n",
    "        save_dir.mkdir(parents=True, exist_ok=True)\n",
    "        output_df.to_csv(save_dir / \"predictions.csv\", index=False)\n",
    "\n",
    "        train_preds = extract_preds(output_df[input_df[\"split\"] == \"train\"])\n",
    "        train_refs = extract_refs(input_df[input_df[\"split\"] == \"train\"])\n",
    "        val_preds = extract_preds(output_df[input_df[\"split\"] == \"val\"])\n",
    "        val_refs = extract_refs(input_df[input_df[\"split\"] == \"val\"])\n",
    "\n",
    "        metrics = eval_admet(train_preds, train_refs)\n",
    "        print(\"Train metrics:\")\n",
    "        print(json.dumps(metrics, indent=2))\n",
    "\n",
    "        metrics = eval_admet(val_preds, val_refs)\n",
    "        print(\"\\nVal metrics:\")\n",
    "        print(json.dumps(metrics, indent=2))\n",
    "    \n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_paths = [Path(f'../data/asap/datasets/rnd_splits/split_{k}.csv') for k in range(5)]\n",
    "save_dirs = [Path(f'../output/asap/rnd_splits/chemprop/run_0/split_{k}') for k in range(5)]\n",
    "RUN_IDX = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_eval(input_paths, save_dirs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up + run 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(input_paths, save_dirs, output_dir, remove_worst_pct):\n",
    "    smiles_to_remove = defaultdict(set)\n",
    "\n",
    "    for input_path, save_dir in zip(input_paths, save_dirs):\n",
    "        input_df = pd.read_csv(input_path)\n",
    "        input_val_df = input_df[input_df[\"split\"] == \"val\"]\n",
    "        output_df = pd.read_csv(save_dir / \"predictions.csv\")\n",
    "        output_val_df = output_df[input_df[\"split\"] == \"val\"]\n",
    "\n",
    "        for t in TARGET_COLUMNS:\n",
    "            # Sort by absolute error\n",
    "            notna_mask = input_val_df[t].notna()\n",
    "            input_val_df = input_val_df[notna_mask]\n",
    "            output_val_df = output_val_df[notna_mask]\n",
    "\n",
    "            mae = np.abs(input_val_df[t] - output_val_df[f\"pred_{t}\"])\n",
    "            sorted_idx = np.argsort(mae)[::-1]\n",
    "            smiles_to_remove[t].update(\n",
    "                input_val_df.iloc[sorted_idx[:int(remove_worst_pct * len(sorted_idx))]][\"cxsmiles_std\"].tolist()\n",
    "            )\n",
    "\n",
    "    for input_path in input_paths:\n",
    "        input_df = pd.read_csv(input_path)\n",
    "        for t in TARGET_COLUMNS:\n",
    "            input_df.loc[input_df[\"cxsmiles_std\"].isin(smiles_to_remove[t]) & (input_df[\"split\"] == \"train\"), t] = np.nan\n",
    "\n",
    "        input_df.to_csv(output_dir / input_path.name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path(\"../output/asap/rnd_splits/chemprop/run_0/cleaned\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data(input_paths, save_dirs, output_dir, remove_worst_pct = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogHLM            133\n",
       "LogMLM            131\n",
       "LogD               90\n",
       "LogKSOL            69\n",
       "LogMDR1-MDCKII      9\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(input_paths[0])[TARGET_COLUMNS].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogHLM            185\n",
       "LogMLM            179\n",
       "LogD              128\n",
       "LogKSOL           102\n",
       "LogMDR1-MDCKII     45\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(output_dir / input_paths[0].name)[TARGET_COLUMNS].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_paths = [Path(f'../output/asap/rnd_splits/chemprop/run_0/cleaned/split_{k}.csv') for k in range(5)]\n",
    "save_dirs = [Path(f'../output/asap/rnd_splits/chemprop/run_1/split_{k}') for k in range(5)]\n",
    "RUN_IDX = \"1_clean_worst_pct_0.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_eval(input_paths, save_dirs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up + run 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path(\"../output/asap/rnd_splits/chemprop/run_1/cleaned\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data(input_paths, save_dirs, output_dir, remove_worst_pct = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_paths = [Path(f'../output/asap/rnd_splits/chemprop/run_1/cleaned/split_{k}.csv') for k in range(5)]\n",
    "save_dirs = [Path(f'../output/asap/rnd_splits/chemprop/run_2/split_{k}') for k in range(5)]\n",
    "RUN_IDX = \"2_clean_worst_pct_0.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_eval(input_paths, save_dirs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up stero impure + run 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(input_paths, save_dirs, output_dir, remove_worst_pct):\n",
    "    smiles_to_remove = defaultdict(set)\n",
    "\n",
    "    for input_path, save_dir in zip(input_paths, save_dirs):\n",
    "        input_df = pd.read_csv(input_path)\n",
    "        input_val_df = input_df[input_df[\"split\"] == \"val\"]\n",
    "        output_df = pd.read_csv(save_dir / \"predictions.csv\")\n",
    "        output_val_df = output_df[input_df[\"split\"] == \"val\"]\n",
    "\n",
    "        for t in TARGET_COLUMNS:\n",
    "            # Sort by absolute error\n",
    "            notna_mask = input_val_df[t].notna()\n",
    "            input_val_df = input_val_df[notna_mask]\n",
    "            output_val_df = output_val_df[notna_mask]\n",
    "\n",
    "            mae = np.abs(input_val_df[t] - output_val_df[f\"pred_{t}\"])\n",
    "            sorted_idx = np.argsort(mae)[::-1]\n",
    "            smiles_to_remove[t].update(\n",
    "                input_val_df.iloc[sorted_idx[:int(remove_worst_pct * len(sorted_idx))]][\"cxsmiles_std\"].tolist()\n",
    "            )\n",
    "\n",
    "    for input_path in input_paths:\n",
    "        input_df = pd.read_csv(input_path)\n",
    "        for t in TARGET_COLUMNS:\n",
    "            input_df.loc[\n",
    "                input_df[\"cxsmiles_std\"].isin(smiles_to_remove[t]) & \\\n",
    "                    ~input_df[\"smiles_ext\"].isna() & \\\n",
    "                    (input_df[\"split\"] == \"train\"),\n",
    "                t\n",
    "            ] = np.nan\n",
    "\n",
    "        input_df.to_csv(output_dir / input_path.name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path(\"../output/asap/rnd_splits/chemprop/run_0/cleaned\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data(input_paths, save_dirs, output_dir, remove_worst_pct = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogHLM            133\n",
       "LogMLM            131\n",
       "LogD               90\n",
       "LogKSOL            69\n",
       "LogMDR1-MDCKII      9\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(input_paths[0])[TARGET_COLUMNS].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogHLM            71\n",
       "LogMLM            55\n",
       "LogD              45\n",
       "LogKSOL           35\n",
       "LogMDR1-MDCKII     7\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = pd.read_csv(input_paths[0])\n",
    "tmp = tmp[tmp[\"smiles_ext\"].isna()]\n",
    "tmp[TARGET_COLUMNS].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogHLM            166\n",
       "LogMLM            158\n",
       "LogD              105\n",
       "LogKSOL            88\n",
       "LogMDR1-MDCKII     27\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(output_dir / input_paths[0].name)[TARGET_COLUMNS].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogHLM            71\n",
       "LogMLM            55\n",
       "LogD              45\n",
       "LogKSOL           35\n",
       "LogMDR1-MDCKII     7\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = pd.read_csv(output_dir / input_paths[0].name)\n",
    "tmp = tmp[tmp[\"smiles_ext\"].isna()]\n",
    "tmp[TARGET_COLUMNS].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_paths = [Path(f'../output/asap/rnd_splits/chemprop/run_0/cleaned/split_{k}.csv') for k in range(5)]\n",
    "save_dirs = [Path(f'../output/asap/rnd_splits/chemprop/run_1/split_{k}') for k in range(5)]\n",
    "RUN_IDX = \"1_clean_worst_pct_0.2_stereo_impure\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_eval(input_paths, save_dirs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing all stereo impure + run 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_paths = [Path(f'../data/asap/datasets/rnd_splits/split_{k}.csv') for k in range(5)]\n",
    "output_dir = Path(\"../data/asap/datasets/rnd_splits/stereo_pure\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for input_path in input_paths:\n",
    "    input_df = pd.read_csv(input_path)\n",
    "    input_df = pd.concat([\n",
    "        input_df[(input_df[\"split\"] == \"train\") & input_df[\"smiles_ext\"].isna()],\n",
    "        input_df[input_df[\"split\"] == \"val\"]\n",
    "    ])\n",
    "    input_df.to_csv(output_dir / input_path.name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_paths = [Path(f'../data/asap/datasets/rnd_splits/stereo_pure/split_{k}.csv') for k in range(5)]\n",
    "save_dirs = [Path(f'../output/asap/rnd_splits/chemprop/run_0/stereo_pure/split_{k}') for k in range(5)]\n",
    "RUN_IDX = \"0_stereo_pure\"\n",
    "train_and_eval(input_paths, save_dirs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "admet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
