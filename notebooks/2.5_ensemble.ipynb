{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_COLUMNS = ['LogHLM', 'LogMLM', 'LogD', 'LogKSOL', 'LogMDR1-MDCKII']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dir = '../output/asap/rnd_splits/'\n",
    "models = ['chemprop', 'roberta']\n",
    "preds = defaultdict(list)\n",
    "for split_idx in range(5):\n",
    "    for model_name in models:\n",
    "        pred_path = Path(pred_dir) / model_name / 'run_0' / f'split_{split_idx}' / 'predictions.csv'\n",
    "        preds[f'split_{split_idx}'].append(pd.read_csv(pred_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copypaste from https://github.com/asapdiscovery/asap-polaris-blind-challenge-examples/blob/a613051bac57060f686d9993e201ecaa15e51009/evaulation.py\n",
    "# with a log-transform fix according to this issue https://github.com/asapdiscovery/asap-polaris-blind-challenge-examples/issues/14\n",
    "\n",
    "from collections import defaultdict\n",
    "from typing import Tuple\n",
    "\n",
    "def mask_nan(y_true, y_pred):\n",
    "    mask = ~np.isnan(y_true)\n",
    "    y_true = np.array(y_true)[mask]\n",
    "    y_pred = np.array(y_pred)[mask]\n",
    "    return y_true, y_pred\n",
    "\n",
    "def eval_admet(preds: dict[str, list], refs: dict[str, list]) -> Tuple[dict[str, float], np.ndarray]:\n",
    "    \"\"\"\n",
    "    Eval ADMET targets with MAE for pre-log10 transformed targets (LogD) and MALE  (MAE on log10 transformed dataset) on non-transformed data\n",
    "\n",
    "    This provides a \"relative\" error metric that will not be as sensitive to the large outliers with huge errors. This is sometimes known as MALE.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    preds : dict[str, list]\n",
    "        Dictionary of predicted ADMET values.\n",
    "    refs : dict[str, list]\n",
    "        Dictionary of reference ADMET values.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict[str, float]\n",
    "        Returns a dictonary of summary statistics\n",
    "    \"\"\"\n",
    "    keys = {\n",
    "        \"MLM\",\n",
    "        \"HLM\",\n",
    "        \"KSOL\",\n",
    "        \"LogD\",\n",
    "        \"MDR1-MDCKII\",\n",
    "    }\n",
    "    # will be treated as is\n",
    "    logscale_endpts = {\"LogD\"}\n",
    "\n",
    "    collect = defaultdict(dict)\n",
    "\n",
    "    for k in keys:\n",
    "        if k not in preds.keys() or k not in refs.keys():\n",
    "            raise ValueError(\"required key not present\")\n",
    "\n",
    "        ref, pred = mask_nan(refs[k], preds[k])\n",
    "\n",
    "        if k in logscale_endpts:\n",
    "            # already log10scaled\n",
    "            mae = mean_absolute_error(ref, pred)\n",
    "            r2 = r2_score(ref, pred)\n",
    "        else:\n",
    "            # clip to a detection limit\n",
    "            # epsilon = 1e-8\n",
    "            # pred = np.clip(pred, a_min=epsilon, a_max=None)\n",
    "            # ref = np.clip(ref, a_min=epsilon, a_max=None)\n",
    "\n",
    "            # transform both log10scale\n",
    "            pred_log10s = np.log10(pred + 1.)\n",
    "            ref_log10s = np.log10(ref + 1.)\n",
    "\n",
    "            # compute MALE and R2 in log space\n",
    "            mae = mean_absolute_error(ref_log10s, pred_log10s)\n",
    "            r2 = r2_score(ref_log10s, pred_log10s)\n",
    "\n",
    "        collect[k][\"mean_absolute_error\"] = mae\n",
    "        collect[k][\"r2\"] = r2\n",
    "\n",
    "    # compute macro average MAE\n",
    "    macro_mae = np.mean([collect[k][\"mean_absolute_error\"] for k in keys])\n",
    "    collect[\"aggregated\"][\"macro_mean_absolute_error\"] = macro_mae\n",
    "\n",
    "    # compute macro average R2\n",
    "    macro_r2 = np.mean([collect[k][\"r2\"] for k in keys])\n",
    "    collect[\"aggregated\"][\"macro_r2\"] = macro_r2\n",
    "\n",
    "    return collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_preds(preds: pd.DataFrame):\n",
    "    preds_dict = {}\n",
    "    for t in TARGET_COLUMNS:\n",
    "        if t in [\"LogHLM\", \"LogMLM\", \"LogKSOL\", \"LogMDR1-MDCKII\"]:\n",
    "            # transform back to non-log scale\n",
    "            preds_dict[t[3:]] = np.power(10, preds.iloc[:, preds.columns.get_loc(f\"pred_{t}\")].values) - 1.\n",
    "        else:\n",
    "            preds_dict[t] = preds.iloc[:, preds.columns.get_loc(f\"pred_{t}\")].values\n",
    "    \n",
    "    return preds_dict\n",
    "\n",
    "def extract_refs(refs: pd.DataFrame):\n",
    "    refs_dict = {}\n",
    "    for t in TARGET_COLUMNS:\n",
    "        if t in [\"LogHLM\", \"LogMLM\", \"LogKSOL\", \"LogMDR1-MDCKII\"]:\n",
    "            refs_dict[t[3:]] = refs.iloc[:, refs.columns.get_loc(t[3:])].values\n",
    "        else:\n",
    "            refs_dict[t] = refs.iloc[:, refs.columns.get_loc(t)].values\n",
    "    \n",
    "    return refs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Val metrics:\n",
      "{\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.46723858925373135,\n",
      "    \"r2\": 0.7299866884942934\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.40572426009085244,\n",
      "    \"r2\": 0.33521354172613305\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.18577788307780504,\n",
      "    \"r2\": 0.5648184088917596\n",
      "  },\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.2654457301867883,\n",
      "    \"r2\": 0.5565578358026182\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.27365844977556714,\n",
      "    \"r2\": 0.2730263573939373\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.31956898247694887,\n",
      "    \"macro_r2\": 0.49192056646174825\n",
      "  }\n",
      "}\n",
      "\n",
      "Val metrics:\n",
      "{\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.39726614218571427,\n",
      "    \"r2\": 0.8057381337306473\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.3000724625310541,\n",
      "    \"r2\": 0.6877236304349188\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.1942623224901187,\n",
      "    \"r2\": 0.4221671113762564\n",
      "  },\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.3053094021621843,\n",
      "    \"r2\": 0.5046082298758539\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.31252964144761813,\n",
      "    \"r2\": 0.5640470941012812\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.3018879941633379,\n",
      "    \"macro_r2\": 0.5968568399037915\n",
      "  }\n",
      "}\n",
      "\n",
      "Val metrics:\n",
      "{\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.42708009420289855,\n",
      "    \"r2\": 0.7231958378388798\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.2840974878760748,\n",
      "    \"r2\": 0.5834224206128046\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.18253748205729117,\n",
      "    \"r2\": 0.49107419431140076\n",
      "  },\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.31144093197987704,\n",
      "    \"r2\": 0.5608735162572314\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.24077277674753142,\n",
      "    \"r2\": 0.6480050159123506\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.2891857545727346,\n",
      "    \"macro_r2\": 0.6013141969865334\n",
      "  }\n",
      "}\n",
      "\n",
      "Val metrics:\n",
      "{\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.36385039676388886,\n",
      "    \"r2\": 0.8191827371845817\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.28733727296917494,\n",
      "    \"r2\": 0.5827038684035473\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.16329716856798668,\n",
      "    \"r2\": 0.6946484692996567\n",
      "  },\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.30290366212779196,\n",
      "    \"r2\": 0.6061579078544002\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.3354142441394214,\n",
      "    \"r2\": 0.35728929216069116\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.2905605489136528,\n",
      "    \"macro_r2\": 0.6119964549805754\n",
      "  }\n",
      "}\n",
      "\n",
      "Val metrics:\n",
      "{\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.42033029121212123,\n",
      "    \"r2\": 0.6872220282892854\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.2271221026239578,\n",
      "    \"r2\": 0.619491057538581\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.15723638077320298,\n",
      "    \"r2\": 0.7261227064557936\n",
      "  },\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.3412047928559605,\n",
      "    \"r2\": 0.48495456049224506\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.23676070510210007,\n",
      "    \"r2\": 0.7500237744837639\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.2765308545134685,\n",
      "    \"macro_r2\": 0.6535628254519338\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# average predictions across splits\n",
    "for split_idx, split_preds in preds.items():\n",
    "    for t in TARGET_COLUMNS:\n",
    "        pred = split_preds[0].copy()\n",
    "        pred[t] = (split_preds[0][t] + split_preds[1][t]) / 2\n",
    "\n",
    "    train_preds = extract_preds(pred[pred[\"split\"] == \"train\"])\n",
    "    train_refs = extract_refs(pred[pred[\"split\"] == \"train\"])\n",
    "    val_preds = extract_preds(pred[pred[\"split\"] == \"val\"])\n",
    "    val_refs = extract_refs(pred[pred[\"split\"] == \"val\"])\n",
    "\n",
    "    metrics = eval_admet(val_preds, val_refs)\n",
    "    print(\"\\nVal metrics:\")\n",
    "    print(json.dumps(metrics, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "admet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
