{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[03:00:52] Initializing Normalizer\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import json\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "from lightning import pytorch as pl\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import wandb\n",
    "\n",
    "sys.path.insert(0, '../agenticadmet')\n",
    "from datasets import RegressionDataset\n",
    "from eval import eval_admet, extract_preds, extract_refs\n",
    "from models import TransformerRegressionModel\n",
    "from utils import CheckpointParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORKERS = 0 # number of workers for dataloader. 0 means using main process for data loading\n",
    "SMILES_COLUMN = 'smiles_std'\n",
    "TARGET_COLUMNS = ['LogHLM', 'LogMLM', 'LogD', 'LogKSOL', 'LogMDR1-MDCKII']\n",
    "\n",
    "MODEL_PARAMS = {\n",
    "    'config': {\n",
    "        'vocab_size': 500,\n",
    "        'hidden_size': 384,\n",
    "        'num_hidden_layers': 6,\n",
    "        'num_attention_heads': 8,\n",
    "        'intermediate_size': 1024,\n",
    "        'hidden_act': \"gelu\",\n",
    "        'hidden_dropout_prob': 0.1,\n",
    "        'attention_probs_dropout_prob': 0.1,\n",
    "        'max_position_embeddings': 512,\n",
    "        'initializer_range': 0.02,\n",
    "        'layer_norm_eps': 1e-12,\n",
    "        'pad_token_id': 0,\n",
    "        'position_embedding_type': \"absolute\",\n",
    "        'use_cache': True,\n",
    "        'type_vocab_size': 2\n",
    "    },\n",
    "    'output_dim': len(TARGET_COLUMNS),\n",
    "    'bias_final': False\n",
    "}\n",
    "MODEL_CONFIG = {\n",
    "    \"hidden_dim\": 384,\n",
    "    \"num_layers\": 1,\n",
    "    \"dropout\": 0.034136,\n",
    "    \"weight_decay\": 0.000810,\n",
    "    \"batch_size\": 32,\n",
    "    \"max_epochs\": 200\n",
    "}\n",
    "TOKENIZER_NAME = '<gs_bucket>/artifacts/tokenizers/zinc'\n",
    "CHECKPOINTS = [\n",
    "    CheckpointParams(\n",
    "        path=str(Path('../output/artifacts/mol_mlm_roberta_zinc/last.ckpt').absolute()),\n",
    "        module_from='roberta',\n",
    "        module_to='roberta',\n",
    "        strict=True\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(input_path, batch_size):\n",
    "    train_dset = RegressionDataset(\n",
    "        data_path=input_path,\n",
    "        smiles_col=SMILES_COLUMN,\n",
    "        target_cols=TARGET_COLUMNS,\n",
    "        split='train',\n",
    "        tokenizer_name=TOKENIZER_NAME,\n",
    "        mol_masking_prob=0.3,\n",
    "        mol_masking_val=0.15\n",
    "    )\n",
    "    val_dset = RegressionDataset(\n",
    "        data_path=input_path,\n",
    "        smiles_col=SMILES_COLUMN,\n",
    "        target_cols=TARGET_COLUMNS,\n",
    "        split='val',\n",
    "        tokenizer_name=TOKENIZER_NAME\n",
    "    )\n",
    "    pred_dset = RegressionDataset(\n",
    "        data_path=input_path,\n",
    "        smiles_col=SMILES_COLUMN,\n",
    "        target_cols=TARGET_COLUMNS,\n",
    "        split=None,\n",
    "        tokenizer_name=TOKENIZER_NAME,\n",
    "        remove_all_nan_targets=False\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dset, batch_size=batch_size, shuffle=True,\n",
    "        num_workers=NUM_WORKERS, collate_fn=train_dset.collate_fn\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dset, batch_size=batch_size, shuffle=False,\n",
    "        num_workers=NUM_WORKERS, collate_fn=val_dset.collate_fn\n",
    "    )\n",
    "    pred_loader = DataLoader(\n",
    "        pred_dset, batch_size=batch_size, shuffle=False,\n",
    "        num_workers=NUM_WORKERS, collate_fn=pred_dset.collate_fn\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader, pred_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config, train_loader, val_loader, save_dir, run_idx):\n",
    "    model_params = MODEL_PARAMS.copy()\n",
    "    model_params['hidden_dim'] = config['hidden_dim']\n",
    "    model_params['num_layers'] = config['num_layers']\n",
    "    model_params['dropout'] = config['dropout']\n",
    "    weight_decay = config['weight_decay']\n",
    "\n",
    "    model = TransformerRegressionModel(\n",
    "        model_name='roberta-base',\n",
    "        model_params=model_params,\n",
    "        weight_decay=weight_decay,\n",
    "        checkpoints=CHECKPOINTS\n",
    "    )\n",
    "\n",
    "    ckpt_callback = ModelCheckpoint(\n",
    "        save_top_k=0,\n",
    "        save_last=True\n",
    "    )\n",
    "\n",
    "    exp_name = f\"roberta_run_{run_idx}\"\n",
    "    logger = WandbLogger(\n",
    "        project=\"admet-challenge\",\n",
    "        name=exp_name,\n",
    "        prefix=f\"{save_dir.stem}\",\n",
    "        save_dir=f\"../wandb/{exp_name}\"\n",
    "    )\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        accelerator=\"auto\",\n",
    "        devices=1,\n",
    "        max_epochs=config['max_epochs'],\n",
    "        enable_progress_bar=False,\n",
    "        callbacks=[ckpt_callback],\n",
    "        default_root_dir=save_dir,\n",
    "        logger=logger\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        trainer.fit(model, train_loader, val_loader)\n",
    "    except Exception as e:\n",
    "        logger.finalize(\"failed\")\n",
    "        wandb.finish(exit_code=1)\n",
    "        raise e\n",
    "    else:\n",
    "        logger.finalize(\"success\")\n",
    "\n",
    "    return model\n",
    "\n",
    "def predict(model, pred_loader):\n",
    "    trainer = pl.Trainer(\n",
    "        accelerator=\"auto\",\n",
    "        devices=1,\n",
    "        enable_progress_bar=False\n",
    "    )\n",
    "\n",
    "    model.eval()\n",
    "    preds = trainer.predict(model, pred_loader, return_predictions=True)\n",
    "    preds = torch.cat(preds)\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval(input_paths, save_dirs, run_idx):\n",
    "    for input_path, save_dir in zip(input_paths, save_dirs):\n",
    "        print(f\"Training and predicting on {input_path}\")\n",
    "        train_loader, val_loader, pred_loader = prepare_data(input_path, MODEL_CONFIG['batch_size'])\n",
    "        model = train_model(MODEL_CONFIG, train_loader, val_loader, save_dir, run_idx)\n",
    "        preds = predict(model, pred_loader)\n",
    "\n",
    "        input_df = pred_loader.dataset.data.copy()\n",
    "        output_df = input_df.copy()\n",
    "        output_df[[\"pred_\" + t for t in TARGET_COLUMNS]] = preds\n",
    "        save_dir.mkdir(parents=True, exist_ok=True)\n",
    "        output_df.to_csv(save_dir / \"predictions.csv\", index=False)\n",
    "\n",
    "        train_preds = extract_preds(output_df[input_df[\"split\"] == \"train\"])\n",
    "        train_refs = extract_refs(input_df[input_df[\"split\"] == \"train\"])\n",
    "        val_preds = extract_preds(output_df[input_df[\"split\"] == \"val\"])\n",
    "        val_refs = extract_refs(input_df[input_df[\"split\"] == \"val\"])\n",
    "\n",
    "        metrics = eval_admet(train_preds, train_refs)\n",
    "        print(\"Train metrics:\")\n",
    "        print(json.dumps(metrics, indent=2))\n",
    "\n",
    "        metrics = eval_admet(val_preds, val_refs)\n",
    "        print(\"\\nVal metrics:\")\n",
    "        print(json.dumps(metrics, indent=2))\n",
    "    \n",
    "    wandb.finish(exit_code=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_paths = [Path(f'../data/asap/datasets/rnd_splits/split_{k}.csv') for k in range(5)]\n",
    "save_dirs = [Path(f'../output/asap/rnd_splits/roberta/run_0/split_{k}') for k in range(5)]\n",
    "RUN_IDX = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predicting on ../data/asap/datasets/rnd_splits/split_0.csv\n",
      "0 out of 323 rows are removed due to missing values\n",
      "Downloading checkpoint from <gs_bucket>/artifacts/tokenizers/zinc...\n",
      "0 out of 81 rows are removed due to missing values\n",
      "Downloading checkpoint from <gs_bucket>/artifacts/tokenizers/zinc...\n",
      "Downloading checkpoint from <gs_bucket>/artifacts/tokenizers/zinc...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting checkpoint from /home/jupyter/AgenticADMET/notebooks/../output/artifacts/mol_mlm_roberta_zinc/last.ckpt...\n",
      "Loading checkpoint from roberta to roberta...\n",
      "<All keys matched successfully>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvladvin111\u001b[0m (\u001b[33mvladvin-org\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Path ../wandb/roberta_run_0/wandb/ wasn't writable, using system temp directory.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/var/tmp/wandb/run-20250313_030054-0nddejpr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vladvin-org/admet-challenge/runs/0nddejpr' target=\"_blank\">roberta_run_0</a></strong> to <a href='https://wandb.ai/vladvin-org/admet-challenge' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/vladvin-org/admet-challenge' target=\"_blank\">https://wandb.ai/vladvin-org/admet-challenge</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/vladvin-org/admet-challenge/runs/0nddejpr' target=\"_blank\">https://wandb.ai/vladvin-org/admet-challenge/runs/0nddejpr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type         | Params | Mode \n",
      "---------------------------------------------------\n",
      "0 | roberta   | RobertaModel | 8.7 M  | train\n",
      "1 | predictor | MLP          | 149 K  | train\n",
      "2 | criterion | MSE          | 0      | train\n",
      "3 | metrics   | ModuleList   | 0      | train\n",
      "---------------------------------------------------\n",
      "8.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "8.8 M     Total params\n",
      "35.297    Total estimated model params size (MB)\n",
      "128       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (11) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.11707774119255619,\n",
      "    \"r2\": 0.9454698514205317\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.1180256760429345,\n",
      "    \"r2\": 0.9404269182067869\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.1316071887586831,\n",
      "    \"r2\": 0.9809175954420684\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.09872270813343852,\n",
      "    \"r2\": 0.8885469168848485\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.09649234817597283,\n",
      "    \"r2\": 0.962364488580419\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.11238513246071705,\n",
      "    \"macro_r2\": 0.9435451541069309\n",
      "  }\n",
      "}\n",
      "\n",
      "Val metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.36893946565001606,\n",
      "    \"r2\": 0.3966473491365998\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.4057323382028666,\n",
      "    \"r2\": 0.15936885348513719\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.6083090476227588,\n",
      "    \"r2\": 0.588662744607156\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.24540030538454707,\n",
      "    \"r2\": 0.2616812497296066\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.3936053420701004,\n",
      "    \"r2\": 0.3452330798822836\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.40439729978605776,\n",
      "    \"macro_r2\": 0.35031865536815665\n",
      "  }\n",
      "}\n",
      "Training and predicting on ../data/asap/datasets/rnd_splits/split_1.csv\n",
      "0 out of 323 rows are removed due to missing values\n",
      "Downloading checkpoint from <gs_bucket>/artifacts/tokenizers/zinc...\n",
      "0 out of 81 rows are removed due to missing values\n",
      "Downloading checkpoint from <gs_bucket>/artifacts/tokenizers/zinc...\n",
      "Downloading checkpoint from <gs_bucket>/artifacts/tokenizers/zinc...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type         | Params | Mode \n",
      "---------------------------------------------------\n",
      "0 | roberta   | RobertaModel | 8.7 M  | train\n",
      "1 | predictor | MLP          | 149 K  | train\n",
      "2 | criterion | MSE          | 0      | train\n",
      "3 | metrics   | ModuleList   | 0      | train\n",
      "---------------------------------------------------\n",
      "8.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "8.8 M     Total params\n",
      "35.297    Total estimated model params size (MB)\n",
      "128       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting checkpoint from /home/jupyter/AgenticADMET/notebooks/../output/artifacts/mol_mlm_roberta_zinc/last.ckpt...\n",
      "Loading checkpoint from roberta to roberta...\n",
      "<All keys matched successfully>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (11) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.09330826908521925,\n",
      "    \"r2\": 0.9587816336538684\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.1311126036960067,\n",
      "    \"r2\": 0.915786143011917\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.29641040158413706,\n",
      "    \"r2\": 0.9011874536169845\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.13083977592158175,\n",
      "    \"r2\": 0.8140055254510261\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.1256029422558244,\n",
      "    \"r2\": 0.9331682433674124\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.15545479850855384,\n",
      "    \"macro_r2\": 0.9045857998202417\n",
      "  }\n",
      "}\n",
      "\n",
      "Val metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.3545896258368104,\n",
      "    \"r2\": 0.29647026010569166\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.34027800429424065,\n",
      "    \"r2\": 0.5062724905868579\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.6396299009837888,\n",
      "    \"r2\": 0.5343058104214525\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.23528503859329986,\n",
      "    \"r2\": 0.38228925056070595\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.42636760215193914,\n",
      "    \"r2\": 0.312163252543852\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.39923003437201576,\n",
      "    \"macro_r2\": 0.40630021284371204\n",
      "  }\n",
      "}\n",
      "Training and predicting on ../data/asap/datasets/rnd_splits/split_2.csv\n",
      "0 out of 323 rows are removed due to missing values\n",
      "Downloading checkpoint from <gs_bucket>/artifacts/tokenizers/zinc...\n",
      "0 out of 81 rows are removed due to missing values\n",
      "Downloading checkpoint from <gs_bucket>/artifacts/tokenizers/zinc...\n",
      "Downloading checkpoint from <gs_bucket>/artifacts/tokenizers/zinc...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type         | Params | Mode \n",
      "---------------------------------------------------\n",
      "0 | roberta   | RobertaModel | 8.7 M  | train\n",
      "1 | predictor | MLP          | 149 K  | train\n",
      "2 | criterion | MSE          | 0      | train\n",
      "3 | metrics   | ModuleList   | 0      | train\n",
      "---------------------------------------------------\n",
      "8.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "8.8 M     Total params\n",
      "35.297    Total estimated model params size (MB)\n",
      "128       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting checkpoint from /home/jupyter/AgenticADMET/notebooks/../output/artifacts/mol_mlm_roberta_zinc/last.ckpt...\n",
      "Loading checkpoint from roberta to roberta...\n",
      "<All keys matched successfully>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (11) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.17329893675353814,\n",
      "    \"r2\": 0.906333832501286\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.10462049972040749,\n",
      "    \"r2\": 0.942030259200478\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.13688569842054257,\n",
      "    \"r2\": 0.9762734583686631\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.15012075925236545,\n",
      "    \"r2\": 0.7969013429967426\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.12070718059307903,\n",
      "    \"r2\": 0.9440402108888682\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.13712661494798656,\n",
      "    \"macro_r2\": 0.9131158207912076\n",
      "  }\n",
      "}\n",
      "\n",
      "Val metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.44007656154869956,\n",
      "    \"r2\": 0.22322918373146305\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.3510105578940846,\n",
      "    \"r2\": 0.25925973241816436\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.6124803167762178,\n",
      "    \"r2\": 0.6002605932152225\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.21742825486745174,\n",
      "    \"r2\": 0.42529843149957125\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.4025883892481567,\n",
      "    \"r2\": 0.2575432993251845\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.4047168160669221,\n",
      "    \"macro_r2\": 0.3531182480379212\n",
      "  }\n",
      "}\n",
      "Training and predicting on ../data/asap/datasets/rnd_splits/split_3.csv\n",
      "0 out of 323 rows are removed due to missing values\n",
      "Downloading checkpoint from <gs_bucket>/artifacts/tokenizers/zinc...\n",
      "0 out of 81 rows are removed due to missing values\n",
      "Downloading checkpoint from <gs_bucket>/artifacts/tokenizers/zinc...\n",
      "Downloading checkpoint from <gs_bucket>/artifacts/tokenizers/zinc...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type         | Params | Mode \n",
      "---------------------------------------------------\n",
      "0 | roberta   | RobertaModel | 8.7 M  | train\n",
      "1 | predictor | MLP          | 149 K  | train\n",
      "2 | criterion | MSE          | 0      | train\n",
      "3 | metrics   | ModuleList   | 0      | train\n",
      "---------------------------------------------------\n",
      "8.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "8.8 M     Total params\n",
      "35.297    Total estimated model params size (MB)\n",
      "128       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting checkpoint from /home/jupyter/AgenticADMET/notebooks/../output/artifacts/mol_mlm_roberta_zinc/last.ckpt...\n",
      "Loading checkpoint from roberta to roberta...\n",
      "<All keys matched successfully>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (11) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.11284646369754178,\n",
      "    \"r2\": 0.9445926974173611\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.11933312973867408,\n",
      "    \"r2\": 0.925095950896668\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.2438219081307117,\n",
      "    \"r2\": 0.9423412722264201\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.09365060123064267,\n",
      "    \"r2\": 0.8944969586975243\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.14537945635294044,\n",
      "    \"r2\": 0.9226030211347436\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.14300631183010215,\n",
      "    \"macro_r2\": 0.9258259800745433\n",
      "  }\n",
      "}\n",
      "\n",
      "Val metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.2888275660137743,\n",
      "    \"r2\": 0.4741857068955867\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.31520020386605424,\n",
      "    \"r2\": 0.3642760963583215\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.6188895040842203,\n",
      "    \"r2\": 0.4979882502286591\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.2524138817879424,\n",
      "    \"r2\": 0.09807755139150531\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.41397542035794205,\n",
      "    \"r2\": 0.15901814589038699\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.3778613152219866,\n",
      "    \"macro_r2\": 0.3187091501528919\n",
      "  }\n",
      "}\n",
      "Training and predicting on ../data/asap/datasets/rnd_splits/split_4.csv\n",
      "0 out of 324 rows are removed due to missing values\n",
      "Downloading checkpoint from <gs_bucket>/artifacts/tokenizers/zinc...\n",
      "0 out of 80 rows are removed due to missing values\n",
      "Downloading checkpoint from <gs_bucket>/artifacts/tokenizers/zinc...\n",
      "Downloading checkpoint from <gs_bucket>/artifacts/tokenizers/zinc...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type         | Params | Mode \n",
      "---------------------------------------------------\n",
      "0 | roberta   | RobertaModel | 8.7 M  | train\n",
      "1 | predictor | MLP          | 149 K  | train\n",
      "2 | criterion | MSE          | 0      | train\n",
      "3 | metrics   | ModuleList   | 0      | train\n",
      "---------------------------------------------------\n",
      "8.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "8.8 M     Total params\n",
      "35.297    Total estimated model params size (MB)\n",
      "128       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting checkpoint from /home/jupyter/AgenticADMET/notebooks/../output/artifacts/mol_mlm_roberta_zinc/last.ckpt...\n",
      "Loading checkpoint from roberta to roberta...\n",
      "<All keys matched successfully>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (11) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.1164171334834601,\n",
      "    \"r2\": 0.9419397233682875\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.1028978622056051,\n",
      "    \"r2\": 0.9509841057503561\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.15070896101436873,\n",
      "    \"r2\": 0.9772361119643264\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.07961315075839501,\n",
      "    \"r2\": 0.9227259289622288\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.08248505188120814,\n",
      "    \"r2\": 0.9718402954710741\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.10642443186860742,\n",
      "    \"macro_r2\": 0.9529452331032544\n",
      "  }\n",
      "}\n",
      "\n",
      "Val metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.2867313222443941,\n",
      "    \"r2\": 0.7013819267458912\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.3081166543311521,\n",
      "    \"r2\": 0.3878684762909246\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.5451459044516087,\n",
      "    \"r2\": 0.5842822482942774\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.19047527476813583,\n",
      "    \"r2\": 0.5332327900343475\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.36031081105196167,\n",
      "    \"r2\": 0.44182544005704927\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.33815599336945046,\n",
      "    \"macro_r2\": 0.529718176284498\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>split_0-epoch</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>split_0-lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_0-train_loss_epoch</td><td>█▅▃▃▃▂▂▂▂▂▂▂▁▂▂▁▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_0-train_loss_step</td><td>█▅▆▃▃▄▂▂▃▃▂▂▃▃▂▂▃▂▃▂▂▃▃▂▂▂▁▂▃▂▂▂▁▁▂▁▂▁▁▁</td></tr><tr><td>split_0-val/mae</td><td>▇█▅▅▃▁▂▁▄▃▄▃▃▃▃▂▃▃▃▂▂▃▃▃▃▃▃▂▂▃▂▂▄▃▃▃▂▂▂▂</td></tr><tr><td>split_0-val/r2</td><td>▁▂▅▆▆▇▇▇█▅▆▅▆▇▆▆▆▆▇▅▆▆▆▆▆▇▆█▇▇▇▆▇▆▆▆▆▅▇▆</td></tr><tr><td>split_0-val_loss</td><td>██▂▂▃▃▂▄▆▃▃▃▃▃▄▂▃▄▄▃▃▃▄▃▃▄▃▁▂▃▃▄▃▄▄▅▄▅▂▄</td></tr><tr><td>split_1-epoch</td><td>▁▁▁▁▂▂▃▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>split_1-lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_1-train_loss_epoch</td><td>█▄▃▂▃▂▂▂▂▂▁▁▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_1-train_loss_step</td><td>█▆▄▃▄▂▃▂▃▄▂▂▃▃▃▃▂▂▂▂▁▂▁▁▃▁▂▂▁▂▁▂▁▁▂▃▁▁▂▃</td></tr><tr><td>split_1-val/mae</td><td>█▆▃▄▅▃▃▅▃▂▂▂▃▂▁▂▁▃▃▃▃▄▂▂▂▁▂▂▃▂▂▂▂▂▄▃▂▂▂▃</td></tr><tr><td>split_1-val/r2</td><td>▃▁▃▄▂▅▃▃▅▄▇▇▃▅▄▆▆▆█▅▆▇▇▅▅▆▆▅▆█▆▄▇▅▇▇█▇▆▆</td></tr><tr><td>split_1-val_loss</td><td>▅▅▇█▅▂▅▅▂▄▄▄▃▄▃▄▃▄▆▅▃▅▅▅▃▃▂▁▅▃▂▂▄▄▂▄▂▃▃▂</td></tr><tr><td>split_2-epoch</td><td>▁▁▁▁▁▂▂▂▂▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇██</td></tr><tr><td>split_2-lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_2-train_loss_epoch</td><td>█▇▅▃▃▂▃▂▂▂▂▂▂▂▂▂▁▂▂▁▁▁▁▁▁▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_2-train_loss_step</td><td>█▆▄▄▂▃▃▂▂▂▂▂▂▂▁▃▂▂▂▂▁▂▂▂▂▂▂▂▂▁▁▁▂▂▂▃▁▂▁▂</td></tr><tr><td>split_2-val/mae</td><td>█▆▆▆▄▃▄▃▃▃▃▃▃▂▁▂▂▂▂▁▁▂▃▂▃▂▃▃▂▂▃▃▄▃▂▂▃▃▃▃</td></tr><tr><td>split_2-val/r2</td><td>▁▇▆▆▆▆▇▇█▇███▇▇▇▇▇██▇▇████▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>split_2-val_loss</td><td>█▆▄▃▃▄▃▃▃▃▃▂▂▁▁▁▂▂▂▂▂▂▂▁▁▂▁▂▁▂▂▃▂▃▃▂▃▃▂▃</td></tr><tr><td>split_3-epoch</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>split_3-lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_3-train_loss_epoch</td><td>▇█▆▅▄▄▃▃▃▄▃▃▄▃▃▄▃▄▃▃▃▄▃▂▂▂▂▃▂▂▂▂▂▄▂▂▁▁▁▂</td></tr><tr><td>split_3-train_loss_step</td><td>█▅▃█▂▃▃▃▂▃▂▃▃▂▃▂▂▁▂▃▁▂▂▁▁▂▁▂▂▂▁▁▂▄▁▃▁▂▂▁</td></tr><tr><td>split_3-val/mae</td><td>█▄▃▃▁▂▂▂▂▂▂▂▃▂▂▂▂▂▂▂▂▃▂▂▂▂▂▂▂▂▂▂▂▁▂▁▂▁▁▂</td></tr><tr><td>split_3-val/r2</td><td>▁▄▅▃▅▇▇▆▇▆▆▆▇▇▇▆▆▆▇▆▆▇▆▆▆▇▇▆▇▆▆▇▆▇▇▇▇▇█▇</td></tr><tr><td>split_3-val_loss</td><td>█▇▄▄▂▁▁▂▂▂▂▂▁▂▂▃▂▂▂▂▂▂▃▂▂▂▂▂▂▁▁▂▂▂▂▁▁▁▁▁</td></tr><tr><td>split_4-epoch</td><td>▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>split_4-lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_4-train_loss_epoch</td><td>█▅▅▅▄▄▃▃▃▂▂▂▂▂▂▂▂▂▁▁▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▂▁▁▁</td></tr><tr><td>split_4-train_loss_step</td><td>█▄▃▄▄▃▄▃▂▂▃▃▃▃▂▂▂▂▁▂▂▂▂▂▂▁▂▃▂▂▁▁▁▁▁▁▂▁▂▁</td></tr><tr><td>split_4-val/mae</td><td>█▃▃▅▃▃▄▄▃▂▂▂▃▃▃▃▃▃▃▂▁▂▂▂▁▂▂▂▁▂▂▁▂▂▃▂▂▄▂▂</td></tr><tr><td>split_4-val/r2</td><td>▁▄▆▅▅▅▆▇▆▆▆▆▆▅▇▆▆▆▇▆▆▆▇▇▇▇▇▇▇█▆▆▇▇▆▆▆▅▇▇</td></tr><tr><td>split_4-val_loss</td><td>▅▅▅█▄▃▄▄▃▃▄▃▃▂▃▃▃▂▅▄▄▂▂▃▂▂▁▃▂▃▃▃▁▁▂▃▂▂▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▃▄▅▅▆▆▇█▁▂▃▅▅▆▇▇▇▂▂▂▄▅▁▂▅▅▅▇██▁▄▅▆▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>split_0-epoch</td><td>199</td></tr><tr><td>split_0-lr</td><td>0.0001</td></tr><tr><td>split_0-train_loss_epoch</td><td>0.06252</td></tr><tr><td>split_0-train_loss_step</td><td>0.04133</td></tr><tr><td>split_0-val/mae</td><td>0.39546</td></tr><tr><td>split_0-val/r2</td><td>0.66972</td></tr><tr><td>split_0-val_loss</td><td>0.31512</td></tr><tr><td>split_1-epoch</td><td>199</td></tr><tr><td>split_1-lr</td><td>0.0001</td></tr><tr><td>split_1-train_loss_epoch</td><td>0.06819</td></tr><tr><td>split_1-train_loss_step</td><td>0.15922</td></tr><tr><td>split_1-val/mae</td><td>0.39466</td></tr><tr><td>split_1-val/r2</td><td>0.66386</td></tr><tr><td>split_1-val_loss</td><td>0.32934</td></tr><tr><td>split_2-epoch</td><td>199</td></tr><tr><td>split_2-lr</td><td>0.0001</td></tr><tr><td>split_2-train_loss_epoch</td><td>0.06011</td></tr><tr><td>split_2-train_loss_step</td><td>0.07684</td></tr><tr><td>split_2-val/mae</td><td>0.39854</td></tr><tr><td>split_2-val/r2</td><td>0.67834</td></tr><tr><td>split_2-val_loss</td><td>0.32669</td></tr><tr><td>split_3-epoch</td><td>199</td></tr><tr><td>split_3-lr</td><td>0.0001</td></tr><tr><td>split_3-train_loss_epoch</td><td>0.0583</td></tr><tr><td>split_3-train_loss_step</td><td>0.041</td></tr><tr><td>split_3-val/mae</td><td>0.37131</td></tr><tr><td>split_3-val/r2</td><td>0.66952</td></tr><tr><td>split_3-val_loss</td><td>0.31678</td></tr><tr><td>split_4-epoch</td><td>199</td></tr><tr><td>split_4-lr</td><td>0.0001</td></tr><tr><td>split_4-train_loss_epoch</td><td>0.04713</td></tr><tr><td>split_4-train_loss_step</td><td>0.05378</td></tr><tr><td>split_4-val/mae</td><td>0.33075</td></tr><tr><td>split_4-val/r2</td><td>0.75099</td></tr><tr><td>split_4-val_loss</td><td>0.22422</td></tr><tr><td>trainer/global_step</td><td>2199</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">roberta_run_0</strong> at: <a href='https://wandb.ai/vladvin-org/admet-challenge/runs/0nddejpr' target=\"_blank\">https://wandb.ai/vladvin-org/admet-challenge/runs/0nddejpr</a><br> View project at: <a href='https://wandb.ai/vladvin-org/admet-challenge' target=\"_blank\">https://wandb.ai/vladvin-org/admet-challenge</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>../wandb/roberta_run_0/wandb/run-20250313_030054-0nddejpr/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_and_eval(input_paths, save_dirs, RUN_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up + run 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(input_paths, save_dirs, output_dir, remove_worst_pct):\n",
    "    smiles_to_remove = defaultdict(set)\n",
    "\n",
    "    for input_path, save_dir in zip(input_paths, save_dirs):\n",
    "        input_df = pd.read_csv(input_path)\n",
    "        input_val_df = input_df[input_df[\"split\"] == \"val\"]\n",
    "        output_df = pd.read_csv(save_dir / \"predictions.csv\")\n",
    "        output_val_df = output_df[input_df[\"split\"] == \"val\"]\n",
    "\n",
    "        for t in TARGET_COLUMNS:\n",
    "            # Sort by absolute error\n",
    "            notna_mask = input_val_df[t].notna()\n",
    "            input_val_df = input_val_df[notna_mask]\n",
    "            output_val_df = output_val_df[notna_mask]\n",
    "\n",
    "            mae = np.abs(input_val_df[t] - output_val_df[f\"pred_{t}\"])\n",
    "            sorted_idx = np.argsort(mae)[::-1]\n",
    "            smiles_to_remove[t].update(\n",
    "                input_val_df.iloc[sorted_idx[:int(remove_worst_pct * len(sorted_idx))]][\"cxsmiles_std\"].tolist()\n",
    "            )\n",
    "\n",
    "    for input_path in input_paths:\n",
    "        input_df = pd.read_csv(input_path)\n",
    "        for t in TARGET_COLUMNS:\n",
    "            input_df.loc[input_df[\"cxsmiles_std\"].isin(smiles_to_remove[t]) & (input_df[\"split\"] == \"train\"), t] = np.nan\n",
    "\n",
    "        input_df.to_csv(output_dir / input_path.name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path(\"../output/asap/rnd_splits/roberta/run_0/cleaned\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data(input_paths, save_dirs, output_dir, remove_worst_pct = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogHLM            150\n",
       "LogMLM            140\n",
       "LogD               86\n",
       "LogKSOL            74\n",
       "LogMDR1-MDCKII     15\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(input_paths[0])[TARGET_COLUMNS].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogHLM            189\n",
       "LogMLM            173\n",
       "LogD              116\n",
       "LogKSOL            99\n",
       "LogMDR1-MDCKII     40\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(output_dir / input_paths[0].name)[TARGET_COLUMNS].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_paths = [Path(f'../output/asap/rnd_splits/roberta/run_0/cleaned/split_{k}.csv') for k in range(5)]\n",
    "save_dirs = [Path(f'../output/asap/rnd_splits/roberta/run_1/split_{k}') for k in range(5)]\n",
    "RUN_IDX = \"1_clean_worst_pct_0.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predicting on ../output/asap/rnd_splits/roberta/run_0/cleaned/split_0.csv\n",
      "1 out of 323 rows are removed due to missing values\n",
      "Downloading checkpoint from <gs_bucket>/artifacts/tokenizers/zinc...\n",
      "0 out of 81 rows are removed due to missing values\n",
      "Downloading checkpoint from <gs_bucket>/artifacts/tokenizers/zinc...\n",
      "Downloading checkpoint from <gs_bucket>/artifacts/tokenizers/zinc...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Path ../wandb/roberta_run_1_clean_worst_pct_0.2/wandb/ wasn't writable, using system temp directory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting checkpoint from /home/jupyter/AgenticADMET/notebooks/../output/artifacts/mol_mlm_roberta_zinc/last.ckpt...\n",
      "Loading checkpoint from roberta to roberta...\n",
      "<All keys matched successfully>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/var/tmp/wandb/run-20250313_031800-16am8nkt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vladvin-org/admet-challenge/runs/16am8nkt' target=\"_blank\">roberta_run_1_clean_worst_pct_0.2</a></strong> to <a href='https://wandb.ai/vladvin-org/admet-challenge' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/vladvin-org/admet-challenge' target=\"_blank\">https://wandb.ai/vladvin-org/admet-challenge</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/vladvin-org/admet-challenge/runs/16am8nkt' target=\"_blank\">https://wandb.ai/vladvin-org/admet-challenge/runs/16am8nkt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type         | Params | Mode \n",
      "---------------------------------------------------\n",
      "0 | roberta   | RobertaModel | 8.7 M  | train\n",
      "1 | predictor | MLP          | 149 K  | train\n",
      "2 | criterion | MSE          | 0      | train\n",
      "3 | metrics   | ModuleList   | 0      | train\n",
      "---------------------------------------------------\n",
      "8.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "8.8 M     Total params\n",
      "35.297    Total estimated model params size (MB)\n",
      "128       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (11) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.1861880703660957,\n",
      "    \"r2\": 0.8391420174084648\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.1908893015533692,\n",
      "    \"r2\": 0.6894910349779328\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.18807809859908095,\n",
      "    \"r2\": 0.9608093818258516\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.12862705999906573,\n",
      "    \"r2\": 0.775925789907343\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.2190807342436571,\n",
      "    \"r2\": 0.6857194162123077\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.18257265295225372,\n",
      "    \"macro_r2\": 0.7902175280663799\n",
      "  }\n",
      "}\n",
      "\n",
      "Val metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.35872086094375744,\n",
      "    \"r2\": 0.4466622297405026\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.33513700619755515,\n",
      "    \"r2\": 0.3943504962569053\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.5637464449444756,\n",
      "    \"r2\": 0.6422735980639326\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.251403870991386,\n",
      "    \"r2\": 0.40430013468538895\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.37214802811571895,\n",
      "    \"r2\": 0.427258471681656\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.3762312422385786,\n",
      "    \"macro_r2\": 0.46296898608567705\n",
      "  }\n",
      "}\n",
      "Training and predicting on ../output/asap/rnd_splits/roberta/run_0/cleaned/split_1.csv\n",
      "1 out of 323 rows are removed due to missing values\n",
      "Downloading checkpoint from <gs_bucket>/artifacts/tokenizers/zinc...\n",
      "0 out of 81 rows are removed due to missing values\n",
      "Downloading checkpoint from <gs_bucket>/artifacts/tokenizers/zinc...\n",
      "Downloading checkpoint from <gs_bucket>/artifacts/tokenizers/zinc...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type         | Params | Mode \n",
      "---------------------------------------------------\n",
      "0 | roberta   | RobertaModel | 8.7 M  | train\n",
      "1 | predictor | MLP          | 149 K  | train\n",
      "2 | criterion | MSE          | 0      | train\n",
      "3 | metrics   | ModuleList   | 0      | train\n",
      "---------------------------------------------------\n",
      "8.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "8.8 M     Total params\n",
      "35.297    Total estimated model params size (MB)\n",
      "128       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting checkpoint from /home/jupyter/AgenticADMET/notebooks/../output/artifacts/mol_mlm_roberta_zinc/last.ckpt...\n",
      "Loading checkpoint from roberta to roberta...\n",
      "<All keys matched successfully>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (11) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.16721194062419129,\n",
      "    \"r2\": 0.7940058914717372\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.20520051848674017,\n",
      "    \"r2\": 0.6376883139012195\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.17506692140891744,\n",
      "    \"r2\": 0.9677546793374956\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.1164217988984394,\n",
      "    \"r2\": 0.7841876208170787\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.21701591204787996,\n",
      "    \"r2\": 0.6548718454690614\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.17618341829323364,\n",
      "    \"macro_r2\": 0.7677016701993185\n",
      "  }\n",
      "}\n",
      "\n",
      "Val metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.3519687553851859,\n",
      "    \"r2\": 0.3483658417976192\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.2863231141084363,\n",
      "    \"r2\": 0.6465202454592247\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.5637253118983725,\n",
      "    \"r2\": 0.6298274113152076\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.2093833709346587,\n",
      "    \"r2\": 0.47561516835853945\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.38895571523924827,\n",
      "    \"r2\": 0.4218462373544195\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.3600712535131803,\n",
      "    \"macro_r2\": 0.5044349808570021\n",
      "  }\n",
      "}\n",
      "Training and predicting on ../output/asap/rnd_splits/roberta/run_0/cleaned/split_2.csv\n",
      "2 out of 323 rows are removed due to missing values\n",
      "Downloading checkpoint from <gs_bucket>/artifacts/tokenizers/zinc...\n",
      "0 out of 81 rows are removed due to missing values\n",
      "Downloading checkpoint from <gs_bucket>/artifacts/tokenizers/zinc...\n",
      "Downloading checkpoint from <gs_bucket>/artifacts/tokenizers/zinc...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type         | Params | Mode \n",
      "---------------------------------------------------\n",
      "0 | roberta   | RobertaModel | 8.7 M  | train\n",
      "1 | predictor | MLP          | 149 K  | train\n",
      "2 | criterion | MSE          | 0      | train\n",
      "3 | metrics   | ModuleList   | 0      | train\n",
      "---------------------------------------------------\n",
      "8.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "8.8 M     Total params\n",
      "35.297    Total estimated model params size (MB)\n",
      "128       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting checkpoint from /home/jupyter/AgenticADMET/notebooks/../output/artifacts/mol_mlm_roberta_zinc/last.ckpt...\n",
      "Loading checkpoint from roberta to roberta...\n",
      "<All keys matched successfully>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (11) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.16687742969039165,\n",
      "    \"r2\": 0.7793437744404141\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.1979477858244393,\n",
      "    \"r2\": 0.6791301266878864\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.1430160059992756,\n",
      "    \"r2\": 0.9767994349492954\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.11508012927661541,\n",
      "    \"r2\": 0.8182881614642596\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.21042692909516053,\n",
      "    \"r2\": 0.6736467483302966\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.16666965597717648,\n",
      "    \"macro_r2\": 0.7854416491744305\n",
      "  }\n",
      "}\n",
      "\n",
      "Val metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.38089361750573253,\n",
      "    \"r2\": 0.3766956136726052\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.3296096560003307,\n",
      "    \"r2\": 0.43105501926763756\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.6151401022076606,\n",
      "    \"r2\": 0.6138632676973725\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.19496424406707838,\n",
      "    \"r2\": 0.576959867397232\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.3458251778857903,\n",
      "    \"r2\": 0.409010872247904\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.3732865595333185,\n",
      "    \"macro_r2\": 0.4815169280565502\n",
      "  }\n",
      "}\n",
      "Training and predicting on ../output/asap/rnd_splits/roberta/run_0/cleaned/split_3.csv\n",
      "2 out of 323 rows are removed due to missing values\n",
      "Downloading checkpoint from <gs_bucket>/artifacts/tokenizers/zinc...\n",
      "0 out of 81 rows are removed due to missing values\n",
      "Downloading checkpoint from <gs_bucket>/artifacts/tokenizers/zinc...\n",
      "Downloading checkpoint from <gs_bucket>/artifacts/tokenizers/zinc...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type         | Params | Mode \n",
      "---------------------------------------------------\n",
      "0 | roberta   | RobertaModel | 8.7 M  | train\n",
      "1 | predictor | MLP          | 149 K  | train\n",
      "2 | criterion | MSE          | 0      | train\n",
      "3 | metrics   | ModuleList   | 0      | train\n",
      "---------------------------------------------------\n",
      "8.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "8.8 M     Total params\n",
      "35.297    Total estimated model params size (MB)\n",
      "128       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting checkpoint from /home/jupyter/AgenticADMET/notebooks/../output/artifacts/mol_mlm_roberta_zinc/last.ckpt...\n",
      "Loading checkpoint from roberta to roberta...\n",
      "<All keys matched successfully>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (11) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.1916461688694457,\n",
      "    \"r2\": 0.7820991165737339\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.26648661104413385,\n",
      "    \"r2\": 0.5205534734179863\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.3608438359221,\n",
      "    \"r2\": 0.8853724671553\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.12682683600030822,\n",
      "    \"r2\": 0.7929192216773814\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.24735903413867785,\n",
      "    \"r2\": 0.6621351792955923\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.2386324971949331,\n",
      "    \"macro_r2\": 0.7286158916239988\n",
      "  }\n",
      "}\n",
      "\n",
      "Val metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.32558674179347524,\n",
      "    \"r2\": 0.4346933022514592\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.3829194125486838,\n",
      "    \"r2\": 0.2783141183859964\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.6665370801687241,\n",
      "    \"r2\": 0.49101399629863407\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.24928068616697638,\n",
      "    \"r2\": 0.18454591198170023\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.41198637448991704,\n",
      "    \"r2\": 0.13892186603258427\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.40726205903355533,\n",
      "    \"macro_r2\": 0.3054978389900748\n",
      "  }\n",
      "}\n",
      "Training and predicting on ../output/asap/rnd_splits/roberta/run_0/cleaned/split_4.csv\n",
      "2 out of 324 rows are removed due to missing values\n",
      "Downloading checkpoint from <gs_bucket>/artifacts/tokenizers/zinc...\n",
      "0 out of 80 rows are removed due to missing values\n",
      "Downloading checkpoint from <gs_bucket>/artifacts/tokenizers/zinc...\n",
      "Downloading checkpoint from <gs_bucket>/artifacts/tokenizers/zinc...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type         | Params | Mode \n",
      "---------------------------------------------------\n",
      "0 | roberta   | RobertaModel | 8.7 M  | train\n",
      "1 | predictor | MLP          | 149 K  | train\n",
      "2 | criterion | MSE          | 0      | train\n",
      "3 | metrics   | ModuleList   | 0      | train\n",
      "---------------------------------------------------\n",
      "8.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "8.8 M     Total params\n",
      "35.297    Total estimated model params size (MB)\n",
      "128       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting checkpoint from /home/jupyter/AgenticADMET/notebooks/../output/artifacts/mol_mlm_roberta_zinc/last.ckpt...\n",
      "Loading checkpoint from roberta to roberta...\n",
      "<All keys matched successfully>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (11) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.20594558012252492,\n",
      "    \"r2\": 0.7471845358715377\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.1983428200696025,\n",
      "    \"r2\": 0.6458137340257575\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.17462980678468418,\n",
      "    \"r2\": 0.9682475988525927\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.09449591768654753,\n",
      "    \"r2\": 0.8353163475080205\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.19677022944513534,\n",
      "    \"r2\": 0.6361286880435564\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.17403687082169889,\n",
      "    \"macro_r2\": 0.766538180860293\n",
      "  }\n",
      "}\n",
      "\n",
      "Val metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.26153112658020516,\n",
      "    \"r2\": 0.773941659086051\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.27148158146499257,\n",
      "    \"r2\": 0.5707034832706072\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.5877801813234885,\n",
      "    \"r2\": 0.5535538397222723\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.19545590712410288,\n",
      "    \"r2\": 0.505989661283273\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.3616734796500182,\n",
      "    \"r2\": 0.4952754061197343\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.3355844552285615,\n",
      "    \"macro_r2\": 0.5798928098963876\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>split_0-epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>split_0-lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_0-train_loss_epoch</td><td>█▆▄▂▂▃▂▂▂▂▂▂▁▂▂▁▁▁▂▂▁▁▂▁▂▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁</td></tr><tr><td>split_0-train_loss_step</td><td>▇█▄▃▃▄▄▃▃▃▃▃▃▃▃▃▃▂▄▂▃▂▂▂▂▂▂▂▂▂▂▃▂▂▂▂▂▂▂▁</td></tr><tr><td>split_0-val/mae</td><td>█▅▅▃▁▂▅▂▂▃▂▁▂▃▂▃▃▅▂▂▂▃▄▃▄▂▄▂▂▃▂▁▂▁▁▂▁▁▂▄</td></tr><tr><td>split_0-val/r2</td><td>▁▃▃▃▃▃▆▄▆▆▆▆▆▇▅▆▆▆▇▂▅▆▇▆▆▅▆▆▆▇▇▆██▇▇█▇█▇</td></tr><tr><td>split_0-val_loss</td><td>█▅▄▃▃▃▂▂▂▃▂▂▂▂▃▁▁▂▁▂▃▂▂▂▂▂▁▂▂▂▁▁▁▁▁▁▂▂▂▁</td></tr><tr><td>split_1-epoch</td><td>▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>split_1-lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_1-train_loss_epoch</td><td>█▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▁▁▁▁▁</td></tr><tr><td>split_1-train_loss_step</td><td>█▆▇▅▆▃▃▃▃▃▁▁▃▃▂▃▃▃▂▄▂▁▂▂▂▁▂▂▁▂▁▁▄▁▃▁▁▂▁▁</td></tr><tr><td>split_1-val/mae</td><td>█▆▃▃▃▂▂▂▂▁▂▁▂▁▂▂▁▂▂▂▂▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▂▁</td></tr><tr><td>split_1-val/r2</td><td>▁▂▃▄▅▆▅▆▆▇▆▆██▆█▇▇▇▆▇█▇██▇▇▇▅▇▇▆▆▇▇▇▇▇██</td></tr><tr><td>split_1-val_loss</td><td>█▇▇▇▇▅▃▅▄▄▁▆▃▂▃▅▄▃▂▁▃▂▃▃▃▃▃▃▆▅█▆▄▅▃▃▁▂▃▃</td></tr><tr><td>split_2-epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>split_2-lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_2-train_loss_epoch</td><td>█▆▅▂▃▃▃▂▂▂▂▁▁▁▂▁▂▂▂▂▂▃▂▁▂▁▁▂▁▂▁▁▂▁▁▁▁▁▁▁</td></tr><tr><td>split_2-train_loss_step</td><td>█▅▄▃▄▃▄▄▄▃▂▃▄▃▂▂▂▄▂▂▂▂▃▂▂▂▂▃▁▂▃▂▂▂▂▂▃▂▂▁</td></tr><tr><td>split_2-val/mae</td><td>█▇▆▅▅▄▄▅▃▂▇▅▄▄▃▂▃▆▂▃▂▄▇▃▂▅▃▃▂▂▂▁▁▂▃▃▂▂▁▂</td></tr><tr><td>split_2-val/r2</td><td>▅▄▆▄▃▄▅▅▁▄▄▅▇▅▄▇▄▅▆▆▄█▆▆▃▅▆▄▅▇▅▄▄▆▆▆▆▇▇█</td></tr><tr><td>split_2-val_loss</td><td>█▅▃▂▃▃▃▃▄▃▃▂▃▂▂▃▂▂▃▂▂▂▂▃▃▃▂▃▂▂▂▂▂▂▂▂▂▂▁▁</td></tr><tr><td>split_3-epoch</td><td>▁▁▁▂▂▂▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>split_3-lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_3-train_loss_epoch</td><td>█▃▂▂▄▂▂▂▃▂▂▂▂▂▂▁▁▁▁▂▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁</td></tr><tr><td>split_3-train_loss_step</td><td>█▄▂▃▃▂▃▂▃▃▂▃▃▃▂▂▂▂▃▃▂▂▂▃▂▂▂▂▂▂▂▃▂▂▂▂▂▂▂▁</td></tr><tr><td>split_3-val/mae</td><td>█▄▃▂▃▄▃▂▃▂▂▂▃▃▄▂▂▂▃▅▃▃▄▃▃▂▃▂▁▂▂▁▁▂▂▂▂▄▂▃</td></tr><tr><td>split_3-val/r2</td><td>▁▄▆▇▆▇▇▇█▆▇▆▅▇▇▆▆▅▆▄▆▆▇▇▇██▇▇▆▇▆▇▇█▇▇▇█▇</td></tr><tr><td>split_3-val_loss</td><td>▂▂▁█▄▃▄▂▄▄▆▆▃▅▄▄█▄▇▄▄▂▅▂▂▄▅▃▄▂▂▂▂▂▂▃▃▂▄▂</td></tr><tr><td>split_4-epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇██</td></tr><tr><td>split_4-lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_4-train_loss_epoch</td><td>█▃▃▂▂▂▂▂▂▂▁▂▂▁▂▁▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_4-train_loss_step</td><td>█▄▆▄▃▃▃▅▃▃▂▃▃▃▂▃▃▃▂▂▃▂▁▁▂▂▂▂▂▁▂▁▂▂▂▂▁▂▃▁</td></tr><tr><td>split_4-val/mae</td><td>█▇▆▃▂▃▂▂▂▃▂▂▂▁▃▂▂▂▂▂▂▂▂▂▂▃▂▂▂▂▃▁▁▂▁▁▂▂▃▁</td></tr><tr><td>split_4-val/r2</td><td>▁▅▆▆▆▇▇▇▇▇▆▇▇▇▇▇███▇▇▇▇█▇████▇████▇▇▇▇▇▇</td></tr><tr><td>split_4-val_loss</td><td>▃█▂▃▆▂▁▁▂▅▃▆▁▃▁▂▃▁▂▃▁▁▂▂▁▁▂▁▂▄▄▄▃▂▃▃▄▃▂▄</td></tr><tr><td>trainer/global_step</td><td>▃▄▄▅▇▇██▁▂▅▅▅▆▆▁▁▂▂▃▃▄▅▆▆▇█▂▂▆██▁▁▁▃▅▆▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>split_0-epoch</td><td>199</td></tr><tr><td>split_0-lr</td><td>0.0001</td></tr><tr><td>split_0-train_loss_epoch</td><td>0.0511</td></tr><tr><td>split_0-train_loss_step</td><td>0.01009</td></tr><tr><td>split_0-val/mae</td><td>0.37099</td></tr><tr><td>split_0-val/r2</td><td>0.71815</td></tr><tr><td>split_0-val_loss</td><td>0.26517</td></tr><tr><td>split_1-epoch</td><td>199</td></tr><tr><td>split_1-lr</td><td>0.0001</td></tr><tr><td>split_1-train_loss_epoch</td><td>0.03776</td></tr><tr><td>split_1-train_loss_step</td><td>0.04465</td></tr><tr><td>split_1-val/mae</td><td>0.35719</td></tr><tr><td>split_1-val/r2</td><td>0.72274</td></tr><tr><td>split_1-val_loss</td><td>0.26918</td></tr><tr><td>split_2-epoch</td><td>199</td></tr><tr><td>split_2-lr</td><td>0.0001</td></tr><tr><td>split_2-train_loss_epoch</td><td>0.05581</td></tr><tr><td>split_2-train_loss_step</td><td>0.02594</td></tr><tr><td>split_2-val/mae</td><td>0.36756</td></tr><tr><td>split_2-val/r2</td><td>0.7237</td></tr><tr><td>split_2-val_loss</td><td>0.28255</td></tr><tr><td>split_3-epoch</td><td>199</td></tr><tr><td>split_3-lr</td><td>0.0001</td></tr><tr><td>split_3-train_loss_epoch</td><td>0.05712</td></tr><tr><td>split_3-train_loss_step</td><td>0.00234</td></tr><tr><td>split_3-val/mae</td><td>0.39962</td></tr><tr><td>split_3-val/r2</td><td>0.65945</td></tr><tr><td>split_3-val_loss</td><td>0.32389</td></tr><tr><td>split_4-epoch</td><td>199</td></tr><tr><td>split_4-lr</td><td>0.0001</td></tr><tr><td>split_4-train_loss_epoch</td><td>0.0456</td></tr><tr><td>split_4-train_loss_step</td><td>0.02726</td></tr><tr><td>split_4-val/mae</td><td>0.32977</td></tr><tr><td>split_4-val/r2</td><td>0.76065</td></tr><tr><td>split_4-val_loss</td><td>0.22216</td></tr><tr><td>trainer/global_step</td><td>2199</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">roberta_run_1_clean_worst_pct_0.2</strong> at: <a href='https://wandb.ai/vladvin-org/admet-challenge/runs/16am8nkt' target=\"_blank\">https://wandb.ai/vladvin-org/admet-challenge/runs/16am8nkt</a><br> View project at: <a href='https://wandb.ai/vladvin-org/admet-challenge' target=\"_blank\">https://wandb.ai/vladvin-org/admet-challenge</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>../wandb/roberta_run_1_clean_worst_pct_0.2/wandb/run-20250313_031800-16am8nkt/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_and_eval(input_paths, save_dirs, RUN_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up + run 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path(\"../output/asap/rnd_splits/roberta/run_1/cleaned\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data(input_paths, save_dirs, output_dir, remove_worst_pct = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_paths = [Path(f'../output/asap/rnd_splits/roberta/run_1/cleaned/split_{k}.csv') for k in range(5)]\n",
    "save_dirs = [Path(f'../output/asap/rnd_splits/roberta/run_2/split_{k}') for k in range(5)]\n",
    "RUN_IDX = \"2_clean_worst_pct_0.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predicting on ../output/asap/rnd_splits/roberta/run_1/cleaned/split_0.csv\n",
      "3 out of 323 rows are removed due to missing values\n",
      "Downloading checkpoint from <gs_bucket>/artifacts/tokenizers/zinc...\n",
      "0 out of 81 rows are removed due to missing values\n",
      "Downloading checkpoint from <gs_bucket>/artifacts/tokenizers/zinc...\n",
      "Downloading checkpoint from <gs_bucket>/artifacts/tokenizers/zinc...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Path ../wandb/roberta_run_2_clean_worst_pct_0.2/wandb/ wasn't writable, using system temp directory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting checkpoint from /home/jupyter/AgenticADMET/notebooks/../output/artifacts/mol_mlm_roberta_zinc/last.ckpt...\n",
      "Loading checkpoint from roberta to roberta...\n",
      "<All keys matched successfully>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/var/tmp/wandb/run-20250313_033545-uwo8ed43</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vladvin-org/admet-challenge/runs/uwo8ed43' target=\"_blank\">roberta_run_2_clean_worst_pct_0.2</a></strong> to <a href='https://wandb.ai/vladvin-org/admet-challenge' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/vladvin-org/admet-challenge' target=\"_blank\">https://wandb.ai/vladvin-org/admet-challenge</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/vladvin-org/admet-challenge/runs/uwo8ed43' target=\"_blank\">https://wandb.ai/vladvin-org/admet-challenge/runs/uwo8ed43</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type         | Params | Mode \n",
      "---------------------------------------------------\n",
      "0 | roberta   | RobertaModel | 8.7 M  | train\n",
      "1 | predictor | MLP          | 149 K  | train\n",
      "2 | criterion | MSE          | 0      | train\n",
      "3 | metrics   | ModuleList   | 0      | train\n",
      "---------------------------------------------------\n",
      "8.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "8.8 M     Total params\n",
      "35.297    Total estimated model params size (MB)\n",
      "128       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (10) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.1688764883476707,\n",
      "    \"r2\": 0.810708980990781\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.21892033010725034,\n",
      "    \"r2\": 0.6044276024825277\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.16438498869750173,\n",
      "    \"r2\": 0.9726207727307689\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.0830587430861154,\n",
      "    \"r2\": 0.8390308523204506\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.21806808120499632,\n",
      "    \"r2\": 0.6105358902473206\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.17066172628870688,\n",
      "    \"macro_r2\": 0.7674648197543699\n",
      "  }\n",
      "}\n",
      "\n",
      "Val metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.31923898589557,\n",
      "    \"r2\": 0.490842814061037\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.41090528139300986,\n",
      "    \"r2\": 0.05414296687545972\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.5605308732927822,\n",
      "    \"r2\": 0.5977805934594193\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.24737459971764894,\n",
      "    \"r2\": 0.27236219014480856\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.35850982274140836,\n",
      "    \"r2\": 0.4387014381089629\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.37931191260808383,\n",
      "    \"macro_r2\": 0.37076600052993747\n",
      "  }\n",
      "}\n",
      "Training and predicting on ../output/asap/rnd_splits/roberta/run_1/cleaned/split_1.csv\n",
      "2 out of 323 rows are removed due to missing values\n",
      "Downloading checkpoint from <gs_bucket>/artifacts/tokenizers/zinc...\n",
      "0 out of 81 rows are removed due to missing values\n",
      "Downloading checkpoint from <gs_bucket>/artifacts/tokenizers/zinc...\n",
      "Downloading checkpoint from <gs_bucket>/artifacts/tokenizers/zinc...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type         | Params | Mode \n",
      "---------------------------------------------------\n",
      "0 | roberta   | RobertaModel | 8.7 M  | train\n",
      "1 | predictor | MLP          | 149 K  | train\n",
      "2 | criterion | MSE          | 0      | train\n",
      "3 | metrics   | ModuleList   | 0      | train\n",
      "---------------------------------------------------\n",
      "8.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "8.8 M     Total params\n",
      "35.297    Total estimated model params size (MB)\n",
      "128       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting checkpoint from /home/jupyter/AgenticADMET/notebooks/../output/artifacts/mol_mlm_roberta_zinc/last.ckpt...\n",
      "Loading checkpoint from roberta to roberta...\n",
      "<All keys matched successfully>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (11) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.21847773128467338,\n",
      "    \"r2\": 0.7039749403747437\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.23758899482591825,\n",
      "    \"r2\": 0.5374173862791407\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.16002167430363204,\n",
      "    \"r2\": 0.9725841808609879\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.13783883449438258,\n",
      "    \"r2\": 0.6918611170370554\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.25262237338562316,\n",
      "    \"r2\": 0.5526316818041206\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.20130992165884587,\n",
      "    \"macro_r2\": 0.6916938612712097\n",
      "  }\n",
      "}\n",
      "\n",
      "Val metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.3299965061609436,\n",
      "    \"r2\": 0.40889901397028683\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.2863003751519956,\n",
      "    \"r2\": 0.631326630156547\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.5232384482703427,\n",
      "    \"r2\": 0.6638988949665213\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.20992483841001638,\n",
      "    \"r2\": 0.4976847727842081\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.3616977449114946,\n",
      "    \"r2\": 0.5657189791693104\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.3422315825809586,\n",
      "    \"macro_r2\": 0.5535056582093747\n",
      "  }\n",
      "}\n",
      "Training and predicting on ../output/asap/rnd_splits/roberta/run_1/cleaned/split_2.csv\n",
      "4 out of 323 rows are removed due to missing values\n",
      "Downloading checkpoint from <gs_bucket>/artifacts/tokenizers/zinc...\n",
      "0 out of 81 rows are removed due to missing values\n",
      "Downloading checkpoint from <gs_bucket>/artifacts/tokenizers/zinc...\n",
      "Downloading checkpoint from <gs_bucket>/artifacts/tokenizers/zinc...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type         | Params | Mode \n",
      "---------------------------------------------------\n",
      "0 | roberta   | RobertaModel | 8.7 M  | train\n",
      "1 | predictor | MLP          | 149 K  | train\n",
      "2 | criterion | MSE          | 0      | train\n",
      "3 | metrics   | ModuleList   | 0      | train\n",
      "---------------------------------------------------\n",
      "8.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "8.8 M     Total params\n",
      "35.297    Total estimated model params size (MB)\n",
      "128       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting checkpoint from /home/jupyter/AgenticADMET/notebooks/../output/artifacts/mol_mlm_roberta_zinc/last.ckpt...\n",
      "Loading checkpoint from roberta to roberta...\n",
      "<All keys matched successfully>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (10) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.20347404496082844,\n",
      "    \"r2\": 0.7370575929147101\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.2146639826103183,\n",
      "    \"r2\": 0.6077661260710183\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.09508561579386395,\n",
      "    \"r2\": 0.989610473488095\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.1446976282650623,\n",
      "    \"r2\": 0.7419845582176058\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.20717227170692,\n",
      "    \"r2\": 0.6930418217431803\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.1730187086673986,\n",
      "    \"macro_r2\": 0.753892114486922\n",
      "  }\n",
      "}\n",
      "\n",
      "Val metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.3847592966635836,\n",
      "    \"r2\": 0.40423241433066803\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.31479048916134517,\n",
      "    \"r2\": 0.45912295665292713\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.5582747313560861,\n",
      "    \"r2\": 0.6091206781052205\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.20097978764288948,\n",
      "    \"r2\": 0.5046630818685771\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.3410236912475341,\n",
      "    \"r2\": 0.4336942194174045\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.35996559921428767,\n",
      "    \"macro_r2\": 0.4821666700749594\n",
      "  }\n",
      "}\n",
      "Training and predicting on ../output/asap/rnd_splits/roberta/run_1/cleaned/split_3.csv\n",
      "4 out of 323 rows are removed due to missing values\n",
      "Downloading checkpoint from <gs_bucket>/artifacts/tokenizers/zinc...\n",
      "0 out of 81 rows are removed due to missing values\n",
      "Downloading checkpoint from <gs_bucket>/artifacts/tokenizers/zinc...\n",
      "Downloading checkpoint from <gs_bucket>/artifacts/tokenizers/zinc...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type         | Params | Mode \n",
      "---------------------------------------------------\n",
      "0 | roberta   | RobertaModel | 8.7 M  | train\n",
      "1 | predictor | MLP          | 149 K  | train\n",
      "2 | criterion | MSE          | 0      | train\n",
      "3 | metrics   | ModuleList   | 0      | train\n",
      "---------------------------------------------------\n",
      "8.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "8.8 M     Total params\n",
      "35.297    Total estimated model params size (MB)\n",
      "128       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting checkpoint from /home/jupyter/AgenticADMET/notebooks/../output/artifacts/mol_mlm_roberta_zinc/last.ckpt...\n",
      "Loading checkpoint from roberta to roberta...\n",
      "<All keys matched successfully>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (10) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.14789938085693285,\n",
      "    \"r2\": 0.7786160567069104\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.21081260798453366,\n",
      "    \"r2\": 0.62948593160018\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.15943958467763406,\n",
      "    \"r2\": 0.9740967990457774\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.11074609942349375,\n",
      "    \"r2\": 0.7689788269277718\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.1888147859737648,\n",
      "    \"r2\": 0.6900952854364407\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.16354249178327182,\n",
      "    \"macro_r2\": 0.768254579943416\n",
      "  }\n",
      "}\n",
      "\n",
      "Val metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.30299891862066314,\n",
      "    \"r2\": 0.46129102427188406\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.3466710136315049,\n",
      "    \"r2\": 0.4228847620472026\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.5847438396765635,\n",
      "    \"r2\": 0.538472871700707\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.23974460248718102,\n",
      "    \"r2\": 0.19917148348776592\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.36162868814684906,\n",
      "    \"r2\": 0.2565595161284747\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.36715741251255235,\n",
      "    \"macro_r2\": 0.37567593152720685\n",
      "  }\n",
      "}\n",
      "Training and predicting on ../output/asap/rnd_splits/roberta/run_1/cleaned/split_4.csv\n",
      "3 out of 324 rows are removed due to missing values\n",
      "Downloading checkpoint from <gs_bucket>/artifacts/tokenizers/zinc...\n",
      "0 out of 80 rows are removed due to missing values\n",
      "Downloading checkpoint from <gs_bucket>/artifacts/tokenizers/zinc...\n",
      "Downloading checkpoint from <gs_bucket>/artifacts/tokenizers/zinc...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type         | Params | Mode \n",
      "---------------------------------------------------\n",
      "0 | roberta   | RobertaModel | 8.7 M  | train\n",
      "1 | predictor | MLP          | 149 K  | train\n",
      "2 | criterion | MSE          | 0      | train\n",
      "3 | metrics   | ModuleList   | 0      | train\n",
      "---------------------------------------------------\n",
      "8.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "8.8 M     Total params\n",
      "35.297    Total estimated model params size (MB)\n",
      "128       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting checkpoint from /home/jupyter/AgenticADMET/notebooks/../output/artifacts/mol_mlm_roberta_zinc/last.ckpt...\n",
      "Loading checkpoint from roberta to roberta...\n",
      "<All keys matched successfully>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (11) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.24681134481419628,\n",
      "    \"r2\": 0.6644760513740046\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.283133056233829,\n",
      "    \"r2\": 0.4793869697721731\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.2369130607718645,\n",
      "    \"r2\": 0.9410693538636942\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.15792643543511825,\n",
      "    \"r2\": 0.6391591138866446\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.24836378005059304,\n",
      "    \"r2\": 0.6029419918234791\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.23462953546112023,\n",
      "    \"macro_r2\": 0.665406696143999\n",
      "  }\n",
      "}\n",
      "\n",
      "Val metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.270355116785023,\n",
      "    \"r2\": 0.74775974093329\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.29268326593558414,\n",
      "    \"r2\": 0.45909528491605134\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.6434580408831436,\n",
      "    \"r2\": 0.48287741382700056\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.26575679788468826,\n",
      "    \"r2\": 0.2496315833653031\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.3583735402671593,\n",
      "    \"r2\": 0.4782757009768187\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.36612535235111965,\n",
      "    \"macro_r2\": 0.48352794480369277\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>split_0-epoch</td><td>▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇███</td></tr><tr><td>split_0-lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_0-train_loss_epoch</td><td>█▆▆▄▆▄▅▂▄▃▃▃▃▃▃▂▂▂▃▂▃▂▂▂▂▂▂▂▂▂▂▁▂▁▂▁▂▁▁▂</td></tr><tr><td>split_0-train_loss_step</td><td>█▅▃▃▂▂▂▃▃▂▂▃▂▂▂▂▂▂▂▂▁▂▁▁▁▂▂▁▁▂▁▁▁▁▁▁▂▁▁▂</td></tr><tr><td>split_0-val/mae</td><td>█▄▃▃▃▂▂▂▂▃▂▂▂▂▃▂▂▂▃▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▂▂▂▂▂▁</td></tr><tr><td>split_0-val/r2</td><td>▁▄▇▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▆▇▇█▇▇▇█▇██████▇▇</td></tr><tr><td>split_0-val_loss</td><td>█▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▁▂▁▁▁▁▁▁</td></tr><tr><td>split_1-epoch</td><td>▁▁▁▂▂▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▄▄▄▅▅▆▆▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>split_1-lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_1-train_loss_epoch</td><td>█▄▃▃▃▃▃▂▂▂▂▂▃▂▂▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_1-train_loss_step</td><td>▇▆▆▅█▆▆▄▄▃▃▃▃▄▃▃▃▄▃▃▂▂▂▃▂▂▃▃▃▂▃▃▂▃▂▂▂▂▂▁</td></tr><tr><td>split_1-val/mae</td><td>█▇▇█▆▇▅▅▅▃▄▄▄▆▇▆▃▂▃▃▃▆▃▃▃▃▄▃▃▃▃▅▂▂▂▂▂▂▁▁</td></tr><tr><td>split_1-val/r2</td><td>▁▄▅▆▄▆▅▅▆▆▆▆▇▆▇▆▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇████████</td></tr><tr><td>split_1-val_loss</td><td>█▅▄▃▃▃▂▄▄▅▂▁▂▂▂▂▃▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▂▁</td></tr><tr><td>split_2-epoch</td><td>▁▁▁▁▂▂▃▃▃▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇██</td></tr><tr><td>split_2-lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_2-train_loss_epoch</td><td>█▄▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_2-train_loss_step</td><td>█▇▅▄▂▄▃▃▂▃▃▂▂▂▂▂▂▄▂▂▂▂▂▂▁▁▂▂▁▁▂▁▁▂▁▁▁▁▁▃</td></tr><tr><td>split_2-val/mae</td><td>▇█▄▅▃▁▁▅▂▄▃▃▃▂▂▂▃▂▂▃▃▂▂▂▂▂▁▂▁▃▄▂▂▁▁▂▂▂▂▂</td></tr><tr><td>split_2-val/r2</td><td>▁▇▇▇▇█▇███▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇██████████▇</td></tr><tr><td>split_2-val_loss</td><td>█▅▇▃▃▁▂▁▁▃▂▂▂▂▂▃▃▃▃▃▂▃▂▃▃▂▂▂▂▁▂▂▁▂▂▂▂▃▂▂</td></tr><tr><td>split_3-epoch</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇███</td></tr><tr><td>split_3-lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_3-train_loss_epoch</td><td>█▅▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▁▂▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_3-train_loss_step</td><td>██▄▅▃▄▃▂▂▂▃▂▃▂▂▂▂▂▂▂▂▂▁▂▂▁▁▁▂▁▁▂▁▁▂▁▂▂▁▁</td></tr><tr><td>split_3-val/mae</td><td>█▅▅▄▄▂▃▃▃▃▂▂▃▁▃▂▂▂▂▂▂▂▂▂▂▃▂▂▂▃▂▂▂▁▂▂▂▁▁▁</td></tr><tr><td>split_3-val/r2</td><td>▁▄▄▃▄▄▆▃▃▅▆▄▄▄▄▅▆▅▇▆▅▅▅▄▆▄▆▆▇▇▇▇█▇█▆▇▆▇█</td></tr><tr><td>split_3-val_loss</td><td>█▆▅▅▂▃▃▃▃▃▂▂▂▃▂▃▃▃▂▂▃▂▂▃▂▃▃▂▂▂▂▂▂▂▁▁▂▁▂▁</td></tr><tr><td>split_4-epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇██████</td></tr><tr><td>split_4-lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_4-train_loss_epoch</td><td>█▆▄▆▄▃▃▂▂▂▂▃▂▂▂▂▃▃▂▁▂▁▂▂▁▁▂▂▂▁▂▁▂▂▁▂▁▁▁▁</td></tr><tr><td>split_4-train_loss_step</td><td>█▅▅▄▂▂▂▂▃▂▂▁▂▂▁▆▂▂▂▁▂▂▂▁▂▁▁▂▁▁▁▂▂▂▂▁▁▁▁▂</td></tr><tr><td>split_4-val/mae</td><td>█▅▆▃▄▃▃▂▂▃▁▂▂▄▂▁▂▂▃▅▂▂▂▃▁▂▁▁▂▂▂▃▂▂▁▁▁▂▂▂</td></tr><tr><td>split_4-val/r2</td><td>▁▁▄▇▇██▇█▇▇█▆▇▇▆▇▇▆▇▇▇▇▇▇▇▇▇▇▇▇█▇██▆▇█▇▇</td></tr><tr><td>split_4-val_loss</td><td>█▅▄▄▇▄▄▅▂▂▃▃▃▃▄▄▁▂▃▇▄▄▄▃▆▂▃▃▂▃▄▅▅▂▁▁▂▁▃▃</td></tr><tr><td>trainer/global_step</td><td>▁▂▂▃▄▅▇▇▁▂▅▆▆▆▆▇█▄▄▅▆▆▇▇▇▂▅▅▅▆▁▂▃▃▄▄▄▅▆█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>split_0-epoch</td><td>199</td></tr><tr><td>split_0-lr</td><td>0.0001</td></tr><tr><td>split_0-train_loss_epoch</td><td>0.03426</td></tr><tr><td>split_0-train_loss_step</td><td>0.03557</td></tr><tr><td>split_0-val/mae</td><td>0.37063</td></tr><tr><td>split_0-val/r2</td><td>0.6844</td></tr><tr><td>split_0-val_loss</td><td>0.29876</td></tr><tr><td>split_1-epoch</td><td>199</td></tr><tr><td>split_1-lr</td><td>0.0001</td></tr><tr><td>split_1-train_loss_epoch</td><td>0.05292</td></tr><tr><td>split_1-train_loss_step</td><td>0.01242</td></tr><tr><td>split_1-val/mae</td><td>0.33941</td></tr><tr><td>split_1-val/r2</td><td>0.75183</td></tr><tr><td>split_1-val_loss</td><td>0.23699</td></tr><tr><td>split_2-epoch</td><td>199</td></tr><tr><td>split_2-lr</td><td>0.0001</td></tr><tr><td>split_2-train_loss_epoch</td><td>0.03514</td></tr><tr><td>split_2-train_loss_step</td><td>0.08048</td></tr><tr><td>split_2-val/mae</td><td>0.35492</td></tr><tr><td>split_2-val/r2</td><td>0.72585</td></tr><tr><td>split_2-val_loss</td><td>0.28191</td></tr><tr><td>split_3-epoch</td><td>199</td></tr><tr><td>split_3-lr</td><td>0.0001</td></tr><tr><td>split_3-train_loss_epoch</td><td>0.02493</td></tr><tr><td>split_3-train_loss_step</td><td>0.02715</td></tr><tr><td>split_3-val/mae</td><td>0.36156</td></tr><tr><td>split_3-val/r2</td><td>0.69453</td></tr><tr><td>split_3-val_loss</td><td>0.29574</td></tr><tr><td>split_4-epoch</td><td>199</td></tr><tr><td>split_4-lr</td><td>0.0001</td></tr><tr><td>split_4-train_loss_epoch</td><td>0.06053</td></tr><tr><td>split_4-train_loss_step</td><td>0.09559</td></tr><tr><td>split_4-val/mae</td><td>0.36388</td></tr><tr><td>split_4-val/r2</td><td>0.71842</td></tr><tr><td>split_4-val_loss</td><td>0.26024</td></tr><tr><td>trainer/global_step</td><td>2199</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">roberta_run_2_clean_worst_pct_0.2</strong> at: <a href='https://wandb.ai/vladvin-org/admet-challenge/runs/uwo8ed43' target=\"_blank\">https://wandb.ai/vladvin-org/admet-challenge/runs/uwo8ed43</a><br> View project at: <a href='https://wandb.ai/vladvin-org/admet-challenge' target=\"_blank\">https://wandb.ai/vladvin-org/admet-challenge</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>../wandb/roberta_run_2_clean_worst_pct_0.2/wandb/run-20250313_033545-uwo8ed43/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_and_eval(input_paths, save_dirs, RUN_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up stero impure + run 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(input_paths, save_dirs, output_dir, remove_worst_pct):\n",
    "    smiles_to_remove = defaultdict(set)\n",
    "\n",
    "    for input_path, save_dir in zip(input_paths, save_dirs):\n",
    "        input_df = pd.read_csv(input_path)\n",
    "        input_val_df = input_df[input_df[\"split\"] == \"val\"]\n",
    "        output_df = pd.read_csv(save_dir / \"predictions.csv\")\n",
    "        output_val_df = output_df[input_df[\"split\"] == \"val\"]\n",
    "\n",
    "        for t in TARGET_COLUMNS:\n",
    "            # Sort by absolute error\n",
    "            notna_mask = input_val_df[t].notna()\n",
    "            input_val_df = input_val_df[notna_mask]\n",
    "            output_val_df = output_val_df[notna_mask]\n",
    "\n",
    "            mae = np.abs(input_val_df[t] - output_val_df[f\"pred_{t}\"])\n",
    "            sorted_idx = np.argsort(mae)[::-1]\n",
    "            smiles_to_remove[t].update(\n",
    "                input_val_df.iloc[sorted_idx[:int(remove_worst_pct * len(sorted_idx))]][\"cxsmiles_std\"].tolist()\n",
    "            )\n",
    "\n",
    "    for input_path in input_paths:\n",
    "        input_df = pd.read_csv(input_path)\n",
    "        for t in TARGET_COLUMNS:\n",
    "            input_df.loc[\n",
    "                input_df[\"cxsmiles_std\"].isin(smiles_to_remove[t]) & \\\n",
    "                    ~input_df[\"smiles_ext\"].isna() & \\\n",
    "                    (input_df[\"split\"] == \"train\"),\n",
    "                t\n",
    "            ] = np.nan\n",
    "\n",
    "        input_df.to_csv(output_dir / input_path.name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path(\"../output/asap/rnd_splits/roberta/run_0/cleaned\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data(input_paths, save_dirs, output_dir, remove_worst_pct = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogHLM            201\n",
       "LogMLM            186\n",
       "LogD              132\n",
       "LogKSOL           109\n",
       "LogMDR1-MDCKII     55\n",
       "dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(input_paths[0])[TARGET_COLUMNS].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogHLM            100\n",
       "LogMLM             82\n",
       "LogD               74\n",
       "LogKSOL            56\n",
       "LogMDR1-MDCKII     33\n",
       "dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = pd.read_csv(input_paths[0])\n",
    "tmp = tmp[tmp[\"smiles_ext\"].isna()]\n",
    "tmp[TARGET_COLUMNS].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogHLM            204\n",
       "LogMLM            188\n",
       "LogD              134\n",
       "LogKSOL           109\n",
       "LogMDR1-MDCKII     56\n",
       "dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(output_dir / input_paths[0].name)[TARGET_COLUMNS].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogHLM            100\n",
       "LogMLM             82\n",
       "LogD               74\n",
       "LogKSOL            56\n",
       "LogMDR1-MDCKII     33\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = pd.read_csv(output_dir / input_paths[0].name)\n",
    "tmp = tmp[tmp[\"smiles_ext\"].isna()]\n",
    "tmp[TARGET_COLUMNS].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_paths = [Path(f'../output/asap/rnd_splits/roberta/run_0/cleaned/split_{k}.csv') for k in range(5)]\n",
    "save_dirs = [Path(f'../output/asap/rnd_splits/roberta/run_1/split_{k}') for k in range(5)]\n",
    "RUN_IDX = \"1_clean_worst_pct_0.2_stereo_impure\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predicting on ../output/asap/rnd_splits/roberta/run_0/cleaned/split_0.csv\n",
      "3 out of 323 rows are removed due to missing values\n",
      "Downloading checkpoint from <gs_bucket>/artifacts/tokenizers/zinc...\n",
      "0 out of 81 rows are removed due to missing values\n",
      "Downloading checkpoint from <gs_bucket>/artifacts/tokenizers/zinc...\n",
      "Downloading checkpoint from <gs_bucket>/artifacts/tokenizers/zinc...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Path ../wandb/roberta_run_1_clean_worst_pct_0.2_stereo_impure/wandb/ wasn't writable, using system temp directory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting checkpoint from /home/jupyter/AgenticADMET/notebooks/../output/artifacts/mol_mlm_roberta_zinc/last.ckpt...\n",
      "Loading checkpoint from roberta to roberta...\n",
      "<All keys matched successfully>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/var/tmp/wandb/run-20250313_035310-165n38zj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vladvin-org/admet-challenge/runs/165n38zj' target=\"_blank\">roberta_run_1_clean_worst_pct_0.2_stereo_impure</a></strong> to <a href='https://wandb.ai/vladvin-org/admet-challenge' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/vladvin-org/admet-challenge' target=\"_blank\">https://wandb.ai/vladvin-org/admet-challenge</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/vladvin-org/admet-challenge/runs/165n38zj' target=\"_blank\">https://wandb.ai/vladvin-org/admet-challenge/runs/165n38zj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type         | Params | Mode \n",
      "---------------------------------------------------\n",
      "0 | roberta   | RobertaModel | 8.7 M  | train\n",
      "1 | predictor | MLP          | 149 K  | train\n",
      "2 | criterion | MSE          | 0      | train\n",
      "3 | metrics   | ModuleList   | 0      | train\n",
      "---------------------------------------------------\n",
      "8.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "8.8 M     Total params\n",
      "35.297    Total estimated model params size (MB)\n",
      "128       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (10) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.21637993941611766,\n",
      "    \"r2\": 0.7582770243124185\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.20689065525625036,\n",
      "    \"r2\": 0.6510633189776583\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.21599684359639457,\n",
      "    \"r2\": 0.956354620210111\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.1188643528206647,\n",
      "    \"r2\": 0.7913687607928919\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.22602909318495562,\n",
      "    \"r2\": 0.6126703803350781\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.19683217685487658,\n",
      "    \"macro_r2\": 0.7539468209256316\n",
      "  }\n",
      "}\n",
      "\n",
      "Val metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.3663063745356479,\n",
      "    \"r2\": 0.3734249731387834\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.3511783545996734,\n",
      "    \"r2\": 0.22844983126282514\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.6061596973655653,\n",
      "    \"r2\": 0.5775076071696578\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.24213701043718402,\n",
      "    \"r2\": 0.315796488015866\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.37177200994889487,\n",
      "    \"r2\": 0.40736652087158687\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.3875106893773931,\n",
      "    \"macro_r2\": 0.3805090840917439\n",
      "  }\n",
      "}\n",
      "Training and predicting on ../output/asap/rnd_splits/roberta/run_0/cleaned/split_1.csv\n",
      "2 out of 323 rows are removed due to missing values\n",
      "Downloading checkpoint from <gs_bucket>/artifacts/tokenizers/zinc...\n",
      "0 out of 81 rows are removed due to missing values\n",
      "Downloading checkpoint from <gs_bucket>/artifacts/tokenizers/zinc...\n",
      "Downloading checkpoint from <gs_bucket>/artifacts/tokenizers/zinc...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type         | Params | Mode \n",
      "---------------------------------------------------\n",
      "0 | roberta   | RobertaModel | 8.7 M  | train\n",
      "1 | predictor | MLP          | 149 K  | train\n",
      "2 | criterion | MSE          | 0      | train\n",
      "3 | metrics   | ModuleList   | 0      | train\n",
      "---------------------------------------------------\n",
      "8.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "8.8 M     Total params\n",
      "35.297    Total estimated model params size (MB)\n",
      "128       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting checkpoint from /home/jupyter/AgenticADMET/notebooks/../output/artifacts/mol_mlm_roberta_zinc/last.ckpt...\n",
      "Loading checkpoint from roberta to roberta...\n",
      "<All keys matched successfully>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (11) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.17985456647294418,\n",
      "    \"r2\": 0.7711845276216105\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.2795525482428253,\n",
      "    \"r2\": 0.47320592423725216\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.12848516861597697,\n",
      "    \"r2\": 0.9817497381054636\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.1052773447636852,\n",
      "    \"r2\": 0.7967621788264461\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.28378065666609764,\n",
      "    \"r2\": 0.519931488952263\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.19539005695230588,\n",
      "    \"macro_r2\": 0.7085667715486071\n",
      "  }\n",
      "}\n",
      "\n",
      "Val metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.3202843887055576,\n",
      "    \"r2\": 0.4531943747077313\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.2968271666619024,\n",
      "    \"r2\": 0.6004963594952379\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.5650080225142566,\n",
      "    \"r2\": 0.5921705142435527\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.18745901996950964,\n",
      "    \"r2\": 0.5706955054300911\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.3996844974301857,\n",
      "    \"r2\": 0.4582095149148965\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.3538526190562824,\n",
      "    \"macro_r2\": 0.5349532537583019\n",
      "  }\n",
      "}\n",
      "Training and predicting on ../output/asap/rnd_splits/roberta/run_0/cleaned/split_2.csv\n",
      "4 out of 323 rows are removed due to missing values\n",
      "Downloading checkpoint from <gs_bucket>/artifacts/tokenizers/zinc...\n",
      "0 out of 81 rows are removed due to missing values\n",
      "Downloading checkpoint from <gs_bucket>/artifacts/tokenizers/zinc...\n",
      "Downloading checkpoint from <gs_bucket>/artifacts/tokenizers/zinc...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type         | Params | Mode \n",
      "---------------------------------------------------\n",
      "0 | roberta   | RobertaModel | 8.7 M  | train\n",
      "1 | predictor | MLP          | 149 K  | train\n",
      "2 | criterion | MSE          | 0      | train\n",
      "3 | metrics   | ModuleList   | 0      | train\n",
      "---------------------------------------------------\n",
      "8.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "8.8 M     Total params\n",
      "35.297    Total estimated model params size (MB)\n",
      "128       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting checkpoint from /home/jupyter/AgenticADMET/notebooks/../output/artifacts/mol_mlm_roberta_zinc/last.ckpt...\n",
      "Loading checkpoint from roberta to roberta...\n",
      "<All keys matched successfully>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (10) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.15803647145515687,\n",
      "    \"r2\": 0.7654894410334752\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.25505299555102867,\n",
      "    \"r2\": 0.5495092529966441\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.1495011202969413,\n",
      "    \"r2\": 0.9761505648298849\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.08708374817601616,\n",
      "    \"r2\": 0.865175843554411\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.25826024249559987,\n",
      "    \"r2\": 0.5680676790466859\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.18158691559494858,\n",
      "    \"macro_r2\": 0.7448785562922202\n",
      "  }\n",
      "}\n",
      "\n",
      "Val metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.3424811369704895,\n",
      "    \"r2\": 0.4842668003770868\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.3645291216199908,\n",
      "    \"r2\": 0.3459998320430775\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.6052344694191759,\n",
      "    \"r2\": 0.5309440622210555\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.17900463858855412,\n",
      "    \"r2\": 0.6042203634700931\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.3634214366902311,\n",
      "    \"r2\": 0.41664176941671416\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.3709341606576883,\n",
      "    \"macro_r2\": 0.47641456550560546\n",
      "  }\n",
      "}\n",
      "Training and predicting on ../output/asap/rnd_splits/roberta/run_0/cleaned/split_3.csv\n",
      "4 out of 323 rows are removed due to missing values\n",
      "Downloading checkpoint from <gs_bucket>/artifacts/tokenizers/zinc...\n",
      "0 out of 81 rows are removed due to missing values\n",
      "Downloading checkpoint from <gs_bucket>/artifacts/tokenizers/zinc...\n",
      "Downloading checkpoint from <gs_bucket>/artifacts/tokenizers/zinc...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type         | Params | Mode \n",
      "---------------------------------------------------\n",
      "0 | roberta   | RobertaModel | 8.7 M  | train\n",
      "1 | predictor | MLP          | 149 K  | train\n",
      "2 | criterion | MSE          | 0      | train\n",
      "3 | metrics   | ModuleList   | 0      | train\n",
      "---------------------------------------------------\n",
      "8.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "8.8 M     Total params\n",
      "35.297    Total estimated model params size (MB)\n",
      "128       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting checkpoint from /home/jupyter/AgenticADMET/notebooks/../output/artifacts/mol_mlm_roberta_zinc/last.ckpt...\n",
      "Loading checkpoint from roberta to roberta...\n",
      "<All keys matched successfully>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (10) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.20068204840505485,\n",
      "    \"r2\": 0.7451855893831604\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.20028184091262746,\n",
      "    \"r2\": 0.6410578116745846\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.18405287671379927,\n",
      "    \"r2\": 0.9683440364607722\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.09169863207182473,\n",
      "    \"r2\": 0.8118967730115927\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.2048059811696603,\n",
      "    \"r2\": 0.6709933966117079\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.17630427585459332,\n",
      "    \"macro_r2\": 0.7674955214283636\n",
      "  }\n",
      "}\n",
      "\n",
      "Val metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.3067547027194055,\n",
      "    \"r2\": 0.4801342495613511\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.32863408619332773,\n",
      "    \"r2\": 0.4504511524061908\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.6072904193768134,\n",
      "    \"r2\": 0.46543092896710136\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.23243133030150626,\n",
      "    \"r2\": 0.26347934935566353\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.3402284162694375,\n",
      "    \"r2\": 0.2822940592953683\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.36306779097209807,\n",
      "    \"macro_r2\": 0.38835794791713496\n",
      "  }\n",
      "}\n",
      "Training and predicting on ../output/asap/rnd_splits/roberta/run_0/cleaned/split_4.csv\n",
      "3 out of 324 rows are removed due to missing values\n",
      "Downloading checkpoint from <gs_bucket>/artifacts/tokenizers/zinc...\n",
      "0 out of 80 rows are removed due to missing values\n",
      "Downloading checkpoint from <gs_bucket>/artifacts/tokenizers/zinc...\n",
      "Downloading checkpoint from <gs_bucket>/artifacts/tokenizers/zinc...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type         | Params | Mode \n",
      "---------------------------------------------------\n",
      "0 | roberta   | RobertaModel | 8.7 M  | train\n",
      "1 | predictor | MLP          | 149 K  | train\n",
      "2 | criterion | MSE          | 0      | train\n",
      "3 | metrics   | ModuleList   | 0      | train\n",
      "---------------------------------------------------\n",
      "8.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "8.8 M     Total params\n",
      "35.297    Total estimated model params size (MB)\n",
      "128       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting checkpoint from /home/jupyter/AgenticADMET/notebooks/../output/artifacts/mol_mlm_roberta_zinc/last.ckpt...\n",
      "Loading checkpoint from roberta to roberta...\n",
      "<All keys matched successfully>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (11) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.22414353032524362,\n",
      "    \"r2\": 0.6547209535755518\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.2620421606547088,\n",
      "    \"r2\": 0.5080727833299641\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.1727487322013333,\n",
      "    \"r2\": 0.9674805799240126\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.13689891973463994,\n",
      "    \"r2\": 0.6549811281113724\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.27058520916549894,\n",
      "    \"r2\": 0.5662023008892434\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.21328371041628494,\n",
      "    \"macro_r2\": 0.6702915491660288\n",
      "  }\n",
      "}\n",
      "\n",
      "Val metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.25754414585017843,\n",
      "    \"r2\": 0.7548105852999295\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.2608813230939951,\n",
      "    \"r2\": 0.5939974777296552\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.587562834451596,\n",
      "    \"r2\": 0.5146416477669302\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.2848023897378914,\n",
      "    \"r2\": -0.01366880114162905\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.3575240527238283,\n",
      "    \"r2\": 0.40602307288031125\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.3496629491714978,\n",
      "    \"macro_r2\": 0.4511607965070394\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>split_0-epoch</td><td>▁▁▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇█</td></tr><tr><td>split_0-lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_0-train_loss_epoch</td><td>█▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_0-train_loss_step</td><td>█▅▄▃▃▄▃▃▂▂▂▃▂▂▂▂▂▂▂▁▂▁▂▂▁▁▂▁▁▁▂▂▁▁▂▂▁▂▂▂</td></tr><tr><td>split_0-val/mae</td><td>█▄▄▂▂▂▂▂▂▁▂▂▁▂▁▁▁▁▁▂▂▂▂▂▁▁▂▁▁▁▂▁▁▂▂▂▁▂▁▂</td></tr><tr><td>split_0-val/r2</td><td>▁▄▅▇██▇▇█████▇▇████▇▇▇▇▇▇██▇▇▇▇█▇██████▇</td></tr><tr><td>split_0-val_loss</td><td>▅▅▅█▁▄▅▅▃▃▁▄▃▇▅▄▅▃▅▄▄▄▆▅▆▆▇▅▆▄▆▄▃▃▃▄▃▄▆▄</td></tr><tr><td>split_1-epoch</td><td>▁▁▂▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇██</td></tr><tr><td>split_1-lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_1-train_loss_epoch</td><td>█▇▄▃▃▃▃▃▂▃▂▄▂▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▂▂▁▁▂▁▁▁▁</td></tr><tr><td>split_1-train_loss_step</td><td>█▄▇▅▄▄▂▃▂▃▂▃▄▂▂▂▂▃▂▂▂▂▃▂▂▃▁▂▂▃▁▁▁▂▂▁▃▂▁▂</td></tr><tr><td>split_1-val/mae</td><td>█▇▅▃▅▅▅▄▅▂▃▂▃▄▄▄▂▂▅▂▃▂▂▄▄▃▂▂▁▃▂▂▂▃▂▃▁▁▁▃</td></tr><tr><td>split_1-val/r2</td><td>▁▄▅▅▆▆▆▆▇▇▇▇▇▇▇▅▆▇▇▇▆▆▇▆▆▇▇██▇████▇▇▇▇█▇</td></tr><tr><td>split_1-val_loss</td><td>█▆▄▄▃▃▃▃▂▃▂▁▃▂▂▂▃▂▂▂▂▃▂▂▂▁▁▁▁▁▁▁▂▁▁▁▂▁▁▁</td></tr><tr><td>split_2-epoch</td><td>▁▂▂▂▂▂▂▂▃▃▄▄▄▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇████</td></tr><tr><td>split_2-lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_2-train_loss_epoch</td><td>█▄▃▂▂▁▂▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_2-train_loss_step</td><td>▆█▃▃▃▂▂▂▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▃▁▁▁▁▁▁▁▂▁▂▁▁▂▁▁▁</td></tr><tr><td>split_2-val/mae</td><td>█▆▂▄▃▂▃▂▃▃▂▂▂▃▂▂▃▂▂▂▂▂▂▂▄▂▁▁▁▂▂▁▂▂▃▂▂▂▂▂</td></tr><tr><td>split_2-val/r2</td><td>▅▅▅▄▃▄▁▂▄▄▄▁▃▃▆▆▃▄▄▄▄▄▄▁▆▇▆█▇▆▆▅▅▄▆▄▅▆▅▆</td></tr><tr><td>split_2-val_loss</td><td>▄▁▂▂▄▂▃▂▄▅▆▅█▇▅▄▃▆▇▄▃▄█▅▃▃▂▂▂▃▂▁▅▄▃▄▂▃▃▃</td></tr><tr><td>split_3-epoch</td><td>▁▁▁▂▂▂▂▂▂▂▃▃▃▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇███</td></tr><tr><td>split_3-lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_3-train_loss_epoch</td><td>█▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_3-train_loss_step</td><td>█▅▃▃▃▂▂▃▂▂▂▂▂▃▁▁▂▁▁▂▂▁▂▂▁▁▂▂▁▁▁▂▁▁▁▁▂▁▁▁</td></tr><tr><td>split_3-val/mae</td><td>▇█▃▃▃▃▃▂▂▂▂▂▂▂▁▂▂▂▂▂▂▂▂▂▁▂▂▁▁▂▂▂▁▂▂▁▁▁▂▁</td></tr><tr><td>split_3-val/r2</td><td>▁▃▂▅▆▅▆▅▅▆▇▆▆▆▆▇▇▇▇▇▇▇▇▇███▇▇▇▇███▇██▇██</td></tr><tr><td>split_3-val_loss</td><td>█▄▄▃▃▄▃▃▂▃▂▂▂▃▂▂▂▂▂▂▂▁▁▁▁▂▁▁▂▂▁▁▁▁▁▂▁▁▁▁</td></tr><tr><td>split_4-epoch</td><td>▁▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>split_4-lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_4-train_loss_epoch</td><td>█▅▄▃▃▂▄▃▂▂▂▂▂▂▂▂▃▂▂▂▁▁▁▂▄▁▂▁▁▂▂▂▁▁▂▁▁▁▁▁</td></tr><tr><td>split_4-train_loss_step</td><td>█▅▄▅▅▄▅█▅▅▄▃▄▃▄▂▃▃▃▂▄▃▃▃▂▄▃▂▃▃▂▃▃▂▂▃▃▂▃▁</td></tr><tr><td>split_4-val/mae</td><td>█▇▅▃▂▅▂▄▃▃▂▃▂▂▁▁▂▂▂▂▃▃▃▂▂▃▃▃▃▂▂▂▂▃▂▃▂▃▄▃</td></tr><tr><td>split_4-val/r2</td><td>▂▁▆▆▆▆▆▆▄▄▇▇▄▆▆▇█▇▇▇▄▄▇▆▆▅▅▄▅▅▆▅▆▇▇▇▇▇▆▆</td></tr><tr><td>split_4-val_loss</td><td>█▅▄▂▂▁▂▂▂▂▃▃▁▁▁▂▂▂▁▁▂▂▂▂▁▂▂▂▂▂▂▂▁▂▂▂▁▁▁▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▂▃▅▇▇█▂▅▆▇█▃▄▆▆▇▇█▁▁▃▃▄▆▆▆▆▇▁▁▁▃▅▅▆▆▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>split_0-epoch</td><td>199</td></tr><tr><td>split_0-lr</td><td>0.0001</td></tr><tr><td>split_0-train_loss_epoch</td><td>0.03611</td></tr><tr><td>split_0-train_loss_step</td><td>0.04082</td></tr><tr><td>split_0-val/mae</td><td>0.38113</td></tr><tr><td>split_0-val/r2</td><td>0.67435</td></tr><tr><td>split_0-val_loss</td><td>0.31034</td></tr><tr><td>split_1-epoch</td><td>199</td></tr><tr><td>split_1-lr</td><td>0.0001</td></tr><tr><td>split_1-train_loss_epoch</td><td>0.03809</td></tr><tr><td>split_1-train_loss_step</td><td>0.06345</td></tr><tr><td>split_1-val/mae</td><td>0.34822</td></tr><tr><td>split_1-val/r2</td><td>0.72323</td></tr><tr><td>split_1-val_loss</td><td>0.2637</td></tr><tr><td>split_2-epoch</td><td>199</td></tr><tr><td>split_2-lr</td><td>0.0001</td></tr><tr><td>split_2-train_loss_epoch</td><td>0.02972</td></tr><tr><td>split_2-train_loss_step</td><td>0.02848</td></tr><tr><td>split_2-val/mae</td><td>0.36364</td></tr><tr><td>split_2-val/r2</td><td>0.70161</td></tr><tr><td>split_2-val_loss</td><td>0.30438</td></tr><tr><td>split_3-epoch</td><td>199</td></tr><tr><td>split_3-lr</td><td>0.0001</td></tr><tr><td>split_3-train_loss_epoch</td><td>0.03752</td></tr><tr><td>split_3-train_loss_step</td><td>0.02169</td></tr><tr><td>split_3-val/mae</td><td>0.35891</td></tr><tr><td>split_3-val/r2</td><td>0.67763</td></tr><tr><td>split_3-val_loss</td><td>0.30657</td></tr><tr><td>split_4-epoch</td><td>199</td></tr><tr><td>split_4-lr</td><td>0.0001</td></tr><tr><td>split_4-train_loss_epoch</td><td>0.05783</td></tr><tr><td>split_4-train_loss_step</td><td>0.00086</td></tr><tr><td>split_4-val/mae</td><td>0.34936</td></tr><tr><td>split_4-val/r2</td><td>0.71951</td></tr><tr><td>split_4-val_loss</td><td>0.25854</td></tr><tr><td>trainer/global_step</td><td>2199</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">roberta_run_1_clean_worst_pct_0.2_stereo_impure</strong> at: <a href='https://wandb.ai/vladvin-org/admet-challenge/runs/165n38zj' target=\"_blank\">https://wandb.ai/vladvin-org/admet-challenge/runs/165n38zj</a><br> View project at: <a href='https://wandb.ai/vladvin-org/admet-challenge' target=\"_blank\">https://wandb.ai/vladvin-org/admet-challenge</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>../wandb/roberta_run_1_clean_worst_pct_0.2_stereo_impure/wandb/run-20250313_035310-165n38zj/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_and_eval(input_paths, save_dirs, RUN_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing all stereo impure + run 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_paths = [Path(f'../data/asap/datasets/rnd_splits/split_{k}.csv') for k in range(5)]\n",
    "output_dir = Path(\"../data/asap/datasets/rnd_splits/stereo_pure\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for input_path in input_paths:\n",
    "    input_df = pd.read_csv(input_path)\n",
    "    input_df = pd.concat([\n",
    "        input_df[(input_df[\"split\"] == \"train\") & input_df[\"smiles_ext\"].isna()],\n",
    "        input_df[input_df[\"split\"] == \"val\"]\n",
    "    ])\n",
    "    input_df.to_csv(output_dir / input_path.name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predicting on ../data/asap/datasets/rnd_splits/stereo_pure/split_0.csv\n",
      "0 out of 166 rows are removed due to missing values\n",
      "Downloading checkpoint from <gs_bucket>/artifacts/tokenizers/zinc...\n",
      "0 out of 81 rows are removed due to missing values\n",
      "Downloading checkpoint from <gs_bucket>/artifacts/tokenizers/zinc...\n",
      "Downloading checkpoint from <gs_bucket>/artifacts/tokenizers/zinc...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Path ../wandb/roberta_run_0_stereo_pure/wandb/ wasn't writable, using system temp directory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting checkpoint from /home/jupyter/AgenticADMET/notebooks/../output/artifacts/mol_mlm_roberta_zinc/last.ckpt...\n",
      "Loading checkpoint from roberta to roberta...\n",
      "<All keys matched successfully>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/var/tmp/wandb/run-20250313_041023-36tf3og4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vladvin-org/admet-challenge/runs/36tf3og4' target=\"_blank\">roberta_run_0_stereo_pure</a></strong> to <a href='https://wandb.ai/vladvin-org/admet-challenge' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/vladvin-org/admet-challenge' target=\"_blank\">https://wandb.ai/vladvin-org/admet-challenge</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/vladvin-org/admet-challenge/runs/36tf3og4' target=\"_blank\">https://wandb.ai/vladvin-org/admet-challenge/runs/36tf3og4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type         | Params | Mode \n",
      "---------------------------------------------------\n",
      "0 | roberta   | RobertaModel | 8.7 M  | train\n",
      "1 | predictor | MLP          | 149 K  | train\n",
      "2 | criterion | MSE          | 0      | train\n",
      "3 | metrics   | ModuleList   | 0      | train\n",
      "---------------------------------------------------\n",
      "8.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "8.8 M     Total params\n",
      "35.297    Total estimated model params size (MB)\n",
      "128       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.20010931505162605,\n",
      "    \"r2\": 0.8915152256344944\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.09812466124933268,\n",
      "    \"r2\": 0.9636607789535374\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.13463153686431736,\n",
      "    \"r2\": 0.982303189357938\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.08364396180401804,\n",
      "    \"r2\": 0.919976012823846\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.10709459299818337,\n",
      "    \"r2\": 0.9571038976882112\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.12472081359349549,\n",
      "    \"macro_r2\": 0.9429118208916055\n",
      "  }\n",
      "}\n",
      "\n",
      "Val metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.5393368077844162,\n",
      "    \"r2\": -0.18617652139218221\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.38943063077011925,\n",
      "    \"r2\": 0.037898266646701595\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.7456228902984838,\n",
      "    \"r2\": 0.4414945104236615\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.34159407699615496,\n",
      "    \"r2\": -0.06403151123248496\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.515107715774647,\n",
      "    \"r2\": -0.08414467211453691\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.5062184243247643,\n",
      "    \"macro_r2\": 0.029008014466231802\n",
      "  }\n",
      "}\n",
      "Training and predicting on ../data/asap/datasets/rnd_splits/stereo_pure/split_1.csv\n",
      "0 out of 162 rows are removed due to missing values\n",
      "Downloading checkpoint from <gs_bucket>/artifacts/tokenizers/zinc...\n",
      "0 out of 81 rows are removed due to missing values\n",
      "Downloading checkpoint from <gs_bucket>/artifacts/tokenizers/zinc...\n",
      "Downloading checkpoint from <gs_bucket>/artifacts/tokenizers/zinc...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type         | Params | Mode \n",
      "---------------------------------------------------\n",
      "0 | roberta   | RobertaModel | 8.7 M  | train\n",
      "1 | predictor | MLP          | 149 K  | train\n",
      "2 | criterion | MSE          | 0      | train\n",
      "3 | metrics   | ModuleList   | 0      | train\n",
      "---------------------------------------------------\n",
      "8.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "8.8 M     Total params\n",
      "35.297    Total estimated model params size (MB)\n",
      "128       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting checkpoint from /home/jupyter/AgenticADMET/notebooks/../output/artifacts/mol_mlm_roberta_zinc/last.ckpt...\n",
      "Loading checkpoint from roberta to roberta...\n",
      "<All keys matched successfully>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.13298658536801106,\n",
      "    \"r2\": 0.924057989899633\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.12197437751781198,\n",
      "    \"r2\": 0.9312980007714555\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.17123579642705378,\n",
      "    \"r2\": 0.9717439737478506\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.09992811517066318,\n",
      "    \"r2\": 0.8424431383586694\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.11811861301417097,\n",
      "    \"r2\": 0.9439754383041796\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.12884869749954217,\n",
      "    \"macro_r2\": 0.9227037082163576\n",
      "  }\n",
      "}\n",
      "\n",
      "Val metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.40106590515922175,\n",
      "    \"r2\": 0.3017659584282395\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.44553102142641576,\n",
      "    \"r2\": 0.10369377973676319\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.752715515945897,\n",
      "    \"r2\": 0.3614376857504773\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.2642845536980625,\n",
      "    \"r2\": 0.23992817855297532\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.545712727683707,\n",
      "    \"r2\": -0.032158114095523604\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.48186194478266076,\n",
      "    \"macro_r2\": 0.19493349767458631\n",
      "  }\n",
      "}\n",
      "Training and predicting on ../data/asap/datasets/rnd_splits/stereo_pure/split_2.csv\n",
      "0 out of 159 rows are removed due to missing values\n",
      "Downloading checkpoint from <gs_bucket>/artifacts/tokenizers/zinc...\n",
      "0 out of 81 rows are removed due to missing values\n",
      "Downloading checkpoint from <gs_bucket>/artifacts/tokenizers/zinc...\n",
      "Downloading checkpoint from <gs_bucket>/artifacts/tokenizers/zinc...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type         | Params | Mode \n",
      "---------------------------------------------------\n",
      "0 | roberta   | RobertaModel | 8.7 M  | train\n",
      "1 | predictor | MLP          | 149 K  | train\n",
      "2 | criterion | MSE          | 0      | train\n",
      "3 | metrics   | ModuleList   | 0      | train\n",
      "---------------------------------------------------\n",
      "8.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "8.8 M     Total params\n",
      "35.297    Total estimated model params size (MB)\n",
      "128       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting checkpoint from /home/jupyter/AgenticADMET/notebooks/../output/artifacts/mol_mlm_roberta_zinc/last.ckpt...\n",
      "Loading checkpoint from roberta to roberta...\n",
      "<All keys matched successfully>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.11852502549003416,\n",
      "    \"r2\": 0.9602615398010264\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.11195401739273254,\n",
      "    \"r2\": 0.9501199182391339\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.11989784106966993,\n",
      "    \"r2\": 0.988037431663729\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.05464037963369611,\n",
      "    \"r2\": 0.9604001714746205\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.11324171539446744,\n",
      "    \"r2\": 0.9554210888496576\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.10365179579612005,\n",
      "    \"macro_r2\": 0.9628480300056335\n",
      "  }\n",
      "}\n",
      "\n",
      "Val metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.4593483829811132,\n",
      "    \"r2\": 0.27239270708362984\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.42824647313442316,\n",
      "    \"r2\": 0.07051352011138634\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.8410545934691573,\n",
      "    \"r2\": 0.36046948782579447\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.27294469652312464,\n",
      "    \"r2\": 0.1257303876045559\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.5149275979770027,\n",
      "    \"r2\": -0.09295007241680775\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.5033043488169643,\n",
      "    \"macro_r2\": 0.14723120604171175\n",
      "  }\n",
      "}\n",
      "Training and predicting on ../data/asap/datasets/rnd_splits/stereo_pure/split_3.csv\n",
      "0 out of 171 rows are removed due to missing values\n",
      "Downloading checkpoint from <gs_bucket>/artifacts/tokenizers/zinc...\n",
      "0 out of 81 rows are removed due to missing values\n",
      "Downloading checkpoint from <gs_bucket>/artifacts/tokenizers/zinc...\n",
      "Downloading checkpoint from <gs_bucket>/artifacts/tokenizers/zinc...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type         | Params | Mode \n",
      "---------------------------------------------------\n",
      "0 | roberta   | RobertaModel | 8.7 M  | train\n",
      "1 | predictor | MLP          | 149 K  | train\n",
      "2 | criterion | MSE          | 0      | train\n",
      "3 | metrics   | ModuleList   | 0      | train\n",
      "---------------------------------------------------\n",
      "8.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "8.8 M     Total params\n",
      "35.297    Total estimated model params size (MB)\n",
      "128       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting checkpoint from /home/jupyter/AgenticADMET/notebooks/../output/artifacts/mol_mlm_roberta_zinc/last.ckpt...\n",
      "Loading checkpoint from roberta to roberta...\n",
      "<All keys matched successfully>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.10942242981138496,\n",
      "    \"r2\": 0.967106496541727\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.0831861982122054,\n",
      "    \"r2\": 0.9712389214731779\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.16561844741115134,\n",
      "    \"r2\": 0.9761749556891458\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.06351076254426723,\n",
      "    \"r2\": 0.9524496569135843\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.11056433340385553,\n",
      "    \"r2\": 0.9557888687146641\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.1064604342765729,\n",
      "    \"macro_r2\": 0.9645517798664598\n",
      "  }\n",
      "}\n",
      "\n",
      "Val metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.5235238723778832,\n",
      "    \"r2\": -0.1609556742324807\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.4780674737353976,\n",
      "    \"r2\": -0.12465477298513838\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.8683317369314341,\n",
      "    \"r2\": 0.16295159100040224\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.2940396834922703,\n",
      "    \"r2\": -0.03265329931880645\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.4535368237359115,\n",
      "    \"r2\": -0.0967930287105001\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.5234999180545794,\n",
      "    \"macro_r2\": -0.05042103684930468\n",
      "  }\n",
      "}\n",
      "Training and predicting on ../data/asap/datasets/rnd_splits/stereo_pure/split_4.csv\n",
      "0 out of 162 rows are removed due to missing values\n",
      "Downloading checkpoint from <gs_bucket>/artifacts/tokenizers/zinc...\n",
      "0 out of 80 rows are removed due to missing values\n",
      "Downloading checkpoint from <gs_bucket>/artifacts/tokenizers/zinc...\n",
      "Downloading checkpoint from <gs_bucket>/artifacts/tokenizers/zinc...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type         | Params | Mode \n",
      "---------------------------------------------------\n",
      "0 | roberta   | RobertaModel | 8.7 M  | train\n",
      "1 | predictor | MLP          | 149 K  | train\n",
      "2 | criterion | MSE          | 0      | train\n",
      "3 | metrics   | ModuleList   | 0      | train\n",
      "---------------------------------------------------\n",
      "8.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "8.8 M     Total params\n",
      "35.297    Total estimated model params size (MB)\n",
      "128       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting checkpoint from /home/jupyter/AgenticADMET/notebooks/../output/artifacts/mol_mlm_roberta_zinc/last.ckpt...\n",
      "Loading checkpoint from roberta to roberta...\n",
      "<All keys matched successfully>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.10881194538078122,\n",
      "    \"r2\": 0.9559973147700999\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.13697251945789846,\n",
      "    \"r2\": 0.9315443100159168\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.13320418093786685,\n",
      "    \"r2\": 0.9806549379758814\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.0875027711120591,\n",
      "    \"r2\": 0.8999674472779172\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.08088506654617339,\n",
      "    \"r2\": 0.9752515762132834\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.1094752966869558,\n",
      "    \"macro_r2\": 0.9486831172506196\n",
      "  }\n",
      "}\n",
      "\n",
      "Val metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.4163545562125055,\n",
      "    \"r2\": 0.4205903388125979\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.450075913200766,\n",
      "    \"r2\": -0.10658784070746163\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.8097103596528371,\n",
      "    \"r2\": 0.2681078579989802\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.29141314974787097,\n",
      "    \"r2\": 0.12345889966982182\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.5171881925147317,\n",
      "    \"r2\": -0.3116756935913272\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.4969484342657422,\n",
      "    \"macro_r2\": 0.07877871243652221\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>split_0-epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇█</td></tr><tr><td>split_0-lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_0-train_loss_epoch</td><td>█▅▄▄▃▂▂▂▂▂▁▂▂▂▂▂▁▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁</td></tr><tr><td>split_0-train_loss_step</td><td>█▃▃▃▂▃▂▁▁▂▂▁▂▂▁▁▁▁▁▂▁▁▁▁</td></tr><tr><td>split_0-val/mae</td><td>█▆▅▆▄▃▄▄▂▃▃▃▁▂▂▂▃▂▂▃▂▅▃▃▅▄▃▃▃▃▄▄▅▅▂▃▂▂▄▁</td></tr><tr><td>split_0-val/r2</td><td>▃▁▅▅▄█▆▅▅▄▆▅▆▇▆▇▇▇▆▅▂▅▂▅▃▅▆▅▅▅▅▅▃▄▆▅▆▆█▅</td></tr><tr><td>split_0-val_loss</td><td>▅▄▇▆▇▃▅▅▂▄▅▅▅▃▄▄▆▄▅▆█▇▅█▆▆▅▄▅▅▆█▄▄▄▆▁▄▂▂</td></tr><tr><td>split_1-epoch</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>split_1-lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_1-train_loss_epoch</td><td>█▇▇▅▃▃▃▄▃▃▃▂▂▂▂▂▂▂▃▂▁▂▁▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▂</td></tr><tr><td>split_1-train_loss_step</td><td>█▃▁▄▃▂▂▂▁▃▂▇▁▁▂▂▁▁▁▂▁▁▁▃</td></tr><tr><td>split_1-val/mae</td><td>█▃▄▄▂▂▄▄▂▂▂▂▂▂▃▂▄▂▂▁▂▃▃▄▃▃▁▂▂▁▃▁▂▂▁▂▂▃▃▂</td></tr><tr><td>split_1-val/r2</td><td>▁▃▆▄▅▇▆▅▅▅▆▄▇▇▇▆▆▅▇▆▅▇▆▆▆▆▆▇▆█▆█▇▆▇▇▇▇▇▆</td></tr><tr><td>split_1-val_loss</td><td>█▇▆▅▅▅▇▅▄▄▄▃▆▆▄▃▃▂▂▄▆▆▄▅▄▃▄▆▂▄▆▃▃▃▁▄▃▄▃▄</td></tr><tr><td>split_2-epoch</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>split_2-lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_2-train_loss_epoch</td><td>█▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_2-train_loss_step</td><td>█▅▆▄▃▃▂▂▄▃▂▂▂▂▃▂▁▁▂▂</td></tr><tr><td>split_2-val/mae</td><td>█▆▂▄▄▁▂▃▅▅▄▅▅▃▅▆▄▂▃▅▄▄▂▄▄▄▃▁▃▅▂▃▃▃▂▃▂▂▂▃</td></tr><tr><td>split_2-val/r2</td><td>▁▇▇█▇█▆▆▇██▇▇▇▇█▇▇▇▆▆▆▆▆▆▆▇▇▇▇▇▇██▇▇█▇▇▇</td></tr><tr><td>split_2-val_loss</td><td>▅▃▄▄▅█▂▄▆▃▆▅▅▅█▅▄▄▆▃▇▄▅▄▆▆▅▅▃▁▅▄▅▄▃▄▃▃▄▃</td></tr><tr><td>split_3-epoch</td><td>▁▁▁▁▂▂▂▂▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇█████</td></tr><tr><td>split_3-lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_3-train_loss_epoch</td><td>█▅▂▂▂▂▁▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_3-train_loss_step</td><td>█▃▆▃▃▂▃▂▁▄▂▂▃▂▂▂▂▁▁▁▂▁▁▁</td></tr><tr><td>split_3-val/mae</td><td>█▂▃▂▃█▅▄▄▇▄▅▄▄▆▄▄▃▄▄▃▄▂▂▃▅▄▃▄▅▃▃▄▂▃▂▁▁▂▄</td></tr><tr><td>split_3-val/r2</td><td>▄▃▃▃▄▄▃▄▆▅▄▁▅▄▃▆▂▅▇▅▅▆▅▇▇▄▆▅▅▆█▇█▇▇███▆▇</td></tr><tr><td>split_3-val_loss</td><td>▆▆▇▇▇▅▆▅▄▄▇█▄▃▅▂▃▆▃▆▅▃▅▄▄▄▂▂▁▂▂▂▂▁▃▂▃▃▃▃</td></tr><tr><td>split_4-epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▄▄▄▄▄▅▅▅▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>split_4-lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_4-train_loss_epoch</td><td>█▆▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_4-train_loss_step</td><td>█▅▃▄▄▇▃▃▂▃▃█▂▄▂▃▂▁▂▂▂▂▂▁</td></tr><tr><td>split_4-val/mae</td><td>▂▁▂▂▃▄▅▄▅▃▅▇▆▅▆▃▆▃▄▅▅▃▄▄▄▄▄▆▆▄▇█▅▄▃▄▅▄▅▃</td></tr><tr><td>split_4-val/r2</td><td>▁▄▅▅█▇▆▆▆▆▆▅▅▆▆▆▅▅▅▇▅▆▆▆▆▆▆▆▅▅▅▅▄▄▄▅▄▅▄▅</td></tr><tr><td>split_4-val_loss</td><td>█▂▄▄▂▁▂▂▂▁▄▂▂▃▄▁▃▃▃▂▃▂▃▃▃▃▃▄▄▄▄▄▄▅▄▃▃▃▃▃</td></tr><tr><td>trainer/global_step</td><td>▃▃▄▄▅▇▇▂▃▃▃▆▇▇██▁▁▂▂▆▆▇▇▁▂▃▃▄▄▅▆█▃▃▄▅▆▆▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>split_0-epoch</td><td>199</td></tr><tr><td>split_0-lr</td><td>0.0001</td></tr><tr><td>split_0-train_loss_epoch</td><td>0.0576</td></tr><tr><td>split_0-train_loss_step</td><td>0.02648</td></tr><tr><td>split_0-val/mae</td><td>0.50206</td></tr><tr><td>split_0-val/r2</td><td>0.50822</td></tr><tr><td>split_0-val_loss</td><td>0.45468</td></tr><tr><td>split_1-epoch</td><td>199</td></tr><tr><td>split_1-lr</td><td>0.0001</td></tr><tr><td>split_1-train_loss_epoch</td><td>0.05892</td></tr><tr><td>split_1-train_loss_step</td><td>0.14548</td></tr><tr><td>split_1-val/mae</td><td>0.47159</td></tr><tr><td>split_1-val/r2</td><td>0.54667</td></tr><tr><td>split_1-val_loss</td><td>0.43614</td></tr><tr><td>split_2-epoch</td><td>199</td></tr><tr><td>split_2-lr</td><td>0.0001</td></tr><tr><td>split_2-train_loss_epoch</td><td>0.05567</td></tr><tr><td>split_2-train_loss_step</td><td>0.06155</td></tr><tr><td>split_2-val/mae</td><td>0.49573</td></tr><tr><td>split_2-val/r2</td><td>0.55421</td></tr><tr><td>split_2-val_loss</td><td>0.44881</td></tr><tr><td>split_3-epoch</td><td>199</td></tr><tr><td>split_3-lr</td><td>0.0001</td></tr><tr><td>split_3-train_loss_epoch</td><td>0.04762</td></tr><tr><td>split_3-train_loss_step</td><td>0.02162</td></tr><tr><td>split_3-val/mae</td><td>0.51881</td></tr><tr><td>split_3-val/r2</td><td>0.45256</td></tr><tr><td>split_3-val_loss</td><td>0.5261</td></tr><tr><td>split_4-epoch</td><td>199</td></tr><tr><td>split_4-lr</td><td>0.0001</td></tr><tr><td>split_4-train_loss_epoch</td><td>0.04752</td></tr><tr><td>split_4-train_loss_step</td><td>0.00988</td></tr><tr><td>split_4-val/mae</td><td>0.4872</td></tr><tr><td>split_4-val/r2</td><td>0.52989</td></tr><tr><td>split_4-val_loss</td><td>0.42098</td></tr><tr><td>trainer/global_step</td><td>1199</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">roberta_run_0_stereo_pure</strong> at: <a href='https://wandb.ai/vladvin-org/admet-challenge/runs/36tf3og4' target=\"_blank\">https://wandb.ai/vladvin-org/admet-challenge/runs/36tf3og4</a><br> View project at: <a href='https://wandb.ai/vladvin-org/admet-challenge' target=\"_blank\">https://wandb.ai/vladvin-org/admet-challenge</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>../wandb/roberta_run_0_stereo_pure/wandb/run-20250313_041023-36tf3og4/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_paths = [Path(f'../data/asap/datasets/rnd_splits/stereo_pure/split_{k}.csv') for k in range(5)]\n",
    "save_dirs = [Path(f'../output/asap/rnd_splits/roberta/run_0/stereo_pure/split_{k}') for k in range(5)]\n",
    "RUN_IDX = \"0_stereo_pure\"\n",
    "train_and_eval(input_paths, save_dirs, RUN_IDX)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "admet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
