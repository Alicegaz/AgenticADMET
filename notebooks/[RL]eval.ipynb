{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alisavin/AgenticADMET/openr1/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-02 03:25:52 __init__.py:190] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-02 03:25:52,942\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from dataset import load_polaris_dataset, validate_dataset\n",
    "from train import get_dataset\n",
    "import numpy as np\n",
    "from latex2sympy2_extended import NormalizationConfig\n",
    "from math_verify import LatexExtractionConfig, parse\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    PreTrainedTokenizer\n",
    ")\n",
    "import torch\n",
    "from peft import PeftModel\n",
    "from peft import prepare_model_for_kbit_training\n",
    "from trl import ModelConfig\n",
    "from munch import Munch\n",
    "import json\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "import hashlib\n",
    "from collections import defaultdict\n",
    "\n",
    "from train import GRPOTrainer2\n",
    "import os\n",
    "from trl import (\n",
    "    GRPOConfig, \n",
    "    GRPOTrainer,\n",
    "    get_peft_config\n",
    ")\n",
    "from dataclasses import field, dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_CHAT_TEMPLATE = \"{% for message in messages %}\\n{% if message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'system' %}\\n{{ '<|system|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>\\n'  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '<|assistant|>' }}\\n{% endif %}\\n{% endfor %}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mae(completions, ground_truth=None, log_normalize=False, model_name=\"pred\", **kwargs):\n",
    "    \n",
    "    smiles = kwargs.get(\"smiles\")\n",
    "    num_generations = len(completions) / len(set(smiles))\n",
    "    \n",
    "    Path(f\"./test/completions/{model_name}_{num_generations}/\").mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    contents = [completion[0][\"content\"] for completion in completions]\n",
    "    rewards = []\n",
    "\n",
    "    solutions = kwargs.get(\"solution\") # Get solutions from kwargs\n",
    "    \n",
    "\n",
    "    if solutions is None:\n",
    "        return [0.5] * len(completions) # Return neutral reward if no solution\n",
    "    smiles2conts = defaultdict(list)\n",
    "    #TODO: not parsed empty list\n",
    "    #TODO: mae mean per group, take meadian and calculate\n",
    "    #TODO: make more generations, save separately\n",
    "    for content, gold_val, smiles_i in zip(contents, solutions, smiles):\n",
    "        \n",
    "        if gold_val is not None:  # Check if parsing was successful\n",
    "            # Parse the model's answer with relaxed normalization\n",
    "            answer_parsed = parse(\n",
    "                content,\n",
    "                extraction_config=[\n",
    "                    LatexExtractionConfig(\n",
    "                        normalization_config=NormalizationConfig(\n",
    "                            nits=False,\n",
    "                            malformed_operators=False,\n",
    "                            basic_latex=True,\n",
    "                            equations=True,\n",
    "                            boxed=\"all\",\n",
    "                            units=True,\n",
    "                        ),\n",
    "                        boxed_match_priority=0,\n",
    "                        try_extract_without_anchor=False,\n",
    "                    )\n",
    "                ],\n",
    "                extraction_mode=\"first_match\",\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                answer_val = float(answer_parsed[0])\n",
    "                mae = np.mean(np.abs(gold_val - answer_val))\n",
    "                print(\"parsed correctly\", answer_val, gold_val)\n",
    "            except Exception as e:\n",
    "                answer_val = None\n",
    "                mae = 6\n",
    "                if len(answer_parsed) > 0:\n",
    "                    print(e, answer_parsed)\n",
    "        else:\n",
    "            answer_val = None\n",
    "            mae = 6\n",
    "            print(\"Warning: Gold solution is None:\", gold_val)\n",
    "        if answer_val is not None:\n",
    "            post = \"parsed_\"\n",
    "        else:\n",
    "            post = \"\"\n",
    "        smiles_hash = hashlib.blake2b(smiles_i.encode('utf-8'), digest_size=4).hexdigest()\n",
    "        rewards.append(mae)\n",
    "        smiles2conts[smiles_hash].append({\"completion\": content, \n",
    "                       \"gold_val\": str(gold_val), \n",
    "                       \"answer_parsed\": str(answer_parsed), \n",
    "                       \"smiles\": smiles_i,\n",
    "                       \"answer_val\": answer_val,\n",
    "                       \"mae\": mae\n",
    "                       }) \n",
    "    median_maes = []\n",
    "    for k, v in smiles2conts.items():\n",
    "        with open(f\"./test/completions/{model_name}_{num_generations}/{post}{k}.json\", \"w\") as f:\n",
    "                answers_g = [v_i[\"answer_val\"] for v_i in v]\n",
    "                answers_g = [float(v_i) for v_i in answers_g if v_i is not None]\n",
    "                answer_median = np.median(answers_g)\n",
    "                mae_median = np.median(np.abs(float(v[0][\"gold_val\"]) - answer_median))\n",
    "                median_maes.append(mae_median)\n",
    "                json.dump({\"completion\": [v_i[\"completion\"] for v_i in v], \n",
    "                        \"gold_val\": v[0][\"gold_val\"], \n",
    "                        \"answer_parsed\": [v_i[\"answer_parsed\"] for v_i in v], \n",
    "                        \"smiles\": v[0][\"smiles\"],\n",
    "                        \"answer_val\": [v_i[\"answer_val\"] for v_i in v],\n",
    "                        \"mae\": [v_i[\"mae\"] for v_i in v],\n",
    "                        \"mae_median\": str(mae_median)\n",
    "                        }, f, indent=2)\n",
    "    return median_maes\n",
    "\n",
    "def get_tokenizer(\n",
    "    model_args: ModelConfig, training_args, auto_set_chat_template: bool = True\n",
    ") -> PreTrainedTokenizer:\n",
    "    \"\"\"Get the tokenizer for the model.\"\"\"\n",
    "    # https://github.com/huggingface/open-r1/blob/eeca246b078457bc0f69ba2e8297b799df0e2bda/src/open_r1/utils/model_utils.py#L11\n",
    "    print(\"loading tokenizer\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        revision=model_args.model_revision,\n",
    "        trust_remote_code=False, # model_args.trust_remote_code\n",
    "    )\n",
    "    print(\"tokenizer loaded\")\n",
    "\n",
    "    if training_args.chat_template is not None:\n",
    "        tokenizer.chat_template = training_args.chat_template\n",
    "    elif auto_set_chat_template and tokenizer.get_chat_template() is None:\n",
    "        tokenizer.chat_template = DEFAULT_CHAT_TEMPLATE\n",
    "    print(\"chat template\")\n",
    "    # if processing_class is None:\n",
    "    #     processing_class = AutoTokenizer.from_pretrained(model.config._name_or_path, padding_side=\"left\")\n",
    "    return tokenizer\n",
    "\n",
    "def get_reward_functions(script_args, model_name):\n",
    "    \"\"\"\n",
    "    Returns a list of reward functions based on the script arguments.\n",
    "    \"\"\"\n",
    "    reward_funcs_list = []\n",
    "\n",
    "    fnc = partial(compute_mae, model_name=model_name)\n",
    "    fnc.__name__ = compute_mae.__name__\n",
    "    reward_funcs_registry = {\n",
    "        \"mae\": fnc,  # Assuming accuracy_reward is defined in previous steps\n",
    "    }\n",
    "\n",
    "    for func_name in script_args.reward_funcs:\n",
    "        if func_name not in reward_funcs_registry:\n",
    "            raise ValueError(f\"Reward function '{func_name}' not found in registry.\")\n",
    "        reward_funcs_list.append(reward_funcs_registry[func_name])\n",
    "\n",
    "    return reward_funcs_list\n",
    "\n",
    "@dataclass\n",
    "class GRPOScriptArguments:\n",
    "    \"\"\"\n",
    "    Script arguments for GRPO training, specifically related to reward functions.\n",
    "    \"\"\"\n",
    "\n",
    "    reward_funcs: list[str] = field(\n",
    "        default_factory=lambda: [\"mae\"], \n",
    "        metadata={\n",
    "            \"help\": \"List of reward functions. Possible values: 'accuracy', 'format', 'reasoning_steps', 'repetition_penalty'\"        },\n",
    "    )\n",
    "\n",
    "    repetition_n_grams: int = field(\n",
    "        default=3,\n",
    "        metadata={\"help\": \"Number of n-grams for repetition penalty reward\"},\n",
    "    )\n",
    "    repetition_max_penalty: float = field(\n",
    "        default=-0.1,\n",
    "        metadata={\"help\": \"Maximum (negative) penalty for for repetition penalty reward\"},\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 221/221 [00:00<00:00, 5385.97 examples/s]\n",
      "Map: 100%|██████████| 49/49 [00:00<00:00, 4380.61 examples/s]\n",
      "Map:   0%|          | 0/52 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 52/52 [00:00<00:00, 5059.00 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 221\n",
      "Test set size: 49\n",
      "\n",
      "Validating train split:\n",
      "✓ All required fields present\n",
      "✓ Prompt format is correct\n",
      "\n",
      "Validating test split:\n",
      "✓ All required fields present\n",
      "✓ Prompt format is correct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = get_dataset(params=[\"LogD\"], subset_train=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading tokenizer\n",
      "tokenizer loaded\n",
      "chat template\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alisavin/AgenticADMET/openr1/lib/python3.11/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/alisavin/AgenticADMET/openr1/lib/python3.11/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/alisavin/AgenticADMET/openr1/lib/python3.11/site-packages/transformers/training_args.py:2077: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "/home/alisavin/AgenticADMET/openr1/lib/python3.11/site-packages/trl/trainer/grpo_trainer.py:390: UserWarning: The requested device cuda:0 is also being used for training. For higher throughput and to avoid out-of-memory errors, it is recommended to use a dedicated device for vLLM. If this is intentional, you may ignore this warning but should adjust `vllm_gpu_memory_utilization` accordingly.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    trust_remote_code=True,    # if required\n",
    "    torch_dtype=torch.bfloat16,  # if you used bf16\n",
    "    device_map=\"auto\"           # or \"cuda:0\", depending on your environment\n",
    ")\n",
    "\n",
    "model_args_i = Munch.fromDict({\n",
    "        \"model_name_or_path\": MODEL_NAME,\n",
    "        \"model_revision\": \"main\",\n",
    "        \"trust_remote_code\": False # TODO: everyboudy sets to True and default is True\n",
    "        })\n",
    "\n",
    "training_args_i = Munch.fromDict({\"chat_template\": \"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='') %}{%- for message in messages %}{%- if message['role'] == 'system' %}{% set ns.system_prompt = message['content'] %}{%- endif %}{%- endfor %}{{bos_token}}{{ns.system_prompt}}{%- for message in messages %}{%- if message['role'] == 'user' %}{%- set ns.is_tool = false -%}{{'<｜User｜>' + message['content']}}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is none %}{%- set ns.is_tool = false -%}{%- for tool in message['tool_calls']%}{%- if not ns.is_first %}{{'<｜Assistant｜><｜tool▁calls▁begin｜><｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\\\n' + '```json' + '\\\\n' + tool['function']['arguments'] + '\\\\n' + '```' + '<｜tool▁call▁end｜>'}}{%- set ns.is_first = true -%}{%- else %}{{'\\\\n' + '<｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\\\n' + '```json' + '\\\\n' + tool['function']['arguments'] + '\\\\n' + '```' + '<｜tool▁call▁end｜>'}}{{'<｜tool▁calls▁end｜><｜end▁of▁sentence｜>'}}{%- endif %}{%- endfor %}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is not none %}{%- if ns.is_tool %}{{'<｜tool▁outputs▁end｜>' + message['content'] + '<｜end▁of▁sentence｜>'}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message['content'] %}{{'<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>'}}{%- endif %}{%- endif %}{%- if message['role'] == 'tool' %}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{'<｜tool▁outputs▁begin｜><｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- set ns.is_output_first = false %}{%- else %}{{'\\\\n<｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{'<｜tool▁outputs▁end｜>'}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{'<｜Assistant｜>'}}{% endif %}\"})\n",
    "\n",
    "tokenizer = get_tokenizer(model_args_i, training_args_i)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "    \n",
    "script_args = GRPOScriptArguments()\n",
    "\n",
    "reward_functions = get_reward_functions(script_args, model_name=\"tuned_v3_correct_format_v2\") #TODO: check trl they had someshere gpro example and used different rewards including lenght reward\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "        logging_dir=\"./logs/wandb/\",\n",
    "        num_train_epochs=12,             # Total number of training epochs\n",
    "        per_device_train_batch_size=16,  # Batch size per device during training\n",
    "        per_device_eval_batch_size=16,   # Batch size for evaluation TODO: why it says this   File \"/home/alisavin/AgenticADMET/train.py\", line 534, in <module>\n",
    "        gradient_accumulation_steps=4,  # Accumulate gradients to simulate larger batch size\n",
    "        learning_rate=1e-6,            # Initial learning rate for AdamW optimizer\n",
    "        warmup_ratio=0.1,              # Linear warmup over warmup_ratio fraction of training steps\n",
    "        weight_decay=0.01,             # Apply weight decay to all layers except bias and LayerNorm weights\n",
    "        logging_steps=1,              # Log every X updates steps\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_first_step=True,\n",
    "        evaluation_strategy=\"epoch\",    # Evaluate every `eval_steps`\n",
    "        save_strategy=\"no\",      # Disables regular checkpoints\n",
    "        save_total_limit=0,      # Makes sure no checkpoints are kept\n",
    "        load_best_model_at_end=False,  # Disables saving the best model\n",
    "        dataloader_num_workers=4,      # Number of subprocesses to use for data loading\n",
    "        seed=42,                       # Random seed for reproducibility\n",
    "        bf16=True,                     # Use mixed precision BFP16 training #TODO: ??????\n",
    "        push_to_hub=False,             # Whether to push the final model to Hugging Face Hub\n",
    "        report_to=[\"wandb\"],              # Reporting to no one\n",
    "        run_name=\"test\",\n",
    "        disable_tqdm=False,\n",
    "        gradient_checkpointing=True,   # Enable gradient checkpointing        \n",
    "        remove_unused_columns=False,\n",
    "        do_train=True,\n",
    "        # do_eval=True, #TODO: use\n",
    "        gradient_checkpointing_kwargs={\"use_reentrant\": False}, # TODO: use\n",
    "        lr_scheduler_type=\"cosine_with_min_lr\",\n",
    "        lr_scheduler_kwargs={\"min_lr_rate\": 0.1},\n",
    "        max_steps=-1, #TODO: change to -1\n",
    "    )\n",
    "\n",
    "grpo_config = GRPOConfig(\n",
    "    **training_args.to_dict(), # Convert TrainingArguments to dictionary and unpack\n",
    "    **{ \n",
    "    # REMOVED model_init_kwargs here \n",
    "    # We are passing the instantiated 'model' object, so GRPOTrainer doesn't need model_init_kwargs\n",
    "    },\n",
    "    num_generations=8, #TODO: 16\n",
    "    use_vllm=True, #TODO: use True\n",
    "    vllm_device=\"cuda:0\",\n",
    "    vllm_gpu_memory_utilization=0.25, # TODO: 0.25 0.7\n",
    "    vllm_max_model_len=2048, #TODO: 2048\n",
    "    max_prompt_length=800, #TODO: 800+\n",
    "    max_completion_length=1024, #TODO: 1024+ (better 2048/4048 and more)\n",
    "    temperature=0.7,\n",
    "    reward_weights=[1.0]\n",
    "    )\n",
    "\n",
    "model_args = ModelConfig(model_name_or_path=MODEL_NAME, use_peft=True)\n",
    "\n",
    "grpo_trainer = GRPOTrainer2(\n",
    "    model=model,                      # Our initialized Qwen model\n",
    "    reward_funcs=reward_functions,    # List of reward functions from previous step\n",
    "    args=grpo_config,                # GRPOConfig (created from TrainingArguments)\n",
    "    train_dataset=dataset['train'],   # Training dataset\n",
    "    eval_dataset=dataset['validation'],    # Evaluation dataset\n",
    "    processing_class=tokenizer, #TODO: check callback from config\n",
    "    peft_config=get_peft_config(model_args) #TODO: check # label_names\n",
    "    # peft_config=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alisavin/AgenticADMET/openr1/lib/python3.11/site-packages/transformers/trainer.py:3423: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)\n",
      "Warning: The following arguments do not match the ones in the `trainer_state.json` within the checkpoint directory: \n",
      "\tlogging_steps: 1 (from args) != 10 (from trainer_state.json)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/alisavin/AgenticADMET/notebooks/wandb/run-20250227_022636-6rex2844</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vladvin-org/huggingface/runs/6rex2844' target=\"_blank\">test</a></strong> to <a href='https://wandb.ai/vladvin-org/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/vladvin-org/huggingface' target=\"_blank\">https://wandb.ai/vladvin-org/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/vladvin-org/huggingface/runs/6rex2844' target=\"_blank\">https://wandb.ai/vladvin-org/huggingface/runs/6rex2844</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alisavin/AgenticADMET/openr1/lib/python3.11/site-packages/transformers/trainer.py:3119: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint_rng_state = torch.load(rng_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsed correctly 3.8 2.1\n",
      "parsed correctly 1.85 2.1\n",
      "parsed correctly 2.5 2.1\n",
      "parsed correctly 2.8 2.1\n",
      "parsed correctly 3.5 2.1\n",
      "parsed correctly 3.2 2.1\n",
      "parsed correctly 3.5 2.1\n",
      "parsed correctly 2.8 2.1\n",
      "parsed correctly 2.5 1.9\n",
      "parsed correctly 1.25 1.9\n",
      "parsed correctly 3.5 1.9\n",
      "parsed correctly 1.3 1.9\n",
      "parsed correctly 3.8 1.9\n",
      "parsed correctly 4.7 1.9\n",
      "parsed correctly 2.5 1.9\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (16) must match the existing size (2) at non-singleton dimension 0.  Target sizes: [16].  Tensor sizes: [2]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_result \u001b[38;5;241m=\u001b[39m \u001b[43mgrpo_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/alisavin/AgenticADMET/outputs/2025-02-26/22-18-57/checkpoint-60/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/AgenticADMET/openr1/lib/python3.11/site-packages/transformers/trainer.py:2241\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2239\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2242\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2246\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/AgenticADMET/openr1/lib/python3.11/site-packages/transformers/trainer.py:2548\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2541\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2542\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2543\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2544\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2545\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2546\u001b[0m )\n\u001b[1;32m   2547\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2548\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2551\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2552\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2553\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2554\u001b[0m ):\n\u001b[1;32m   2555\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2556\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/AgenticADMET/openr1/lib/python3.11/site-packages/transformers/trainer.py:3692\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3689\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mtrain):\n\u001b[1;32m   3690\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m-> 3692\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3693\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[1;32m   3694\u001b[0m     loss_mb \u001b[38;5;241m=\u001b[39m smp_forward_backward(model, inputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps)\n",
      "File \u001b[0;32m~/AgenticADMET/openr1/lib/python3.11/site-packages/trl/trainer/grpo_trainer.py:627\u001b[0m, in \u001b[0;36mGRPOTrainer._prepare_inputs\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    625\u001b[0m         reward_kwargs \u001b[38;5;241m=\u001b[39m {key: [example[key] \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m inputs] \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m keys}\n\u001b[1;32m    626\u001b[0m         output_reward_func \u001b[38;5;241m=\u001b[39m reward_func(prompts\u001b[38;5;241m=\u001b[39mprompts, completions\u001b[38;5;241m=\u001b[39mcompletions, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mreward_kwargs)\n\u001b[0;32m--> 627\u001b[0m         \u001b[43mrewards_per_func\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(output_reward_func, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m    629\u001b[0m \u001b[38;5;66;03m# Gather the reward per function: this part is crucial, because the rewards are normalized per group and the\u001b[39;00m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;66;03m# completions may be distributed across processes\u001b[39;00m\n\u001b[1;32m    631\u001b[0m rewards_per_func \u001b[38;5;241m=\u001b[39m gather(rewards_per_func)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (16) must match the existing size (2) at non-singleton dimension 0.  Target sizes: [16].  Tensor sizes: [2]"
     ]
    }
   ],
   "source": [
    "train_result = grpo_trainer.train(resume_from_checkpoint=\"/home/alisavin/AgenticADMET/outputs/2025-02-26/22-18-57/checkpoint-60/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openr1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
