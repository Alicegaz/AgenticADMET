{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Stage\n",
    "\n",
    "Once you install the required libraries, you can start by importing the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/AgenticADMET/openr1/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import math\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "# Import PyTorch and Hugging Face Transformers\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    TrainerCallback,\n",
    "    TrainerControl,\n",
    "    TrainerState,\n",
    ")\n",
    "\n",
    "# Import dataset utilities\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Import libraries from TRL (Transformers Reinforcement Learning)\n",
    "from trl import (\n",
    "    GRPOConfig, \n",
    "    SFTTrainer\n",
    ")\n",
    "\n",
    "# Import math-related utilities\n",
    "from latex2sympy2_extended import NormalizationConfig\n",
    "from math_verify import LatexExtractionConfig, parse, verify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing our Base Model\n",
    "\n",
    "Since DeepSeek team chose DeepSeek-V3 as their base model to create R1 Zero and R1, but it‚Äôs quite huge **685 GB üíÄ in size** which is obviously not in our reach.\n",
    "\n",
    "To keep it simple, we will use a much smaller base model [Qwen/Qwen2.5‚Äì0.5B-Instruct](https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct) (0.9 GB in size). If you have a higher GPU RAM that can even load unquantized LLMs, you can go for a bigger model, such as [Qwen/Qwen2.5‚Äì7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-7B-Instruct).\n",
    "\n",
    "Let‚Äôs take a look at some of the specification of our base model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 151665\n",
      "Model max length: 16384\n",
      "Pad token: <ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>\n",
      "EOS token: <ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>\n"
     ]
    }
   ],
   "source": [
    "# MODEL_NAME = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "# MODEL_NAME = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\"\n",
    "MODEL_NAME = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "OUTPUT_DIR = \"data/Qwen-GRPO-training\" # For saving our trained model\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Initialize tokenizer with chat template\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    trust_remote_code=True,\n",
    "    padding_side=\"right\"\n",
    ")\n",
    "\n",
    "# Set pad token if not set\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Vocabulary size: {len(tokenizer)}\")\n",
    "print(f\"Model max length: {tokenizer.model_max_length}\")\n",
    "print(f\"Pad token: {tokenizer.pad_token}\")\n",
    "print(f\"EOS token: {tokenizer.eos_token}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are some basic info about the model, take a look at the total number of parameters our base model has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 1,777,088,000\n"
     ]
    }
   ],
   "source": [
    "# Initialize base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "print(f\"Model parameters: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Close to 0.5B params, let‚Äôs print a simple response from it and then we will move on to next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "def get_random_half_dict(original_dict):\n",
    "    # Get the number of items to select (half of the dictionary length)\n",
    "    num_items = len(original_dict) // 10\n",
    "    \n",
    "    # Convert dictionary items to a list\n",
    "    items = list(original_dict.items())\n",
    "    \n",
    "    # Randomly select half of the items\n",
    "    selected_items = random.sample(items, num_items)\n",
    "    \n",
    "    # Convert back to dictionary\n",
    "    result_dict = dict(selected_items)\n",
    "    \n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check CUDA availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Move model to the appropriate device\n",
    "model.to(device)\n",
    "\n",
    "# Test basic inference\n",
    "def test_model_inference(user_input: str):\n",
    "    \"\"\"Test basic model inference with the loaded model and tokenizer.\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a decision maker chemist that predicts logD for a given small molecule based on its SMILES.\"}, #\"A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think><answer> answer here </answer>.\"},\n",
    "        {\"role\": \"user\", \"content\": user_input}\n",
    "    ]\n",
    "\n",
    "    # Apply chat template\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    # Tokenize and generate\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=1000,\n",
    "        do_sample=True,\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "def test_model_inference_v2(text: str):\n",
    "\n",
    "    # Tokenize and generate\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100000,\n",
    "        do_sample=True,\n",
    "        # temperature=0.7\n",
    "    )\n",
    "\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # {'LogD': 1.5, 'KSOL': 340.0, 'MLM': nan, 'MDR1-MDCKII': 1.6, 'HLM': nan}\n",
    "# # Test the model\n",
    "# prompt = f\"\"\"What are the LogD, KSOL, MLM, MDR1-MDCKII, HLM for this small molecule CO[C@H]1C[C@H](N2N=CC3=C(C(=O)NC4=CC=C5CNCC5=C4)C=C(Cl)C=C32)C1?\n",
    "# Here is the scientiffic publication I found that may help you:\n",
    "# Multi-Parameter Optimization: Identifying high quality compounds with a balance of properties\n",
    "# A successful drug that passes the hurdles of clinical trials to gain approval and a strong market position must exhibit a delicate balance of biological and physicochemical properties. Such a compound must, of course, be potent against its intended physiological target(s); however, it must also have appropriate pharmacokinetics to reach the site of the target at a sufficiently high concentration and for an appropriate duration via the intended route of administration. Furthermore, for the compound to be safely administered, it must avoid unintended side-effects, drug-drug interactions and non-specific or idiosyncratic toxicities at the therapeutic dose. The goal of drug discovery is to identify a successful compound as efficiently as possible. But, as the history of drug discovery has proved, this is a challenge of significant proportions [1].\n",
    "# This task is made even more difficult by the fact that, in drug discovery, data on the behavior of the compound in the ultimate target patient population, i.e. humans, is not available. This has led to the development of a plethora of in silico, in vitro, and in vivo animal models from which we can (hopefully) infer the likely in vivo efficacy, disposition and safety of a compound in humans. These include models for the prediction and measurement of potency and selectivity against molecular targets or off-targets; absorption, distribution, metabolism and elimination (ADME) properties; cell-based measurements of pharmacological activity and toxicity; and animal models of pharmacology, pharmacokinetics and toxicity. The cost and throughput of these techniques vary, from in silico methods which typically have the lowest cost and highest throughput, through in vitro and cell-based assays to lengthy and expensive in vivo studies, the use of which we would also like to minimize for ethical reasons. Therefore, drug discovery is a process of simultaneously optimizing all of these factors as compounds are designed, synthesized and progressed through a cascade of assays to accumulate data.\n",
    "# This balancing act is difficult to achieve through a purely intellectual process. Psychologists have repeatedly demonstrated that people are very poor at making decisions based on complex and uncertain data when there is a lot at stake, such as in drug discovery. Several biases in decision-making (described as cognitive biases) have been identified that can detrimentally affect efficiency and productivity in drug discovery. A detailed discussion of some of these, with examples, may be found in [2]; however two illustrative examples are:\n",
    "# ÔÇ∑ Confirmation bias: The tendency to seek data that confirms a pre-formed hypothesis, rather than perform experiments designed to yield results to challenge the hypothesis. This can lead to a premature focus on a small range of options, which may lead to missed opportunities or late stage failures of compounds that have been progressed too far in the search for the one piece of data that would prove the point.\n",
    "# ÔÇ∑ Excess focus on certainty: The tendency to seek additional data to be ‚Äòabsolutely certain‚Äô of a critical factor, even when this data adds little value at a high cost. Often a more significant increase in the confidence around a slightly less important factor may have a greater effect on the overall chance of success. This can lead to inefficient use of resources when considering multiple property requirements and to late stage, expensive failures.\n",
    "# The historical evidence regarding the attrition and productivity of pharmaceutical research and development supports this observation. The increasing complexity and volume of data being generated in drug discovery has not improved success rates in development ‚Äì 11% in 2000 [3] versus 12% in 2010 [1] ‚Äì while the cost per marketed drug has continued to escalate ‚Äì from an estimated fully capitalized cost of $802M in 2001 [4] to $1,778M in 2010 [1] ‚Äì and productivity, as measured by the number of registered new chemical entities, has fallen [5]. There are a wide range of theories regarding the underlying cause of these effects, but it is safe to conclude that generating additional, early-stage data has not resulted in the improvements anticipated in the outcomes.\n",
    "# Fortunately, we may learn from other fields that face the same need to balance many factors in the design of a successful solution. These fields range from engineering disciplines, such as aerospace or automotive design, to economics. The resulting methods are commonly described under the broad term ‚ÄúMulti-parameter Optimization‚Äù (MPO) or sometimes also ‚ÄúMulti-dimensional Optimization‚Äù (MDO) or ‚ÄúMulti-objective Optimization‚Äù (MOOP). For convenience, we will use the term MPO to describe all of the methods in this review.\n",
    "# There is a significant difference between applications of MPO methods to drug discovery and other fields, in particular engineering. This relates to the quality of the data available on the potential designs or prototypes from which a selection must be made. In an engineering discipline, characteristics may commonly be measured to accuracies within parts per million or predicted computationally to within a fraction of a percent. This may be contrasted with drug discovery where measured properties, such as IC50 or Ki values, may have an experimental variability of a factor of two, while predictions may have statistical uncertainties of an order of magnitude. This dramatically increases the challenge because, even if an ideal compound exists among the available options, we cannot expect to identify it with absolute confidence, thus running the risk of missing opportunities for high quality drugs [6].\n",
    "# In our research into the requirements for an ideal MPO method for drug discovery, we identified the following factors that should be taken into account:\n",
    "# ÔÇ∑ Interpretability: The property criteria and their impact on compound priority should be easy to understand. A ‚Äòblack box‚Äô method that does not provide an easy way to understand why a compound has been classified in a given way is likely to be discounted. Furthermore, a ‚Äòblack box‚Äô does not provide any guidance on the way one should go about making improvements in order to increase the chance of success.\n",
    "# ÔÇ∑ Flexibility: Each project will have a different set of property criteria depending on the therapeutic objectives of the project, intended route of administration and competitive conditions in the market. The project team should be able to define appropriate criteria based on their experience or historical evidence.\n",
    "# ÔÇ∑ Weighting: The project team should be able to assign different weights to each property criterion, as different criteria will have different degrees of importance to the outcome of the project. For example identifying a compound that is potent against the intended target is critical, while other properties will be less important, particularly early in a project when there is an opportunity for redesign to overcome liabilities.\n",
    "# ÔÇ∑ Uncertainty: It is important to avoid rejecting potentially valuable compounds based on a property value that fails to meet a criterion if that value has a high level of uncertainty. The opportunity cost of incorrectly rejecting a good compound may be very high, particularly when the range of alternative options is limited.\n",
    "# Coincidentally, it seems that the development of a suitable MPO approach for drug discovery is itself an MPO problem!\n",
    "# One common question is, ‚ÄúCan‚Äôt this be easily solved by visualization of the data?‚Äù While visualization is necessary to understand and communicate results, it is not sufficient to allow conclusions to be easily drawn, given the complexity of the data at hand. One common approach is to plot multi-dimensional data, for example on a three dimensional graph with additional parameters shown by the colors and sizes of the points. An alternative is a ‚Äòtraffic light‚Äô view where the properties of each compound are shown in a table and colored according to whether they ‚Äòpass‚Äô (green), ‚Äòfail‚Äô (red) or are ‚Äòclose‚Äô (yellow) to the relevant criterion. However, even with only five-dimensional data, it is difficult to confidently draw a conclusion from these visualizations even before we consider the relative importance of each property or the uncertainty in the data. An MPO method helps a project team to define a set of criteria and use this pro-actively to guide their decisions to quickly target high quality compounds [7].\n",
    "# In this review, we will explore a range of different MPO approaches that have been applied to drug discovery and compare their strengths and weaknesses relative to the requirements described above. The methods that we will discuss in order of increasing sophistication, include ‚Äòrules-of-thumb‚Äô that provide chemists with guidelines for compound characteristics, simple pass/fail filters, Pareto optimization, desirability functions and probabilistic scoring, which brings together all of the requirements discussed above. We will also consider the role of chemical diversity to mitigate risk when selecting compounds for further investigation. Finally, we will illustrate some of the methods using examples taken from the literature before drawing our conclusions.\n",
    "# Rules of Thumb\n",
    "# Perhaps the most common approach used to consider the quality of compounds relative to criteria beyond potency are ‚Äòrules of thumb‚Äô that provide guidelines regarding desirable compound characteristics. The best known is undoubtedly Lipinski‚Äôs Rule of Five (RoF) [8], which proposes criteria for four basic characteristics that Lipinski identified as being satisfied by the majority of orally absorbed compounds, namely:\n",
    "# ÔÇ∑ Molecular Weight (MW) < 500\n",
    "# ÔÇ∑ Logarithm of the octanol:water partition coefficient (logP) < 5\n",
    "# ÔÇ∑ Number of Hydrogen Bond Donors (HBD) < 5\n",
    "# ÔÇ∑ Number of Hydrogen Bond Acceptors (HBA) < 10\n",
    "# Subsequently, several other rules have been proposed, for example Veber et al. [9] identified that most of the 1100 compounds they studied with oral bioavailability of greater than 20% in rats had less than 10 rotatable bonds and a Polar Surface Area (PSA) of less than 140 AÃä2. However, Lu et al. [10] repeated this study with a set of 434 compounds and showed that the criteria depended on the method used for calculation, providing one illustration of the need for flexibility in the criteria depending on the source of data.\n",
    "# Johnson et al. identified rules based on MW and the logarithm of the octanol:buffer partition coefficient at pH7.4 (logD) to achieve permeability and metabolic stability [11]. In this case, rather than expressing these rules as criteria for the individual characteristics, Johnson et al. identified correlations that led them to express the rules in terms of a ‚Äògolden triangle‚Äô that defines an optimal region in (MW,logD) space in which a compound should lie (illustrated in Figure 1).\n",
    "# elogD\n",
    "#  MW\n",
    "#  Figure 1. An illustration of the ‚ÄúGolden Triangle‚Äù [11] proposed by Johnson et al. Compounds within the shaded region in (MW, logD) space were found to have a higher chance of achieving better outcomes for permeability and metabolic stability. This is a convenient visual rule-of- thumb for selecting compounds.\n",
    "# Other rules, involving parameters such as the fraction of carbons which are sp3 hybridized [12] and the number of aromatic rings [13] have been proposed as measures of developability or likelihood of clinical success. Furthermore, Hughes et al. [14] studied the relationship between physicochemical properties and adverse events observed in in vivo toleration studies. They concluded that compounds with both calculated logP (clogP) > 3 and topological polar surface area (TPSA) < 75 AÃä2 had a significantly increased safety risk.\n",
    "# The undoubted popularity of these rules derives from their simplicity and interpretability, the first requirement for a good MPO method. It is very easy to calculate these characteristics and quickly check if a compound obeys these rules. Similarly it is easy to understand how to modify a compound that fails to meet these rules in order to improve its chance of success; it is clear how MW, HBD or HBA could be reduced and\n",
    "\n",
    "#  chemists have a good understanding of the influence of chemical functionalities on lipophilicity. Therefore, these rules-of-thumb provide an easy approach to selecting compounds and guiding their redesign.\n",
    "# The main disadvantage of these rules-of-thumb is also due to their simplicity. There may be a tendency to over-interpret simple rules and apply them with too much rigor. For example, does a compound with a MW of 501 have a significantly worse chance of oral absorption than one with MW of 500? Indeed, Lipinski‚Äôs original paper [8] suggested that two or more failures against the RoF criteria were required to significantly decrease the chance of oral absorption, so the rules were not intended to be applied individually.\n",
    "# These rules are derived from a review of historically successful drugs and are often treated as absolute rules that define ‚Äòdrug likeness.‚Äô However, compounds for different therapeutic indications or routes of administration may require different characteristics or be more tolerant to violations of these rules. For example, there has been a tendency for the RoF to be considered as a definition of the conditions for ‚Äòdrug likeness‚Äô when it is only based on analysis of the requirements for orally absorbed drugs. Drugs intended for topical, IV, inhaled or other routes of administration can violate some or all of the rules without a significant impact on their chance of success [15]. Therefore, the criteria and weight given to each of these rules-of- thumb should be defined or applied flexibly according to the therapeutic objectives of a project. Unfortunately, this is not a straightforward exercise, as careful statistical analysis of a large number of compounds is required to identify statistically significant criteria.\n",
    "# The majority of the characteristics used in these rules-of-thumb do not have any underlying uncertainty, as they are simple values calculated from the molecular structure. The principal exception to this is lipophilicity (logP or logD) which, if calculated, typically has an uncertainty (root-mean-square-error) of at least 0.5 log units. Therefore, care should be taken when drawing conclusions regarding compounds close to the criterion for lipophilicity.\n",
    "# Finally, we should consider the confidence in the ‚Äòprediction‚Äô by a rule-of-thumb. As an illustrative example, we applied the RoF to a set of 1191 marketed drugs labeled according to whether they have been approved for oral administration and the results are shown in Table 1. Although, one should be careful not to over interpret these results, we can see that passing the RoF is not a guarantee of finding an orally available compound. This is not surprising, as the RoF was derived from observations of absorption and other factors such as first pass metabolism can limit oral bioavailability. However, the specificity of the RoF is also low (21%), as more non-orally administered compounds pass the RoF than fail and a significant proportion of compounds that fail the RoF are orally administered.\n",
    "# Table 1. The results of applying Lipinski‚Äôs Rule of Five to 1191 marketed drugs labeled as oral or non-oral according to their approved route of administration.\n",
    "# Oral 709 59 Non-oral 333 90\n",
    "# In summary, rules-of-thumb can provide very convenient and easily applied guidelines for the selection of compounds with a greater chance of yielding successful drugs, if used in the appropriate context. However, one should be careful about being overly rigid regarding their application as this could lead to missed opportunities.\n",
    "#   RoF result\n",
    "#  Pass\n",
    "# (ÔÇ£1 RoF Failure)\n",
    "#  Fail\n",
    "# (>1 RoF Failure)\n",
    "                  \n",
    "#  Filtering\n",
    "# Another simple approach to applying multiple criteria to the selection of compounds is sequential filtering. In this process the compounds are compared to series of criteria; those that fail to meet a criterion are discarded while those that meet the criterion are progressed for comparison against the next criterion in the sequence. The hope is that one or more ‚Äòideal‚Äô compounds will emerge from the sequence of filters, having passed all of the criteria. Filtering offers the benefit that interpretation is straightforward, because if a compound fails one or more criteria this clearly indicates the focus for improving the compound.\n",
    "# The set of criteria against which compounds are compared can be based on any relevant properties, whether calculated or experimental. This offers the flexibility that a drug discovery project may choose criteria that are tailored to the project objectives, based on the experience of the project team or historical data for successful compounds for the intended therapeutic indication. These criteria are sometimes referred to as a target product profile (TPP) and an illustrative example of such a profile for identification of a lead compound for an orally dosed compound is shown in Table 2. Early in a project, for example when choosing a screening library for high throughput screening, it is also common to apply the criteria indicated by one of more of the rules-of- thumb discussed above as sequential filters.\n",
    "# Table 2. An example of a target product profile for selection of a lead compound intended for oral administration.\n",
    "# Potency against target (Ki)\n",
    "# Selectivity against related off-targets\n",
    "# Physicochemical\n",
    "# LogP\n",
    "# Solubility\n",
    "# MW\n",
    "# ADME\n",
    "# Caco-2* permeability (Papp)\n",
    "# Intrinsic Clearance in Human Liver Microsomes (Clint) Absence of P-glycoprotein transport (Caco2 BA:AB)\n",
    "# Safety\n",
    "# Avoid Cytochrome P450-mediated drug-drug interactions (Ki for CYP3A4, CYP2C9, CYP2D6, CYP1A2) Avoid interaction with hERG potassium ion channel (IC50)\n",
    "# Cytotoxicity in HepG2‚Ä† cells (LD50)\n",
    "# *Human epithelial colorectal adenocarcinoma cell line [16] ‚Ä†Hepatocellular carcinoma cell line [17]\n",
    "# One challenge of filtering is that it is common for no compounds to emerge from the end of the sequence; there are several possible reasons for this:\n",
    "# ÔÇ∑ There are often conflicts between the property criteria; improving one property often leads to an adverse change in another. In these situations, the relative importance of each criterion should be taken into account as this defines acceptable trade-offs against conflicting properties.\n",
    "# ÔÇ∑ Simple yes/no criteria may be too strict; For example, if a compound meets all of the criteria in the TPP, except that it has a logP of 5.1 versus a criterion of <5, does it make sense to reject it?\n",
    "# ÔÇ∑ There may have been a mis-measurement or mis-prediction; one or more compounds may have been incorrectly rejected due to the experimental variability or statistical error in a prediction.\n",
    "# The last of these is probably the biggest concern about filtering because, as we discussed above, there is significant uncertainty in almost all of the data which is available in early drug discovery. If we consider a\n",
    "#   Property\n",
    "#  Criterion\n",
    "#  Pharmacology\n",
    "#      <100 nM >100 ÔÇ¥\n",
    "# <4\n",
    "# >100 ÔÅ≠M <450 Da\n",
    "# >10ÔÇ¥10-6 cm/s\n",
    "# <25 ÔÅ≠L/min/mg protein <3\n",
    "# >1 ÔÅ≠M >10 ÔÅ≠M >1 mM\n",
    "                               \n",
    "#  simple illustrative example in which we have 10 filters that are each 90% accurate in passing/failing a compound, the probability of an ideal compound emerging, even if it was present in the set being filtered, is only 35% (p = 0.910 assuming independence of the error in each filter). Therefore, even in this optimistic case, sequential filtering is more likely to discard an ideal compound than accept it. Furthermore, there is a significant chance of incorrectly passing a poor compound; in this example, if a compound should correctly fail only one of the criteria, the probability of it being incorrectly accepted is 4%. Given that there are typically many more poor compounds than good, this means that any ideal compound that is fortunate enough to be correctly passed by all of the filters is likely to be swamped by poor compounds incorrectly accepted.\n",
    "# Therefore, despite the simplicity and easy interpretation of filtering, it should be treated with caution. The process accumulates error without that being transparent, running the risk of rejecting good compounds and missing opportunities to find a high quality drug.\n",
    "# Calculated Metrics\n",
    "# Rather than defining criteria for multiple, individual properties these may be combined to calculate a single metric that can be optimized to guide selection or design. One of the earliest and most commonly applied metrics is the Ligand Efficiency (LE) proposed by Hopkins et al. [18], with the goal of mitigating the tendency to focus too heavily on the optimization of potency at the cost of other necessary properties. LE was derived from the observation that smaller compounds tend to have better physicochemical and ADME properties than large compounds. Therefore, given two equally potent compounds it is preferable to choose the smaller. Or, alternatively, increasing potency without significantly increasing compound size is desirable. LE is defined as:,\n",
    "# where G is the free energy of binding and NH is the number of heavy (i.e. non-Hydrogen) atoms in the compound. In more common units, this may be expressed as:,\n",
    "# where pIC50 = -log(IC50) and the IC50 is expressed in molar concentration.\n",
    "# The use of the LE metric is particularly popular in fragment-based drug design [19], where the starting point is typically one or more small fragments with low binding affinity and new compounds are designed by growing or linking these fragments to identify a larger compound with sufficient potency. Although the initial fragments bind only weakly, they have a high LE due to their small size and the optimization process may be guided by increasing the potency while maintaining a high LE.\n",
    "# The LE metric inspired other calculated optimization metrics, for example Ligand Lipophilicity Efficiency (LLE) [20], also known as Lipophilic Efficiency (LipE):\n",
    "# where a calculated value of logP is often used. This was motivated by the desire to maximize potency while maintaining as low a lipophilicity as possible, due to the association between high lipophilicity and several issues including poor solubility, membrane permeation and metabolic stability, lack of selectivity and a higher risk of non-specific toxicity [21] [22].\n",
    "# The range of efficiency metrics has been further extended to include percent efficiency index (PEI), defined as the percent inhibition (as a fraction between 0 and 1) divided by MW in kDa; binding efficiency index (BEI), defined as pIC50 divided by MW in kDa; and surface efficiency index (SEI), defined as pIC50 divided by PSA in 100s of AÃä. All of these combine a measure of potency related to another property representing the ‚Äòdrug- likeness‚Äô of the compound and are reviewed in detail in [23]. More complex derivatives of these efficiency indexes have also been proposed including ligand efficiency-dependent lipophilicity (LEDL), defined as logP divided by LE [24] and ‚Äòfit quality‚Äô [25].\n",
    "# These calculated metrics have the advantage that they are simple to apply, as only a single value must be monitored during optimization. They are also easy to interpret ‚Äì Increase potency while minimizing the increase in compound size or lipophilicity ‚Äì although this ease of interpretation may be sacrificed somewhat by the more complex efficiency indexes such as LEDL.\n",
    "#  In many cases rules-of-thumb have been developed for selection of high quality compounds using these metrics. For example, it has been proposed that a LLE of 6 or higher is preferable, corresponding to a potency of better than 10 nM with a logP of 2. Again these provide useful guidelines when applied in an appropriate context, but the same caveats apply here as to the rules-of-thumb discussed above, in particular:\n",
    "# ÔÇ∑ Potency and logP values have significant uncertainty, particularly when predicted, yet it is rare to see the uncertainties propagated through the calculation of the efficiency metric to consider the confidence with which compounds may be chosen based on these metrics.\n",
    "# ÔÇ∑ As noted above, increasing compound size, MW and logP significantly increases the chance of encountering issues with poor physicochemical, ADME and toxicity. However, the correlation with these properties is not perfect, so it may be inappropriate to make selections based too strictly on these metrics, particularly when options are limited.\n",
    "# ÔÇ∑ These rules are not universal and are typically based on identification of orally administered drugs, so the project‚Äôs therapeutic objective should be considered carefully before choosing a criterion.\n",
    "# It is noteworthy that there is a close relationship between the optimization based on these metrics and a recent trend to optimize compounds based on measurements of the thermodynamic parameters of binding using biophysical measurements [26]. This strategy suggests that it is better to increase binding affinity (or equivalently decrease the free energy G, as strong binding is equivalent to a reduction in free energy) by introducing an interaction dominated by decreasing the enthalpy of binding ( H) rather than one dominated by increasing entropy ( S) ‚Äì note G = H - T S, where T is the temperature. Decreasing the binding free energy by reducing the enthalpy is achieved by forming a specific interaction with the target, for example a hydrogen bond with a residue in the binding pocket, which will typically improve the LE or LLE. The free energy can also be reduced by increasing the entropy and this can be achieved by displacing coordinated water molecules from the binding pocket into bulk solvent, for example by adding a bulky lipophilic group to occupy the binding pocket. However, this is often detrimental in the long-run, as such a non-specific interaction will increase the chance of off-target binding or non-specific toxicity and this is reflected by a decrease in the LE or LLE [27].\n",
    "# Pareto Optimization\n",
    "# The concept known as Pareto optimality was proposed by an Italian economist Vilfredo Pareto in the early 20th Century [28]. He suggested that, when considering multiple parameters, there may not be a single best combination of parameters, but rather a family of solutions that each represents a different, optimal combination. More specifically, a solution to a multi-parameter optimization problem is considered to be a Pareto optimum if there is no other solution that is better in all of the parameters.\n",
    "# To illustrate this, consider the two-parameter example, illustrated in Figure 2(a), of a hypothetical drug discovery project which wishes to achieve an optimal balance of potency and metabolic stability to achieve good in vivo exposure and hence efficacy. Ideally, the project would like to identify a compound with high potency (pIC50) and good stability in human liver microsomes (% remaining after incubation for 40 minutes). This ideal goal is represented by the top right corner of the plots in Figure 2. However, as this ideal may be difficult or impossible to achieve, the project would like to find a good balance between potency and metabolic stability. The points shown as solid points in Figure 2 represent compounds with different, Pareto optimal combinations of these two parameters. For example, the point labeled A has no points that have both better potency and better metabolic stability, i.e. there are no points to the right and above; such a point is described as ‚Äònon-dominated‚Äô. Contrast this with the point labeled B, which has a point, C, to the right and above representing a compound that is better in both parameters; B is ‚Äòdominated‚Äô by C. Note that the non- dominated points define a boundary, known as the ‚ÄòPareto front‚Äô and each represents a candidate for further investigation to identify the best balance of potency and metabolic stability to achieve in vivo efficacy.\n",
    "# The concept of Pareto optimality may be generalized to Pareto rank, whereby a point is ranked according to the number of points by which it is dominated, a rank-0 point is non-dominated, rank-1 is dominated by only a single point, etc. This allows compounds to be ranked according to how close they are to the optimum front.\"\"\"\n",
    "\n",
    "with open(\"polaris-antiviral-admet-2025.json\", \"r\") as f:\n",
    "    polaris_dataset = json.load(f)\n",
    "\n",
    "polaris_dataset_train = polaris_dataset.copy()\n",
    "polaris_dataset_train.pop(\"CO[C@H]1C[C@H](N2N=CC3=C(C(=O)NC4=CC=C5CNCC5=C4)C=C(Cl)C=C32)C1\")\n",
    "polaris_dataset_train = get_random_half_dict(polaris_dataset_train)\n",
    "# , KSOL, MLM, MDR1-MDCKII, HLM\n",
    "# prompt = f\"\"\"\n",
    "# LogD <protocol>: like solubility - but then in fatty tissue - LogD is a measure of a molecule's lipophilicity, or how well it dissolves in fat. LogD is calculated by comparing a molecule's solubility in octanol, a fat-like substance, to its solubility in water.\n",
    "# <protocol> Lipophilicity is possibly the most important physicochemical parameter for any potential drug candidate. Lipophilicity measurements are valuable for understanding how drugs are dissolved in plasma and other aqueous biological fluids. Lipophilicity is typically accessed as the distribution of the tested compound between two solvents - typically non-aqueous organic (1-octanol) and aqueous (pH-buffered water), and then LogP is expressed as a Log of the concentration ratio between two phases. LogP is widely used in cheminformatics and is a component of Lipinski‚Äôs ‚Äúrule of five‚Äù, which is a golden standard to evaluate a drug-likeness of a compound. According to this rule, the successful drug candidate should possess LogP value not greater than 5. \n",
    "\n",
    "# LogD is a distribution coefficient widely used to measure the lipophilicity of ionizable compounds, where the partition is a function of the pH. \n",
    "# For nonionizable compounds LogP = LogD throughout pH range, whereas for ionizable compounds LogD takes into account the partition of both ionized and non-ionized forms. LogD is more convenient for practical measurements, as it takes into account solution pH, which is important for the analysis of the drug candidate properties in various biological media with different pH values. The shake flask method is considered the gold standard technique for determining log D.\n",
    "# </protocol>\n",
    "\n",
    "# Mouse Liver Microsomal stability (MLM, protocol): This is a stability assay that tests how quickly a molecule gets broken down by mouse liver microsomes. This is a useful assay that can be used as an estimate on how long a molecule will reside in the mouse body before it gets cleared.\n",
    "# Human Liver Microsomal stability (HLM, protocol): This is a stability assay that tests how quickly a molecule gets broken down by human liver microsomes. This is a useful assay that can be used as an estimate on how long a molecule will reside in the human body before it gets cleared.\n",
    "# Solubility (KSOL, protocol): solubility is essential for drug molecules: this heavily affects the pharmacokinetic and dynamics ('PKPD') of the molecule in the human body.\n",
    "# Cell permeation (MDR1-MDCKII, protocol): MDCKII-MDR1 is a cell line that's used to model cell permeation i.e. how well drug compounds will permeate cell layers. For coronaviruses this is a critical endpoint because there is increasing evidence that afflictions such as long-covid are caused by (remnant) virus particles in the brain, and blood-brain-barrier (BBB) permeation is critical for drug candidates to reach the brain.\n",
    "\n",
    "\n",
    "# What are the LogD for this small molecule CO[C@H]1C[C@H](N2N=CC3=C(C(=O)NC4=CC=C5CNCC5=C4)C=C(Cl)C=C32)C1?\n",
    "# Here is the set of samples that can help make a discovery\n",
    "# {str(polaris_dataset_train)}\n",
    "# \"\"\"\n",
    "# prompt = f\"\"\"\n",
    "# Background:\n",
    "\n",
    "# LogD is a measure of a molecule's lipophilicity, or its ability to dissolve in fats. It's a crucial property in drug discovery, as it influences factors like absorption, distribution, metabolism, and excretion (ADME).\n",
    "# Mouse Liver Microsomal stability (MLM, protocol): This is a stability assay that tests how quickly a molecule gets broken down by mouse liver microsomes. This is a useful assay that can be used as an estimate on how long a molecule will reside in the mouse body before it gets cleared.\n",
    "# Human Liver Microsomal stability (HLM, protocol): This is a stability assay that tests how quickly a molecule gets broken down by human liver microsomes. This is a useful assay that can be used as an estimate on how long a molecule will reside in the human body before it gets cleared.\n",
    "# Solubility (KSOL, protocol): solubility is essential for drug molecules: this heavily affects the pharmacokinetic and dynamics ('PKPD') of the molecule in the human body.\n",
    "# Cell permeation (MDR1-MDCKII, protocol): MDCKII-MDR1 is a cell line that's used to model cell permeation i.e. how well drug compounds will permeate cell layers. For coronaviruses this is a critical endpoint because there is increasing evidence that afflictions such as long-covid are caused by (remnant) virus particles in the brain, and blood-brain-barrier (BBB) permeation is critical for drug candidates to reach the brain.\n",
    "\n",
    "# Additional Context:\n",
    "\n",
    "# You are provided with a dataset of similar molecules and their corresponding LogD values:\n",
    "# {str(polaris_dataset_train)}\n",
    "\n",
    "\n",
    "# What is the LogD for this small molecule CO[C@H]1C[C@H](N2N=CC3=C(C(=O)NC4=CC=C5CNCC5=C4)C=C(Cl)C=C32)C1?\"\"\"\n",
    "# # test_input = f\"\"\"A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think>\n",
    "# # <answer> answer here </answer>. User: {prompt}. Assistant:\"\"\"\n",
    "# response = test_model_inference(prompt)\n",
    "# print(f\"Test Input: {prompt}\")\n",
    "# print(f\"Model Response: {response}\")\n",
    "\n",
    "\n",
    "# prompt = f\"\"\"\n",
    "# Background:\n",
    "# LogD is a measure of a molecule's lipophilicity, or its ability to dissolve in fats. It's a crucial property in drug discovery, as it influences factors like absorption, distribution, metabolism, and excretion (ADME).\n",
    "# Mouse Liver Microsomal stability (MLM, protocol): This is a stability assay that tests how quickly a molecule gets broken down by mouse liver microsomes. This is a useful assay that can be used as an estimate on how long a molecule will reside in the mouse body before it gets cleared.\n",
    "# Human Liver Microsomal stability (HLM, protocol): This is a stability assay that tests how quickly a molecule gets broken down by human liver microsomes. This is a useful assay that can be used as an estimate on how long a molecule will reside in the human body before it gets cleared.\n",
    "# Solubility (KSOL, protocol): solubility is essential for drug molecules: this heavily affects the pharmacokinetic and dynamics ('PKPD') of the molecule in the human body.\n",
    "# Cell permeation (MDR1-MDCKII, protocol): MDCKII-MDR1 is a cell line that's used to model cell permeation i.e. how well drug compounds will permeate cell layers. For coronaviruses this is a critical endpoint because there is increasing evidence that afflictions such as long-covid are caused by (remnant) virus particles in the brain, and blood-brain-barrier (BBB) permeation is critical for drug candidates to reach the brain.\n",
    "\n",
    "# Additional Context:\n",
    "\n",
    "# Here is the first batch of molecules we recieved wet lab results for:\n",
    "# {str(polaris_dataset_train)}\n",
    "\n",
    "# Manual Rules Used by Chemists in Drug Discovery\n",
    "\n",
    "# When evaluating a batch of molecules with ADMET data, experienced chemists use a combination of knowledge, intuition, and established rules to guide their decision-making. Here are some of the key manual rules they often employ:\n",
    "\n",
    "# Golden Triangle Rule: This rule suggests that compounds with 200 Da ‚â§ molecular weight ‚â§ 350 Da and -2 ‚â§ logD ‚â§ 5 are more likely to have a favorable ADMET profile.\n",
    "\n",
    "# Number of rotatable bonds ‚â§ 10\n",
    "# Polar surface area ‚â§ 140 √Ö¬≤ or Total number of hydrogen bond donors and acceptors ‚â§ 12\n",
    "# Rule of Three (Ro3) : This rule is often applied in fragment-based drug discovery. It suggests that fragment hits should ideally have: ¬† \n",
    "\n",
    "# Molecular weight ‚â§ 300 Da\n",
    "# cLogP ‚â§ 3\n",
    "# Number of hydrogen bond donors and acceptors ‚â§ 3\n",
    "# Other Considerations:\n",
    "\n",
    "# \"Lead-likeness\" : Similar to drug-likeness, but with more relaxed criteria, as lead compounds are often optimized further. ¬† \n",
    "# Important Notes:\n",
    "\n",
    "# These rules are guidelines, not absolute requirements. There are exceptions to every rule, and the specific context of the drug discovery program should always be considered.\n",
    "# Experienced chemists often develop their own intuition and heuristics based on their experience and knowledge of specific drug targets and therapeutic areas.\n",
    "\n",
    "# As a cheif chemist DM, comprehensively study the results and look at a candidate molecule CO[C@H]1C[C@H](N2N=CC3=C(C(=O)NC4=CC=C5CNCC5=C4)C=C(Cl)C=C32)C1. Based on its bonds, fragments, atoms, first batch results and your chemistry knoledge, what do you think is its logD value, High-Risk (Pi>thresholdmax) ‚Üí Requires modification to reduce, Weakly Optimized (Pi<thresholdmin) ‚Üí Needs enhancement, Acceptable\"\"\"\n",
    "# test_input = f\"\"\"A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think>\n",
    "# # <answer> answer here </answer>. User: {prompt}. Assistant:\"\"\"\n",
    "# prompt = \"\"\"A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>.\n",
    "\n",
    "# User: LogD measures a molecule's lipophilicity, influencing its absorption, distribution, metabolism, and excretion (ADME). Given the candidate molecule:\n",
    "\n",
    "# CO[C@H]1C[C@H](N2N=CC3=C(C(=O)NC4=CC=C5CNCC5=C4)C=C(Cl)C=C32)C1\n",
    "\n",
    "# analyze its LogD value based on known chemical properties and ADMET guidelines.\n",
    "\n",
    "# Output the LogD classification in one of the following categories:\n",
    "# - High-Risk (LogD > 3)\n",
    "# - Weakly Optimized (LogD < 1)\n",
    "# - Acceptable (1 ‚â§ LogD ‚â§ 3)\n",
    "\n",
    "# Assistant: <think> Step-by-step reasoning process for determining LogD. </think> <answer> Final LogD classification. </answer>\"\"\"\n",
    "# prompt = \"\"\"A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>.\n",
    "\n",
    "# User: Background:\n",
    "# LogD is a measure of a molecule's lipophilicity, or its ability to dissolve in fats. It is a crucial property in drug discovery, as it influences factors like absorption, distribution, metabolism, and excretion (ADME).\n",
    "\n",
    "# Mouse Liver Microsomal Stability (MLM, protocol): This is a stability assay that tests how quickly a molecule is broken down by mouse liver microsomes. It is a useful assay that can estimate how long a molecule will reside in the mouse body before clearance.\n",
    "\n",
    "# Human Liver Microsomal Stability (HLM, protocol): This is a stability assay that tests how quickly a molecule is broken down by human liver microsomes. It is a useful assay that can estimate how long a molecule will reside in the human body before clearance.\n",
    "\n",
    "# Solubility (KSOL, protocol): Solubility is essential for drug molecules as it heavily affects the pharmacokinetics and pharmacodynamics ('PKPD') of the molecule in the human body.\n",
    "\n",
    "# Cell Permeation (MDR1-MDCKII, protocol): MDCKII-MDR1 is a cell line used to model cell permeation, i.e., how well drug compounds permeate cell layers. For coronaviruses, this is a critical endpoint because increasing evidence suggests that afflictions such as long COVID are caused by (remnant) virus particles in the brain. Blood-brain barrier (BBB) permeation is critical for drug candidates targeting the brain.\n",
    "\n",
    "# Additional Context:\n",
    "\n",
    "# Here is the first batch of molecules for which we received wet lab results:\n",
    "# {str(polaris_dataset_train)}\n",
    "\n",
    "# Manual Rules Used by Chemists in Drug Discovery:\n",
    "\n",
    "# When evaluating a batch of molecules with ADMET data, experienced chemists use a combination of knowledge, intuition, and established rules to guide their decision-making. Here are some of the key manual rules they often employ:\n",
    "\n",
    "# Lipinski's Rule of Five (Ro5): This rule helps predict the oral bioavailability of a drug candidate. It states that a molecule is likely to have good oral absorption if it meets the following criteria:\n",
    "# - Molecular weight ‚â§ 500 Da\n",
    "# - Lipophilicity (logP) ‚â§ 5\n",
    "# - Number of hydrogen bond acceptors ‚â§ 10\n",
    "# - Number of hydrogen bond donors ‚â§ 5\n",
    "\n",
    "# Pfizer Rule: This rule focuses on potential toxicity. It suggests that compounds with high lipophilicity (logP > 3) and low polar surface area (TPSA < 75) are more likely to be toxic.\n",
    "\n",
    "# GSK Rule: This rule proposes that compounds with molecular weight ‚â§ 400 Da and logP ‚â§ 4 are more likely to have a favorable ADMET profile.\n",
    "\n",
    "# Golden Triangle Rule: This rule suggests that compounds with 200 Da ‚â§ molecular weight ‚â§ 350 Da and -2 ‚â§ logD ‚â§ 5 are more likely to have a favorable ADMET profile.\n",
    "\n",
    "# Veber's Rule: This rule expands on Lipinski's Ro5 by considering molecular flexibility. It suggests that good oral bioavailability is likely if:   \n",
    "# - Number of rotatable bonds ‚â§ 10\n",
    "# - Polar surface area ‚â§ 140 √Ö¬≤ or total number of hydrogen bond donors and acceptors ‚â§ 12\n",
    "\n",
    "# Rule of Three (Ro3): This rule is often applied in fragment-based drug discovery. It suggests that fragment hits should ideally have:   \n",
    "# - Molecular weight ‚â§ 300 Da\n",
    "# - cLogP ‚â§ 3\n",
    "# - Number of hydrogen bond donors and acceptors ‚â§ 3\n",
    "\n",
    "# Other Considerations:\n",
    "# - **Lead-likeness**: Similar to drug-likeness, but with more relaxed criteria, as lead compounds are often optimized further.\n",
    "# - **Ligand Efficiency (LE)**: A measure of binding affinity relative to the size of the molecule. Higher LE is generally desirable.\n",
    "# - **Lipophilic Efficiency (LiPE)**: A measure of potency relative to lipophilicity. Higher LiPE suggests a better balance between these properties.\n",
    "# - **Synthetic Accessibility**: The ease with which a molecule can be synthesized. Easier synthesis is generally preferred.\n",
    "\n",
    "# Important Notes:\n",
    "# - These rules are guidelines, not absolute requirements. There are exceptions to every rule, and the specific context of the drug discovery program should always be considered.\n",
    "# - Experienced chemists often develop their own intuition and heuristics based on their experience and knowledge of specific drug targets and therapeutic areas.\n",
    "# - Computational tools and predictive models are increasingly used to complement these manual rules and provide more quantitative predictions of ADMET properties.\n",
    "# - By combining these manual rules with their expertise and the available data, chemists can make informed decisions about which molecules to prioritize for further development and optimization.\n",
    "\n",
    "# As a chief chemist DM, comprehensively study the results and examine the candidate molecule:\n",
    "# CO[C@H]1C[C@H](N2N=CC3=C(C(=O)NC4=CC=C5CNCC5=C4)C=C(Cl)C=C32)C1. \n",
    "\n",
    "# Based on its bonds, fragments, atoms, first batch results, and your chemistry knowledge, what do you think is its LogD value?\n",
    "\n",
    "# Output results in one of the three ranges:\n",
    "\n",
    "# **LogD (Lipophilicity):**\n",
    "# - **High-Risk (High Lipophilicity):** LogD > 3. These molecules are highly lipophilic and may have increased metabolism, poor solubility, potential toxicity, and higher risk of off-target binding. Modification to reduce lipophilicity is often needed.\n",
    "# - **Weakly Optimized (Low Lipophilicity):** LogD < 1. These molecules are very polar and may have poor permeability. Enhancement of lipophilicity might be necessary to improve absorption and distribution.\n",
    "# - **Acceptable (Optimal Lipophilicity):** 1 ‚â§ LogD ‚â§ 3. These molecules have a good balance of lipophilicity and polarity, which is generally favorable for drug-like properties.\n",
    "\n",
    "# **HLM (Human Liver Microsomal Stability):**\n",
    "# - High-Risk (Rapid Metabolism): T¬Ω < 15 minutes\n",
    "# - Weakly Optimized (Moderate Metabolism): 15 minutes ‚â§ T¬Ω < 30 minutes\n",
    "# - Acceptable (Good Stability): T¬Ω ‚â• 30 minutes\n",
    "\n",
    "# **MLM (Mouse Liver Microsomal Stability):**\n",
    "# - High-Risk (Rapid Metabolism): T¬Ω < 15 minutes\n",
    "# - Weakly Optimized (Moderate Metabolism): 15 minutes ‚â§ T¬Ω < 30 minutes\n",
    "# - Acceptable (Good Stability): T¬Ω ‚â• 30 minutes\n",
    "\n",
    "# **KSOL (Solubility):**\n",
    "# - High-Risk (Poor Solubility): logS < -5\n",
    "# - Weakly Optimized (Moderate Solubility): -5 ‚â§ logS < -4\n",
    "# - Acceptable (Good Solubility): logS ‚â• -4\n",
    "\n",
    "# **MDR1-MDCKII (Cell Permeability):**\n",
    "# - High-Risk (Poor Permeability): Papp < 1 x 10‚Åª‚Å∂ cm/s\n",
    "# - Weakly Optimized (Moderate Permeability): 1 x 10‚Åª‚Å∂ cm/s ‚â§ Papp < 2 x 10‚Åª‚Å∂ cm/s\n",
    "# - Acceptable (Good Permeability): Papp ‚â• 2 x 10‚Åª‚Å∂ cm/s\n",
    "\n",
    "# Assistant: <think> Step-by-step reasoning process for evaluating LogD, HLM, MLM, KSOL, and MDR1-MDCKII. </think> <answer> \n",
    "# {\n",
    "#     \"LogD\": \"category_here\",\n",
    "#     \"HLM\": \"category_here\",\n",
    "#     \"MLM\": \"category_here\",\n",
    "#     \"KSOL\": \"category_here\",\n",
    "#     \"MDR1-MDCKII\": \"category_here\"\n",
    "# } \n",
    "# </answer>\"\"\"\n",
    "\n",
    "prompt = \"\"\"A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. \n",
    "\n",
    "User: Background:\n",
    "LogD is a measure of a molecule's lipophilicity, or its ability to dissolve in fats. It is a crucial property in drug discovery, as it influences factors like absorption, distribution, metabolism, and excretion (ADME).\n",
    "\n",
    "Mouse Liver Microsomal Stability (MLM, protocol): This is a stability assay that tests how quickly a molecule is broken down by mouse liver microsomes. It is a useful assay that can estimate how long a molecule will reside in the mouse body before clearance.\n",
    "\n",
    "Human Liver Microsomal Stability (HLM, protocol): This is a stability assay that tests how quickly a molecule is broken down by human liver microsomes. It is a useful assay that can estimate how long a molecule will reside in the human body before clearance.\n",
    "\n",
    "Solubility (KSOL, protocol): Solubility is essential for drug molecules as it heavily affects the pharmacokinetics and pharmacodynamics ('PKPD') of the molecule in the human body.\n",
    "\n",
    "Cell Permeation (MDR1-MDCKII, protocol): MDCKII-MDR1 is a cell line used to model cell permeation, i.e., how well drug compounds permeate cell layers. For coronaviruses, this is a critical endpoint because increasing evidence suggests that afflictions such as long COVID are caused by (remnant) virus particles in the brain. Blood-brain barrier (BBB) permeation is critical for drug candidates targeting the brain.\n",
    "\n",
    "Additional Context:\n",
    "\n",
    "Here is the first batch of molecules for which we received wet lab results:\n",
    "{str(polaris_dataset_train)}\n",
    "\n",
    "Manual Rules Used by Chemists in Drug Discovery:\n",
    "\n",
    "When evaluating a batch of molecules with ADMET data, experienced chemists use a combination of knowledge, intuition, and established rules to guide their decision-making. Here are some of the key manual rules they often employ:\n",
    "\n",
    "Lipinski's Rule of Five (Ro5): This rule helps predict the oral bioavailability of a drug candidate. It states that a molecule is likely to have good oral absorption if it meets the following criteria:\n",
    "- Molecular weight ‚â§ 500 Da\n",
    "- Lipophilicity (logP) ‚â§ 5\n",
    "- Number of hydrogen bond acceptors ‚â§ 10\n",
    "- Number of hydrogen bond donors ‚â§ 5\n",
    "\n",
    "Pfizer Rule: This rule focuses on potential toxicity. It suggests that compounds with high lipophilicity (logP > 3) and low polar surface area (TPSA < 75) are more likely to be toxic.\n",
    "\n",
    "GSK Rule: This rule proposes that compounds with molecular weight ‚â§ 400 Da and logP ‚â§ 4 are more likely to have a favorable ADMET profile.\n",
    "\n",
    "Golden Triangle Rule: This rule suggests that compounds with 200 Da ‚â§ molecular weight ‚â§ 350 Da and -2 ‚â§ logD ‚â§ 5 are more likely to have a favorable ADMET profile.\n",
    "\n",
    "Veber's Rule: This rule expands on Lipinski's Ro5 by considering molecular flexibility. It suggests that good oral bioavailability is likely if:   \n",
    "- Number of rotatable bonds ‚â§ 10\n",
    "- Polar surface area ‚â§ 140 √Ö¬≤ or total number of hydrogen bond donors and acceptors ‚â§ 12\n",
    "\n",
    "Rule of Three (Ro3): This rule is often applied in fragment-based drug discovery. It suggests that fragment hits should ideally have:   \n",
    "- Molecular weight ‚â§ 300 Da\n",
    "- cLogP ‚â§ 3\n",
    "- Number of hydrogen bond donors and acceptors ‚â§ 3\n",
    "\n",
    "Other Considerations:\n",
    "- **Lead-likeness**: Similar to drug-likeness, but with more relaxed criteria, as lead compounds are often optimized further.\n",
    "- **Ligand Efficiency (LE)**: A measure of binding affinity relative to the size of the molecule. Higher LE is generally desirable.\n",
    "- **Lipophilic Efficiency (LiPE)**: A measure of potency relative to lipophilicity. Higher LiPE suggests a better balance between these properties.\n",
    "- **Synthetic Accessibility**: The ease with which a molecule can be synthesized. Easier synthesis is generally preferred.\n",
    "\n",
    "Important Notes:\n",
    "- These rules are guidelines, not absolute requirements. There are exceptions to every rule, and the specific context of the drug discovery program should always be considered.\n",
    "- Experienced chemists often develop their own intuition and heuristics based on their experience and knowledge of specific drug targets and therapeutic areas.\n",
    "- Computational tools and predictive models are increasingly used to complement these manual rules and provide more quantitative predictions of ADMET properties.\n",
    "- By combining these manual rules with their expertise and the available data, chemists can make informed decisions about which molecules to prioritize for further development and optimization.\n",
    "\n",
    "As a chief chemist DM, comprehensively study the results and examine the candidate molecule SMILES user provides. \n",
    "\n",
    "Based on its bonds, fragments, atoms, first batch results, and your chemistry knowledge, what do you think is its LogD value?\n",
    "\n",
    "Output results in one of the three ranges:\n",
    "\n",
    "LogD (Lipophilicity):\n",
    "- High-Risk (High Lipophilicity): LogD > 3. These molecules are highly lipophilic and may have increased metabolism, poor solubility, potential toxicity, and higher risk of off-target binding. Modification to reduce lipophilicity is often needed.\n",
    "- Weakly Optimized (Low Lipophilicity): LogD < 1. These molecules are very polar and may have poor permeability. Enhancement of lipophilicity might be necessary to improve absorption and distribution.\n",
    "- Acceptable (Optimal Lipophilicity): 1 ‚â§ LogD ‚â§ 3. These molecules have a good balance of lipophilicity and polarity, which is generally favorable for drug-like properties.\n",
    "\n",
    "HLM (Human Liver Microsomal Stability):\n",
    "- High-Risk (Rapid Metabolism): T¬Ω < 15 minutes\n",
    "- Weakly Optimized (Moderate Metabolism): 15 minutes ‚â§ T¬Ω < 30 minutes\n",
    "- Acceptable (Good Stability): T¬Ω ‚â• 30 minutes\n",
    "\n",
    "MLM (Mouse Liver Microsomal Stability):\n",
    "- High-Risk (Rapid Metabolism): T¬Ω < 15 minutes\n",
    "- Weakly Optimized (Moderate Metabolism): 15 minutes ‚â§ T¬Ω < 30 minutes\n",
    "- Acceptable (Good Stability): T¬Ω ‚â• 30 minutes\n",
    "\n",
    "KSOL (Solubility):\n",
    "- High-Risk (Poor Solubility): logS < -5\n",
    "- Weakly Optimized (Moderate Solubility): -5 ‚â§ logS < -4\n",
    "- Acceptable (Good Solubility): logS ‚â• -4\n",
    "\n",
    "MDR1-MDCKII (Cell Permeability):\n",
    "- High-Risk (Poor Permeability): Papp < 1 x 10‚Åª‚Å∂ cm/s\n",
    "- Weakly Optimized (Moderate Permeability): 1 x 10‚Åª‚Å∂ cm/s ‚â§ Papp < 2 x 10‚Åª‚Å∂ cm/s\n",
    "- Acceptable (Good Permeability): Papp ‚â• 2 x 10‚Åª‚Å∂ cm/s\n",
    "\n",
    "The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e.,<think> Step-by-step reasoning process for evaluating LogD, HLM, MLM, KSOL, and MDR1-MDCKII. </think> <answer> \n",
    "{\n",
    "    \"LogD\": \"value\",\n",
    "    \"HLM\": \"value\",\n",
    "    \"MLM\": \"value\",\n",
    "    \"KSOL\": \"value\",\n",
    "    \"MDR1-MDCKII\": \"value\"\n",
    "} \n",
    "</answer>.\n",
    "User: What are the LogD,HLM,MLM, KSOL and MDR1-MDCKII for CO[C@H]1C[C@H](N2N=CC3=C(C(=O)NC4=CC=C5CNCC5=C4)C=C(Cl)C=C32)C1?. Assistant:\n",
    "\"\"\"\n",
    "\n",
    "# HLM (Human Liver Microsomal Stability):\n",
    "\n",
    "# High-Risk (Rapid Metabolism): T¬Ω < 15 minutes. These molecules are rapidly metabolized and likely to have poor oral bioavailability. Significant optimization is usually needed.\n",
    "# Weakly Optimized (Moderate Metabolism): 15 minutes ‚â§ T¬Ω < 30 minutes. These molecules have moderate stability and may require some optimization to improve their pharmacokinetic properties.\n",
    "# Acceptable (Good Stability): T¬Ω ‚â• 30 minutes. These molecules exhibit good stability and are less likely to be limited by hepatic metabolism.\n",
    "# MLM (Mouse Liver Microsomal Stability):\n",
    "\n",
    "# High-Risk (Rapid Metabolism): T¬Ω < 15 minutes\n",
    "# Weakly Optimized (Moderate Metabolism): 15 minutes ‚â§ T¬Ω < 30 minutes\n",
    "# Acceptable (Good Stability): T¬Ω ‚â• 30 minutes\n",
    "# KSOL (Solubility):\n",
    "\n",
    "# High-Risk (Poor Solubility): logS < -5. These molecules have very low solubility and are likely to face significant challenges with formulation and absorption. Major optimization is often needed.\n",
    "# Weakly Optimized (Moderate Solubility): -5 ‚â§ logS < -4. These molecules have moderate solubility and may require some formulation efforts or structural modifications to improve their solubility.\n",
    "# Acceptable (Good Solubility): logS ‚â• -4. These molecules exhibit good solubility and are less likely to be limited by solubility issues.\n",
    "# LogD (Lipophilicity):\n",
    "\n",
    "# High-Risk (High Lipophilicity): LogD > 3. These molecules are highly lipophilic and may have increased metabolism, poor solubility, potential for toxicity, and higher risk of off-target binding. Modification to reduce lipophilicity is often needed.\n",
    "# Weakly Optimized (Low Lipophilicity): LogD < 1. These molecules are very polar and may have poor permeability. Enhancement of lipophilicity might be necessary to improve absorption and distribution.\n",
    "# Acceptable (Optimal Lipophilicity): 1 ‚â§ LogD ‚â§ 3. These molecules have a good balance of lipophilicity and polarity, which is generally favorable for drug-like properties.\n",
    "# MDR1-MDCKII (Cell Permeability):\n",
    "\n",
    "# High-Risk (Poor Permeability): Papp < 1 x 10‚Åª‚Å∂ cm/s. These molecules have very low permeability and are likely to face challenges with absorption and reaching their target. Significant optimization is usually needed.\n",
    "# Weakly Optimized (Moderate Permeability): 1 x 10‚Åª‚Å∂ cm/s ‚â§ Papp < 2 x 10‚Åª‚Å∂ cm/s. These molecules have moderate permeability and may benefit from some optimization to improve their absorption and distribution.\n",
    "# Acceptable (Good Permeability): Papp ‚â• 2 x 10‚Åª‚Å∂ cm/s. These molecules exhibit good permeability and are less likely to be limited by permeability issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Input: A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. \n",
      "\n",
      "User: Background:\n",
      "LogD is a measure of a molecule's lipophilicity, or its ability to dissolve in fats. It is a crucial property in drug discovery, as it influences factors like absorption, distribution, metabolism, and excretion (ADME).\n",
      "\n",
      "Mouse Liver Microsomal Stability (MLM, protocol): This is a stability assay that tests how quickly a molecule is broken down by mouse liver microsomes. It is a useful assay that can estimate how long a molecule will reside in the mouse body before clearance.\n",
      "\n",
      "Human Liver Microsomal Stability (HLM, protocol): This is a stability assay that tests how quickly a molecule is broken down by human liver microsomes. It is a useful assay that can estimate how long a molecule will reside in the human body before clearance.\n",
      "\n",
      "Solubility (KSOL, protocol): Solubility is essential for drug molecules as it heavily affects the pharmacokinetics and pharmacodynamics ('PKPD') of the molecule in the human body.\n",
      "\n",
      "Cell Permeation (MDR1-MDCKII, protocol): MDCKII-MDR1 is a cell line used to model cell permeation, i.e., how well drug compounds permeate cell layers. For coronaviruses, this is a critical endpoint because increasing evidence suggests that afflictions such as long COVID are caused by (remnant) virus particles in the brain. Blood-brain barrier (BBB) permeation is critical for drug candidates targeting the brain.\n",
      "\n",
      "Additional Context:\n",
      "\n",
      "Here is the first batch of molecules for which we received wet lab results:\n",
      "{str(polaris_dataset_train)}\n",
      "\n",
      "Manual Rules Used by Chemists in Drug Discovery:\n",
      "\n",
      "When evaluating a batch of molecules with ADMET data, experienced chemists use a combination of knowledge, intuition, and established rules to guide their decision-making. Here are some of the key manual rules they often employ:\n",
      "\n",
      "Lipinski's Rule of Five (Ro5): This rule helps predict the oral bioavailability of a drug candidate. It states that a molecule is likely to have good oral absorption if it meets the following criteria:\n",
      "- Molecular weight ‚â§ 500 Da\n",
      "- Lipophilicity (logP) ‚â§ 5\n",
      "- Number of hydrogen bond acceptors ‚â§ 10\n",
      "- Number of hydrogen bond donors ‚â§ 5\n",
      "\n",
      "Pfizer Rule: This rule focuses on potential toxicity. It suggests that compounds with high lipophilicity (logP > 3) and low polar surface area (TPSA < 75) are more likely to be toxic.\n",
      "\n",
      "GSK Rule: This rule proposes that compounds with molecular weight ‚â§ 400 Da and logP ‚â§ 4 are more likely to have a favorable ADMET profile.\n",
      "\n",
      "Golden Triangle Rule: This rule suggests that compounds with 200 Da ‚â§ molecular weight ‚â§ 350 Da and -2 ‚â§ logD ‚â§ 5 are more likely to have a favorable ADMET profile.\n",
      "\n",
      "Veber's Rule: This rule expands on Lipinski's Ro5 by considering molecular flexibility. It suggests that good oral bioavailability is likely if:   \n",
      "- Number of rotatable bonds ‚â§ 10\n",
      "- Polar surface area ‚â§ 140 √Ö¬≤ or total number of hydrogen bond donors and acceptors ‚â§ 12\n",
      "\n",
      "Rule of Three (Ro3): This rule is often applied in fragment-based drug discovery. It suggests that fragment hits should ideally have:   \n",
      "- Molecular weight ‚â§ 300 Da\n",
      "- cLogP ‚â§ 3\n",
      "- Number of hydrogen bond donors and acceptors ‚â§ 3\n",
      "\n",
      "Other Considerations:\n",
      "- **Lead-likeness**: Similar to drug-likeness, but with more relaxed criteria, as lead compounds are often optimized further.\n",
      "- **Ligand Efficiency (LE)**: A measure of binding affinity relative to the size of the molecule. Higher LE is generally desirable.\n",
      "- **Lipophilic Efficiency (LiPE)**: A measure of potency relative to lipophilicity. Higher LiPE suggests a better balance between these properties.\n",
      "- **Synthetic Accessibility**: The ease with which a molecule can be synthesized. Easier synthesis is generally preferred.\n",
      "\n",
      "Important Notes:\n",
      "- These rules are guidelines, not absolute requirements. There are exceptions to every rule, and the specific context of the drug discovery program should always be considered.\n",
      "- Experienced chemists often develop their own intuition and heuristics based on their experience and knowledge of specific drug targets and therapeutic areas.\n",
      "- Computational tools and predictive models are increasingly used to complement these manual rules and provide more quantitative predictions of ADMET properties.\n",
      "- By combining these manual rules with their expertise and the available data, chemists can make informed decisions about which molecules to prioritize for further development and optimization.\n",
      "\n",
      "As a chief chemist DM, comprehensively study the results and examine the candidate molecule SMILES user provides. \n",
      "\n",
      "Based on its bonds, fragments, atoms, first batch results, and your chemistry knowledge, what do you think is its LogD value?\n",
      "\n",
      "Output results in one of the three ranges:\n",
      "\n",
      "LogD (Lipophilicity):\n",
      "- High-Risk (High Lipophilicity): LogD > 3. These molecules are highly lipophilic and may have increased metabolism, poor solubility, potential toxicity, and higher risk of off-target binding. Modification to reduce lipophilicity is often needed.\n",
      "- Weakly Optimized (Low Lipophilicity): LogD < 1. These molecules are very polar and may have poor permeability. Enhancement of lipophilicity might be necessary to improve absorption and distribution.\n",
      "- Acceptable (Optimal Lipophilicity): 1 ‚â§ LogD ‚â§ 3. These molecules have a good balance of lipophilicity and polarity, which is generally favorable for drug-like properties.\n",
      "\n",
      "HLM (Human Liver Microsomal Stability):\n",
      "- High-Risk (Rapid Metabolism): T¬Ω < 15 minutes\n",
      "- Weakly Optimized (Moderate Metabolism): 15 minutes ‚â§ T¬Ω < 30 minutes\n",
      "- Acceptable (Good Stability): T¬Ω ‚â• 30 minutes\n",
      "\n",
      "MLM (Mouse Liver Microsomal Stability):\n",
      "- High-Risk (Rapid Metabolism): T¬Ω < 15 minutes\n",
      "- Weakly Optimized (Moderate Metabolism): 15 minutes ‚â§ T¬Ω < 30 minutes\n",
      "- Acceptable (Good Stability): T¬Ω ‚â• 30 minutes\n",
      "\n",
      "KSOL (Solubility):\n",
      "- High-Risk (Poor Solubility): logS < -5\n",
      "- Weakly Optimized (Moderate Solubility): -5 ‚â§ logS < -4\n",
      "- Acceptable (Good Solubility): logS ‚â• -4\n",
      "\n",
      "MDR1-MDCKII (Cell Permeability):\n",
      "- High-Risk (Poor Permeability): Papp < 1 x 10‚Åª‚Å∂ cm/s\n",
      "- Weakly Optimized (Moderate Permeability): 1 x 10‚Åª‚Å∂ cm/s ‚â§ Papp < 2 x 10‚Åª‚Å∂ cm/s\n",
      "- Acceptable (Good Permeability): Papp ‚â• 2 x 10‚Åª‚Å∂ cm/s\n",
      "\n",
      "The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e.,<think> Step-by-step reasoning process for evaluating LogD, HLM, MLM, KSOL, and MDR1-MDCKII. </think> <answer> \n",
      "{\n",
      "    \"LogD\": \"value\",\n",
      "    \"HLM\": \"value\",\n",
      "    \"MLM\": \"value\",\n",
      "    \"KSOL\": \"value\",\n",
      "    \"MDR1-MDCKII\": \"value\"\n",
      "} \n",
      "</answer>.\n",
      "User: What are the LogD,HLM,MLM, KSOL and MDR1-MDCKII for CO[C@H]1C[C@H](N2N=CC3=C(C(=O)NC4=CC=C5CNCC5=C4)C=C(Cl)C=C32)C1?. Assistant:\n",
      "\n",
      "Model Response: You are a decision maker chemist that predicts logD for a given small molecule based on its SMILES.<ÔΩúUserÔΩú>A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. \n",
      "\n",
      "User: Background:\n",
      "LogD is a measure of a molecule's lipophilicity, or its ability to dissolve in fats. It is a crucial property in drug discovery, as it influences factors like absorption, distribution, metabolism, and excretion (ADME).\n",
      "\n",
      "Mouse Liver Microsomal Stability (MLM, protocol): This is a stability assay that tests how quickly a molecule is broken down by mouse liver microsomes. It is a useful assay that can estimate how long a molecule will reside in the mouse body before clearance.\n",
      "\n",
      "Human Liver Microsomal Stability (HLM, protocol): This is a stability assay that tests how quickly a molecule is broken down by human liver microsomes. It is a useful assay that can estimate how long a molecule will reside in the human body before clearance.\n",
      "\n",
      "Solubility (KSOL, protocol): Solubility is essential for drug molecules as it heavily affects the pharmacokinetics and pharmacodynamics ('PKPD') of the molecule in the human body.\n",
      "\n",
      "Cell Permeation (MDR1-MDCKII, protocol): MDCKII-MDR1 is a cell line used to model cell permeation, i.e., how well drug compounds permeate cell layers. For coronaviruses, this is a critical endpoint because increasing evidence suggests that afflictions such as long COVID are caused by (remnant) virus particles in the brain. Blood-brain barrier (BBB) permeation is critical for drug candidates targeting the brain.\n",
      "\n",
      "Additional Context:\n",
      "\n",
      "Here is the first batch of molecules for which we received wet lab results:\n",
      "{str(polaris_dataset_train)}\n",
      "\n",
      "Manual Rules Used by Chemists in Drug Discovery:\n",
      "\n",
      "When evaluating a batch of molecules with ADMET data, experienced chemists use a combination of knowledge, intuition, and established rules to guide their decision-making. Here are some of the key manual rules they often employ:\n",
      "\n",
      "Lipinski's Rule of Five (Ro5): This rule helps predict the oral bioavailability of a drug candidate. It states that a molecule is likely to have good oral absorption if it meets the following criteria:\n",
      "- Molecular weight ‚â§ 500 Da\n",
      "- Lipophilicity (logP) ‚â§ 5\n",
      "- Number of hydrogen bond acceptors ‚â§ 10\n",
      "- Number of hydrogen bond donors ‚â§ 5\n",
      "\n",
      "Pfizer Rule: This rule focuses on potential toxicity. It suggests that compounds with high lipophilicity (logP > 3) and low polar surface area (TPSA < 75) are more likely to be toxic.\n",
      "\n",
      "GSK Rule: This rule proposes that compounds with molecular weight ‚â§ 400 Da and logP ‚â§ 4 are more likely to have a favorable ADMET profile.\n",
      "\n",
      "Golden Triangle Rule: This rule suggests that compounds with 200 Da ‚â§ molecular weight ‚â§ 350 Da and -2 ‚â§ logD ‚â§ 5 are more likely to have a favorable ADMET profile.\n",
      "\n",
      "Veber's Rule: This rule expands on Lipinski's Ro5 by considering molecular flexibility. It suggests that good oral bioavailability is likely if:   \n",
      "- Number of rotatable bonds ‚â§ 10\n",
      "- Polar surface area ‚â§ 140 √Ö¬≤ or total number of hydrogen bond donors and acceptors ‚â§ 12\n",
      "\n",
      "Rule of Three (Ro3): This rule is often applied in fragment-based drug discovery. It suggests that fragment hits should ideally have:   \n",
      "- Molecular weight ‚â§ 300 Da\n",
      "- cLogP ‚â§ 3\n",
      "- Number of hydrogen bond donors and acceptors ‚â§ 3\n",
      "\n",
      "Other Considerations:\n",
      "- **Lead-likeness**: Similar to drug-likeness, but with more relaxed criteria, as lead compounds are often optimized further.\n",
      "- **Ligand Efficiency (LE)**: A measure of binding affinity relative to the size of the molecule. Higher LE is generally desirable.\n",
      "- **Lipophilic Efficiency (LiPE)**: A measure of potency relative to lipophilicity. Higher LiPE suggests a better balance between these properties.\n",
      "- **Synthetic Accessibility**: The ease with which a molecule can be synthesized. Easier synthesis is generally preferred.\n",
      "\n",
      "Important Notes:\n",
      "- These rules are guidelines, not absolute requirements. There are exceptions to every rule, and the specific context of the drug discovery program should always be considered.\n",
      "- Experienced chemists often develop their own intuition and heuristics based on their experience and knowledge of specific drug targets and therapeutic areas.\n",
      "- Computational tools and predictive models are increasingly used to complement these manual rules and provide more quantitative predictions of ADMET properties.\n",
      "- By combining these manual rules with their expertise and the available data, chemists can make informed decisions about which molecules to prioritize for further development and optimization.\n",
      "\n",
      "As a chief chemist DM, comprehensively study the results and examine the candidate molecule SMILES user provides. \n",
      "\n",
      "Based on its bonds, fragments, atoms, first batch results, and your chemistry knowledge, what do you think is its LogD value?\n",
      "\n",
      "Output results in one of the three ranges:\n",
      "\n",
      "LogD (Lipophilicity):\n",
      "- High-Risk (High Lipophilicity): LogD > 3. These molecules are highly lipophilic and may have increased metabolism, poor solubility, potential toxicity, and higher risk of off-target binding. Modification to reduce lipophilicity is often needed.\n",
      "- Weakly Optimized (Low Lipophilicity): LogD < 1. These molecules are very polar and may have poor permeability. Enhancement of lipophilicity might be necessary to improve absorption and distribution.\n",
      "- Acceptable (Optimal Lipophilicity): 1 ‚â§ LogD ‚â§ 3. These molecules have a good balance of lipophilicity and polarity, which is generally favorable for drug-like properties.\n",
      "\n",
      "HLM (Human Liver Microsomal Stability):\n",
      "- High-Risk (Rapid Metabolism): T¬Ω < 15 minutes\n",
      "- Weakly Optimized (Moderate Metabolism): 15 minutes ‚â§ T¬Ω < 30 minutes\n",
      "- Acceptable (Good Stability): T¬Ω ‚â• 30 minutes\n",
      "\n",
      "MLM (Mouse Liver Microsomal Stability):\n",
      "- High-Risk (Rapid Metabolism): T¬Ω < 15 minutes\n",
      "- Weakly Optimized (Moderate Metabolism): 15 minutes ‚â§ T¬Ω < 30 minutes\n",
      "- Acceptable (Good Stability): T¬Ω ‚â• 30 minutes\n",
      "\n",
      "KSOL (Solubility):\n",
      "- High-Risk (Poor Solubility): logS < -5\n",
      "- Weakly Optimized (Moderate Solubility): -5 ‚â§ logS < -4\n",
      "- Acceptable (Good Solubility): logS ‚â• -4\n",
      "\n",
      "MDR1-MDCKII (Cell Permeability):\n",
      "- High-Risk (Poor Permeability): Papp < 1 x 10‚Åª‚Å∂ cm/s\n",
      "- Weakly Optimized (Moderate Permeability): 1 x 10‚Åª‚Å∂ cm/s ‚â§ Papp < 2 x 10‚Åª‚Å∂ cm/s\n",
      "- Acceptable (Good Permeability): Papp ‚â• 2 x 10‚Åª‚Å∂ cm/s\n",
      "\n",
      "The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e.,<think> Step-by-step reasoning process for evaluating LogD, HLM, MLM, KSOL, and MDR1-MDCKII. </think> <answer> \n",
      "{\n",
      "    \"LogD\": \"value\",\n",
      "    \"HLM\": \"value\",\n",
      "    \"MLM\": \"value\",\n",
      "    \"KSOL\": \"value\",\n",
      "    \"MDR1-MDCKII\": \"value\"\n",
      "} \n",
      "</answer>.\n",
      "User: What are the LogD,HLM,MLM, KSOL and MDR1-MDCKII for CO[C@H]1C[C@H](N2N=CC3=C(C(=O)NC4=CC=C5CNCC5=C4)C=C(Cl)C=C32)C1?. Assistant:\n",
      "<ÔΩúAssistantÔΩú><think>\n",
      "Okay, so I'm trying to figure out the LogD, HLM, MLM, KSOL, and MDR1-MDCKII values for the given SMILES notation. Let's break it down step by step.\n",
      "\n",
      "First, I need to understand what each of these descriptors means. LogD is a measure of lipophilicity, which is how well a molecule interacts with organic lipids. High LogD values mean the molecule is more likely to be lipophilic. So, if I remember correctly, a LogD greater than 3 indicates high lipophilicity, which could be a problem for drug-like molecules because they might be metabolized more quickly, leading to toxicity.\n",
      "\n",
      "HLM stands for Human Liver Microsomal Stability. It measures how quickly a molecule is broken down by human liver microsomes. If HLM is high (T¬Ω < 15 minutes), the molecule metabolizes rapidly, which could be concerning as it might be too unstable. If it's moderate (15 ‚â§ T¬Ω < 30 minutes), the stability is acceptable, and if it's low (T¬Ω ‚â• 30 minutes), the molecule is slow to metabolize.\n",
      "\n",
      "MLM is Mouse Liver Microsomal Stability, similar to HLM but based on mouse microsomes. So, the same interpretation applies here.\n",
      "\n",
      "KSOL is about solubility, specifically how well the molecule dissolves in organic solvents. Low KSOL values indicate poor solubility, which can be a problem because solubility affects how the molecule behaves in biological systems, like how it might bind to cells.\n",
      "\n",
      "MDR1-MDCKII is about cell permeability, measuring how easily the molecule can pass through cell membranes. Low MDR1-MDCKII indicates poor permeability, which could be a security risk as the molecule might get into the bloodstream or other tissues where it's not desired.\n",
      "\n",
      "Now, looking at the SMILES notation provided: CO[C@H]1C[C@H](N2N=CC3=C(C(=O)NC4=CC=C5CNCC5=C4)C=C(Cl)C=C32)C1. This looks like a complex molecule with a central ring structure, possibly a cyclic ether or similar. The presence of Cl and other substituents suggests that the molecule might be polar and have a good lipophilicity.\n",
      "\n",
      "Considering LogD > 3, the molecule is likely to be lipophilic, which could be a concern if it's a drug candidate. However, the other descriptors (HLM, MLM, KSOL, MDR1-MDCKII) suggest moderate or fast metabolization and acceptable solubility and permeability. This combination might make it a candidate for further development or optimization.\n",
      "\n",
      "So, based on the analysis, the LogD is likely in the high range, indicating high lipophilicity. The other descriptors are moderate to acceptable, suggesting the molecule is stable and has good solubility and permeability, which are important for its potential use in drug delivery or cell signaling pathways.\n",
      "</think>\n",
      "\n",
      "{\n",
      "    \"LogD\": \"High-Risk\",\n",
      "    \"HLM\": \"Weakly Optimized\",\n",
      "    \"MLM\": \"Weakly Optimized\",\n",
      "    \"KSOL\": \"Weakly Optimized\",\n",
      "    \"MDR1-MDCKII\": \"Weakly Optimized\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response = test_model_inference(prompt)\n",
    "print(f\"Test Input: {prompt}\")\n",
    "print(f\"Model Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'k>\\n\\n```json\\n{\\n    \"LogD\": \"low lipophilicity\",\\n    \"HLM\": \"moderate metabolism\",\\n    \"MLM\": \"moderate metabolism\",\\n    \"KSOL\": \"poor solubility\",\\n    \"MDR1-MDCKII\": \"good permeability\"\\n}\\n</answer>\\n```'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[-200:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"polaris-antiviral-admet-2025.json\", \"r\") as f:\n",
    "    polaris_dataset = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'HLM': nan, 'MDR1-MDCKII': 1.6, 'MLM': nan, 'LogD': 1.5, 'KSOL': 340.0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polaris_dataset[\"CO[C@H]1C[C@H](N2N=CC3=C(C(=O)NC4=CC=C5CNCC5=C4)C=C(Cl)C=C32)C1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qwen/Qwen2.5-0.5B-Instruct response to \n",
    "# prompt = \"What are the LogD, KSOL, MLM, MDR1-MDCKII, HLM for this small molecule CO[C@H]1C[C@H](N2N=CC3=C(C(=O)NC4=CC=C5CNCC5=C4)C=C(Cl)C=C32)C1?\"\n",
    "# test_input = f\"\"\"A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think>\n",
    "# <answer> answer here </answer>. User: {prompt}. Assistant:\"\"\"\n",
    "\n",
    "# 1. **Identify Key Components**: Look at the molecular formula and identify the functional groups that contribute to these parameters.\n",
    "# 2. **Use Structure-Based Methods**: If possible, use structural databases or software like ChemDraw to visualize the molecule's structure and calculate key properties based on the known information.\n",
    "# 3. **Consult Literature**: Refer to literature reviews or databases where similar molecules are described to get insights into their properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the output of this tiny model is quite reliable and suitable for our DeepSeek lookalike model training for sure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Model (R) In RL Setup\n",
    "\n",
    "Now that we have selected our base model, next we need to understand how a basic RL setup works for training an LLM.\n",
    "\n",
    "For DeepSeek R1 their starting point was (DeepSeek V3) base model and in our case we are starting with Qwen2.5‚Äì0.5B-Instruct. By a starting point I meant that **it has created the DeepSeek R1 zero version**, an initial version which has some errors in it before the final version was created.\n",
    "\n",
    "The initial version (R1 Zero) was created using Reinforcement Learning where (DeepSeek v3/Qwen2.5‚Äì0.5B) acts as an RL agent (actor who takes action). Let‚Äôs first visualize how it works.\n",
    "\n",
    "![Qwen 2.5 as an agent workflow (Created by [Fareed Khan](undefined))](https://cdn-images-1.medium.com/max/5410/1*S6YIXu1vIVmQFl-DgRFktg.png)\n",
    "\n",
    "The RL agent (DeepSeek V3/Qwen2‚Äì0.5B) starts by taking an **Action**, which means it generates an answer and some reasoning for a given problem that‚Äôs put into its **Environment**. The Environment, in this case, is simply the reasoning task itself.\n",
    "\n",
    "After taking an action, the Environment gives back a **Reward**. This Reward is like feedback, it tells our base model (DeepSeek V3/Qwen2‚Äì0.5B) how good its action was. A positive Reward means it did something right, maybe got the answer correct or reasoned well. This feedback signal then goes back to our base model, helping it learn and adjust how it takes actions in the future to get even better Rewards.\n",
    "> In the next section, we will be discussing this methodology in more detail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRPO Algorithm for R1 Zero\n",
    "\n",
    "So that we have understand a basic RL flow now we need to learn what exact RL algorithm DeepSeek uses for R1-Zero.\n",
    "\n",
    "There are many RL algos available, but traditional RL use something called a **‚Äúcritic‚Äù **to help the main decision making part (‚Äúactor‚Äù i.e. DeepSeek-V3/Qwen2-0.5B). This critic is usually just as big and complex as the actor itself, which basically doubles the amount of computational cost.\n",
    "\n",
    "But DeepSeek uses GRPO for training their initial (R1 Zero), **GRPO** does things differently because it figures out a baseline, a kind of reference point for good actions directly from the results it gets from a **group** of actions. Because of this, GRPO doesn‚Äôt need a separate critic model at all. This saves a lot of computation and makes things more efficient.\n",
    "\n",
    "Let‚Äôs draw a flowchart of how GRPO is being used for R1 Zero training, and then we will **interpretate** it.\n",
    "\n",
    "![GRPO Flow for DeepSeek R1 Zero (Created by [Fareed Khan](undefined))](https://cdn-images-1.medium.com/max/6404/1*8mfNzi-gvasR7mSaseswmg.png)\n",
    "\n",
    "Let‚Äôs understand how DeepSeek GRPO implementation works with our base model (Qwen2‚Äì0.5B). \n",
    "\n",
    "First, the **Problem Input (A)** is given to the **Qwen Model (B)**, Qwen attempts to generate an answer through **Generate Completion (C)**. The final result, called the **Completion Output (D)**, includes reasoning steps in <think> tags and the final solution in <answer> tags.\n",
    "\n",
    "Next, the **Problem Input (A)** and the **Ground Truth Solution (E)** are fed into the **Reward Functions (F)**, acting as intelligent graders. These functions compare Qwen **Completion Output (D)** with the correct solution and evaluate different aspects such as:\n",
    "\n",
    " 1. **Accuracy** (is the answer correct?)\n",
    "\n",
    " 2. **Format** (are the <think> and <answer> tags used properly?)\n",
    "\n",
    " 3. **Reasoning Steps** (is the logic clear?)\n",
    "\n",
    " 4. **Cosine Scaling** (is the response concise?)\n",
    "\n",
    " 5. **Repetition Penalty** (is there unnecessary repetition?).\n",
    "\n",
    "These evaluations produce **Reward Scores (G)**, which are then passed to the **GRPO Trainer (H)**. The trainer uses gradients to adjust the **Qwen Model (B)**, fine-tuning how it generates answers. This process is called **Gradient Reward Policy Optimization** because it optimizes Qwen responses using **gradients**, **reward feedback**, and **policy adjustments** to maximize performance.\n",
    "\n",
    "Finally, the updated **Qwen Model (B)** is tested again on new problems, continuously refining itself through repeated cycles. With each iteration, Qwen becomes a better problem solver.\n",
    "\n",
    "> In the upcoming section we will start preprocessing our training dataset for GRPO training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Template\n",
    "\n",
    "We are using the same thinking prompt template that DeepSeek uses for the GRPO algorithm to build R1 Zero, so let‚Äôs define that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepSeek system prompt for GRPO based training\n",
    "SYSTEM_PROMPT = (\n",
    "    \"A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant \"\n",
    "    \"first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning \"\n",
    "    \"process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., \"\n",
    "    \"<think> reasoning process here </think><answer> answer here </answer>\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This **system prompt** tells the base model (Qwen2‚Äì0.5B) its role as a helpful assistant who reasons step-by-step before answering.\n",
    "\n",
    "The `<think>` and `<answer>` tags are used to structure the model response, separating its internal reasoning from the final answer for better evaluation and reward.\n",
    "\n",
    "## Preprocessing Training Data\n",
    "\n",
    "Now that we have our system prompt ready, we need to transform our training data according to our template.\n",
    "\n",
    "![Preprocessing dataset overview (Created by [Fareed Khan](undefined))](https://cdn-images-1.medium.com/max/6160/1*XnM7v4dPD4LtyAh2MLuInA.png)\n",
    "\n",
    "We need to create the make_conversation function that will handle the conversation for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to structure the training data\n",
    "def make_conversation(example):\n",
    "    \"\"\"Convert dataset examples into conversation format.\"\"\"\n",
    "    return {\n",
    "        \"prompt\": [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": example[\"problem\"]},\n",
    "        ],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will take each problem column value from our training dataset and return a dictionary with the system prompt and the appended problem question for each row. Let‚Äôs create this function that will prepare our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare dataset\n",
    "def load_math_dataset():\n",
    "    \"\"\"Load and prepare the mathematics dataset.\"\"\"\n",
    "    dataset = load_dataset(\n",
    "        \"AI-MO/NuminaMath-TIR\",\n",
    "        name=\"default\",\n",
    "        split=['train', 'test']\n",
    "    )\n",
    "    \n",
    "    # Convert splits into dictionary\n",
    "    dataset = {\n",
    "        'train': dataset[0],\n",
    "        'test': dataset[1]\n",
    "    }\n",
    "    \n",
    "    # Apply conversation format\n",
    "    for split in dataset:\n",
    "        dataset[split] = dataset[split].map(make_conversation)\n",
    "\n",
    "        # Remove 'messages' column if exists\n",
    "        if \"messages\" in dataset[split].column_names:\n",
    "            dataset[split] = dataset[split].remove_columns(\"messages\")\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have everything ready, let‚Äôs transform our training data into the required format and print the training and test size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our training dataset and printing train/test size\n",
    "dataset = load_math_dataset()\n",
    "\n",
    "print(f\"Train set size: {len(dataset['train'])}\")\n",
    "print(f\"Test set size: {len(dataset['test'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have split our training dataset, we need to validate our dataset (**Check if user/assistant conversation exist**) before moving to the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_dataset(dataset):\n",
    "    \"\"\"Perform basic validation checks on the dataset.\"\"\"\n",
    "    \n",
    "    # Define the required fields for the dataset\n",
    "    required_fields = [\"problem\", \"prompt\"]\n",
    "\n",
    "    # Loop through the 'train' and 'test' splits of the dataset\n",
    "    for split in ['train', 'test']:\n",
    "        print(f\"\\nValidating {split} split:\")\n",
    "\n",
    "        # Retrieve column names from the dataset\n",
    "        fields = dataset[split].column_names\n",
    "\n",
    "        # Check if any required fields are missing\n",
    "        missing = [field for field in required_fields if field not in fields]\n",
    "        if missing:\n",
    "            print(f\"Warning: Missing fields: {missing}\")  # Warn if fields are missing\n",
    "        else:\n",
    "            print(\"‚úì All required fields present\")  # Confirm all fields are present\n",
    "\n",
    "        # Retrieve the first sample from the dataset split\n",
    "        sample = dataset[split][0]\n",
    "\n",
    "        # Extract the 'prompt' field, which contains a list of messages\n",
    "        messages = sample['prompt']\n",
    "\n",
    "        # Validate the prompt format:\n",
    "        # - It should contain at least two messages\n",
    "        # - The first message should be from the 'system' role\n",
    "        # - The second message should be from the 'user' role\n",
    "        if (len(messages) >= 2 and\n",
    "            messages[0]['role'] == 'system' and\n",
    "            messages[1]['role'] == 'user'):\n",
    "            print(\"‚úì Prompt format is correct\")  # Confirm correct format\n",
    "        else:\n",
    "            print(\"Warning: Incorrect prompt format\")  # Warn if format is incorrect\n",
    "\n",
    "# Validate dataset\n",
    "validate_dataset(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our training dataset is validated successfully üôå, it means we have successfully transformed our dataset for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward Functions\n",
    "\n",
    "We already saw in GRPO section that it evaluate the answer of base model through five different ways:\n",
    "\n",
    "![Reward Functions (Created by [Fareed Khan](undefined))](https://cdn-images-1.medium.com/max/7474/1*kJln8i6Tv4aspnTfMoRW-Q.png)\n",
    "\n",
    " 1. **Accuracy** (is the answer correct?)\n",
    "\n",
    " 2. **Format** (are the `<think>` and `<answer>` tags used properly?)\n",
    "\n",
    " 3. **Reasoning Steps** (is the logic clear?)\n",
    "\n",
    " 4. **Cosine Scaling** (is the response concise?)\n",
    "\n",
    " 5. **Repetition Penalty** (is there unnecessary repetition?).\n",
    "\n",
    "Each of these are functions will calculate the reward for each response, and we need to code them. So, let‚Äôs do that first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Reward\n",
    "\n",
    "Accuracy reward is the most easy to understand but requires a bit complex code. In this reward model we want to check if mathematically our base model response is equivalent to the ground truth solution.\n",
    "\n",
    "![Accuracy Reward (Created by [Fareed Khan](undefined))](https://cdn-images-1.medium.com/max/7860/1*A3tW-OZSZ4m10EEzogjy8Q.png)\n",
    "\n",
    "If the model answer is mathematically correct, we assign a reward of **1.0**. If it is incorrect, the reward is **0.0**. In cases where the ground truth solution cannot be parsed, we assign a neutral reward of **0.5** to avoid unfair penalties.\n",
    "\n",
    "Now, let‚Äôs implement the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_reward(completions, **kwargs):\n",
    "    \"\"\"\n",
    "    Reward function to check if the model's response is mathematically \n",
    "    equivalent to the ground truth solution.\n",
    "    Uses latex2sympy2 for parsing and math_verify for validation.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract responses\n",
    "    contents = [completion[0][\"content\"] for completion in completions]\n",
    "    rewards = []\n",
    "\n",
    "    solutions = kwargs.get(\"solution\") # Get solutions from kwargs\n",
    "\n",
    "    if solutions is None:\n",
    "        return [0.5] * len(completions) # Return neutral reward if no solution\n",
    "    \n",
    "    for content, sol in zip(contents, solutions):\n",
    "        # Parse the ground truth solution\n",
    "        gold_parsed = parse(sol, extraction_mode=\"first_match\", \n",
    "                            extraction_config=[LatexExtractionConfig()])\n",
    "        \n",
    "        if gold_parsed:  # Check if parsing was successful\n",
    "            # Parse the model's answer with relaxed normalization\n",
    "            answer_parsed = parse(\n",
    "                content,\n",
    "                extraction_config=[\n",
    "                    LatexExtractionConfig(\n",
    "                        normalization_config=NormalizationConfig(\n",
    "                            nits=False,\n",
    "                            malformed_operators=False,\n",
    "                            basic_latex=True,\n",
    "                            equations=True,\n",
    "                            boxed=\"all\",\n",
    "                            units=True,\n",
    "                        ),\n",
    "                        boxed_match_priority=0,\n",
    "                        try_extract_without_anchor=False,\n",
    "                    )\n",
    "                ],\n",
    "                extraction_mode=\"first_match\",\n",
    "            )\n",
    "\n",
    "            # Reward 1.0 if correct, 0.0 if incorrect\n",
    "            reward = float(verify(answer_parsed, gold_parsed))\n",
    "        else:\n",
    "            # If ground truth cannot be parsed, assign neutral reward (0.5)\n",
    "            reward = 0.5\n",
    "            print(\"Warning: Failed to parse gold solution:\", sol)\n",
    "\n",
    "        rewards.append(reward)\n",
    "    \n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this function, we check whether the model response is **equivalent** to the correct answer. Instead of comparing raw text, we:\n",
    "\n",
    " 1. Convert the solution into a structured mathematical format using **latex2sympy2**.\n",
    "\n",
    " 2. If parsing fails, assign a neutral reward of **0.5**.\n",
    "\n",
    " 3. Extract the model output and normalize it for better robustness.\n",
    "\n",
    " 4. Use **math_verify** to check if the parsed response matches the parsed solution.\n",
    "\n",
    " 5. If correct assign **1,** if incorrect assign **0**.\n",
    "\n",
    "This ensures that accuracy evaluation is not just about textual similarity but **true mathematical correctness.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format Reward\n",
    "\n",
    "Format Reward is all about making sure our model follows instructions and structures its output correctly. We asked it to put its reasoning in `<think>` tags and the final answer in `<answer>` tags, right? This reward function checks exactly that!\n",
    "\n",
    "![Forward Reward (Created by [Fareed Khan](undefined))](https://cdn-images-1.medium.com/max/6620/1*DbUraziwiOoAj6SvtSJmpw.png)\n",
    "\n",
    "If the model uses those tags correctly, we give it a reward of 1. If it messes up the format, it gets 0. Simple as that! This encourages the model to pay attention to the output structure we want.\n",
    "\n",
    "Let‚Äôs code this up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Format Reward Function\n",
    "def format_reward(completions, **kwargs):\n",
    "    \"\"\"\n",
    "    Reward function to check if the completion has the correct format:\n",
    "    <think>...</think> <answer>...</answer>.\n",
    "    \"\"\"\n",
    "    # Define the regex pattern for the desired format\n",
    "    pattern = r\"^<think>.*?</think>\\s*<answer>.*?</answer>$\"\n",
    "\n",
    "    # Extract the content from each completion\n",
    "    completion_contents = [completion[0][\"content\"] for completion in completions]\n",
    "\n",
    "    # Check if each completion matches the pattern\n",
    "    matches = [re.match(pattern, content, re.DOTALL | re.MULTILINE)\n",
    "               for content in completion_contents]\n",
    "\n",
    "    # Reward 1.0 for correct format, 0.0 otherwise\n",
    "    return [1.0 if match else 0.0 for match in matches]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this function:\n",
    "\n",
    "* We define a pattern using regular expressions (regex). This pattern basically says ‚Äúthe content should *start* with <think>, have *anything* inside until </think>, then some *spaces*, then <answer>, *anything* inside until </answer>, and then *end* there‚Äù.\n",
    "\n",
    "* We get the actual text content from each model completion.\n",
    "\n",
    "* Then we use use re.match to see if each content perfectly matches our pattern. re.DOTALL helps the . in regex match newlines too, and re.MULTILINE makes ^ and $ match the start/end of the whole string, not just lines.\n",
    "\n",
    "* Finally, we give a reward 1 if it matched the format perfectly, 0 if it didn‚Äôt. This is a strict on/off reward for format correctness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reasoning Steps Reward\n",
    "\n",
    "Reasoning Steps Reward is a bit clever. We want to encourage our model to show its **‚Äúthinking process‚Äù**. So, we are going to reward it for including things that *look like* reasoning steps.\n",
    "\n",
    "![Reasoning Steps Reward Encouragement (Created by [Fareed Khan](undefined))](https://cdn-images-1.medium.com/max/5406/1*hx0sAVnY58WOYw6rGF64ug.png)\n",
    "\n",
    "We will look for keywords and patterns that usually show up in step-by-step reasoning, like:\n",
    "\n",
    "* Step 1, Step 2, etc.\n",
    "\n",
    "* Numbered lists like 1, 2\n",
    "\n",
    "* Bullet points like - or *\n",
    "\n",
    "* Transition words like First, Second, Next, Finally\n",
    "\n",
    "The more of these it includes, the better the reward. It‚Äôs like giving points for showing its work!\n",
    "\n",
    "Let‚Äôs code this reasoning encouraging function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reasoning_steps_reward(completions, **kwargs):\n",
    "    r\"\"\"\n",
    "    Reward function to encourage clear step-by-step reasoning.\n",
    "    It looks for patterns like \"Step 1:\", numbered lists, bullet points,\n",
    "    and transition words.\n",
    "    \"\"\"\n",
    "    # Regex pattern to find indicators of reasoning steps\n",
    "    pattern = r\"(Step \\d+:|^\\d+\\.|\\n-|\\n\\*|First,|Second,|Next,|Finally,)\"\n",
    "\n",
    "    # Extract completion contents\n",
    "    completion_contents = [completion[0][\"content\"] for completion in completions]\n",
    "\n",
    "    # Count the number of reasoning step indicators in each completion\n",
    "    matches = [len(re.findall(pattern, content, re.MULTILINE))\n",
    "               for content in completion_contents]\n",
    "\n",
    "    # Reward is proportional to the number of reasoning steps, maxing out at 1.0\n",
    "    # We're using a \"magic number\" 3 here - encourage at least 3 steps for full reward\n",
    "    return [min(1.0, count / 3) for count in matches]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We create a pattern that‚Äôs a bit more complex regex. It looks for all those reasoning indicator things we listed above.\n",
    "\n",
    "We use re.findall to find *all* the matches of our pattern within each content. `len(re.findall(‚Ä¶))` then gives us the *count* of these indicators.\n",
    "\n",
    "The reward is calculated as min(1.0, count / 3). This means\n",
    "\n",
    "* If it finds 3 or more reasoning indicators ( count >= 3), the reward is 1.0 (max reward).\n",
    "\n",
    "* If it finds fewer (e.g., count = 1 or 2), it gets a *partial* reward (like 1/3 or 2/3).\n",
    "\n",
    "* If it finds none (count = 0), the reward is 0.0.\n",
    "\n",
    "The / 3 is a bit of a magic number here. We‚Äôre saying **‚Äúaim for about 3 reasoning steps to get full credit‚Äù** You can tweak this number if you want to encourage more or fewer steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Scaled Reward\n",
    "\n",
    "Cosine Scaled Reward is a bit more advanced. It‚Äôs about encouraging *conciseness* in correct answers and being *less harsh* on longer incorrect answers.\n",
    "\n",
    "![Cosine Scaling Concept (Created by [Fareed Khan](undefined))](https://cdn-images-1.medium.com/max/7094/1*WmG8r1OVeU4R3jObAy0yCg.png)\n",
    "\n",
    "Think of it like this:\n",
    "\n",
    "* **For correct answers:** We want to reward *shorter*, more direct solutions more than long, rambling ones. A short, correct answer is often better.\n",
    "\n",
    "* **For incorrect answers:** A short, wrong answer is probably worse than a longer, wrong answer that at least *tried* to reason. So, we want to penalize short wrong answers *more* than long wrong answers.\n",
    "\n",
    "Let‚Äôs see the code that does this clever scaling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Cosine Scaled Reward Function\n",
    "def get_cosine_scaled_reward(\n",
    "    min_value_wrong: float = -0.5,\n",
    "    max_value_wrong: float = -0.1,\n",
    "    min_value_correct: float = 0.8,\n",
    "    max_value_correct: float = 1.0,\n",
    "    max_len: int = 1000,\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns a cosine scaled reward function. This function scales the accuracy reward\n",
    "    based on completion length. Shorter correct solutions get higher rewards,\n",
    "    longer incorrect solutions get less penalty.\n",
    "    \"\"\"\n",
    "    def cosine_scaled_reward(completions, solution, accuracy_rewards, **kwargs):\n",
    "        \"\"\"\n",
    "        Cosine scaled reward function that adjusts accuracy rewards based on completion length.\n",
    "        \"\"\"\n",
    "        contents = [completion[0][\"content\"] for completion in completions]\n",
    "        rewards = []\n",
    "\n",
    "        for content, sol, acc_reward in zip(contents, solution, accuracy_rewards):\n",
    "            gen_len = len(content)  # Length of the generated answer\n",
    "            progress = gen_len / max_len # How far we are to max length\n",
    "            cosine = math.cos(progress * math.pi) # Cosine value based on progress\n",
    "\n",
    "            if acc_reward > 0.5: # Assuming accuracy_reward gives ~1.0 for correct answers\n",
    "                min_value = min_value_correct\n",
    "                max_value = max_value_correct\n",
    "            else: # Incorrect answer\n",
    "                min_value = max_value_wrong  # Note the swap!\n",
    "                max_value = min_value_wrong\n",
    "\n",
    "            # Cosine scaling formula!\n",
    "            reward = min_value + 0.5 * (max_value - min_value) * (1.0 + cosine)\n",
    "            rewards.append(float(reward))\n",
    "        return rewards\n",
    "    return cosine_scaled_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_cosine_scaled_reward(...)` generates a reward function for training, customizing scaling with parameters like min_value_wrong/max_value_wrong (penalty range for incorrect answers) and min_value_correct/max_value_correct (reward range for correct ones). max_len sets the maximum length for scaling.\n",
    "\n",
    "Inside, `cosine_scaled_reward(...)` we calculate rewards based on completions, solution, and accuracy_rewards.\n",
    "\n",
    "It computes gen_len, normalizes it as progress `= gen_len / max_len`, and derives a cosine value that starts at 1 (short answers) and decreases to -1 (long answers).\n",
    "\n",
    "If `acc_reward > 0.5`, it uses the correct reward range, otherwise it applies the incorrect range but swaps min/max values to penalize longer wrong answers less."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repetition Penalty Reward\n",
    "\n",
    "Repetition Penalty Reward is all about discouraging our model from getting stuck in loops and repeating itself. We want it to generate fresh, varied reasoning and answers, not just copy-paste the same phrases over and over!\n",
    "\n",
    "![Repetition Penalty Idea (Created by [Fareed Khan](undefined))](https://cdn-images-1.medium.com/max/8608/1*9jBhiz-rI_fRGa77g9RZtQ.png)\n",
    "\n",
    "This reward function penalizes the model if it uses the same sequences of words (n-grams) too many times. We‚Äôll use n-grams of size 3 (trigrams) in our example, but you can adjust this.\n",
    "\n",
    "If the model repeats itself a lot, it gets a negative reward (penalty). If it‚Äôs more diverse and avoids repetition, the penalty is less.\n",
    "\n",
    "Let‚Äôs implement the code to penalize repetition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_repetition_penalty_reward(ngram_size: int = 3, max_penalty: float = -0.1):\n",
    "    \"\"\"\n",
    "    Returns a repetition penalty reward function. Penalizes repetitions of n-grams\n",
    "    in the generated text.\n",
    "    \"\"\"\n",
    "    if max_penalty > 0:\n",
    "        raise ValueError(f\"max_penalty {max_penalty} should not be positive\")\n",
    "\n",
    "    def zipngram(text: str, ngram_size: int):\n",
    "        \"\"\"Helper function to generate n-grams from text.\"\"\"\n",
    "        words = text.lower().split() # Lowercase and split into words\n",
    "        return zip(*[words[i:] for i in range(ngram_size)]) # Create n-grams\n",
    "\n",
    "    def repetition_penalty_reward(completions, **kwargs) -> float:\n",
    "        \"\"\"\n",
    "        Repetition penalty reward function.\n",
    "        \"\"\"\n",
    "        contents = [completion[0][\"content\"] for completion in completions]\n",
    "        rewards = []\n",
    "        for completion in contents:\n",
    "            if completion == \"\": # No penalty for empty completions\n",
    "                rewards.append(0.0)\n",
    "                continue\n",
    "            if len(completion.split()) < ngram_size: # No penalty for short completions\n",
    "                rewards.append(0.0)\n",
    "                continue\n",
    "\n",
    "            ngrams = set() # Use a set to store unique n-grams\n",
    "            total = 0\n",
    "            for ng in zipngram(completion, ngram_size): # Generate n-grams\n",
    "                ngrams.add(ng) # Add n-gram to the set (duplicates are ignored)\n",
    "                total += 1 # Count total n-grams\n",
    "\n",
    "            # Calculate scaling factor: more repetition -> higher scaling\n",
    "            scaling = 1 - len(ngrams) / total\n",
    "            reward = scaling * max_penalty # Apply penalty based on scaling\n",
    "            rewards.append(reward)\n",
    "        return rewards\n",
    "    return repetition_penalty_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our `get_repetition_penalty_reward(...)` creates a reward function to penalize repetition, with parameters like ngram_size (default 3, for trigrams) and max_penalty (a negative value, e.g., -0.1).\n",
    "\n",
    "A helper function, `zipngram(text, ngram_size)`, generates n-grams by converting text to lowercase, splitting it into words, and using `zip(*[words[i:] for i in range(ngram_size)])` for efficient extraction.\n",
    "\n",
    "Inside, `repetition_penalty_reward(...)` computes the penalty for each completion. If it's empty or too short, it gets a reward of 0.0.\n",
    "\n",
    "The penalty scales as scaling `= 1 - len(ngrams) / total`, where total is the number of n-grams and len(ngrams) is the unique count. More repetition makes scaling approach 1, increasing the penalty.\n",
    "\n",
    "The final reward is scaling * max_penalty, meaning less repetition results in a smaller penalty, while high repetition leads to a stronger negative reward. \n",
    "\n",
    ">We have implemented all five reward functions, Let‚Äôs move on to next stage where we define our training args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Configurations for R1 Zero\n",
    "\n",
    "Now we to code a configuration where we can fine-tune how our *reward functions* actually work. So, Let‚Äôs define that configuration class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define GRPOScriptArguments for reward function parameters\n",
    "@dataclass\n",
    "class GRPOScriptArguments:\n",
    "    \"\"\"\n",
    "    Script arguments for GRPO training, specifically related to reward functions.\n",
    "    \"\"\"\n",
    "\n",
    "    reward_funcs: list[str] = field(\n",
    "        default_factory=lambda: [\"accuracy\", \"format\"],\n",
    "        metadata={\n",
    "            \"help\": \"List of reward functions. Possible values: 'accuracy', 'format', 'reasoning_steps', 'cosine', 'repetition_penalty'\"\n",
    "        },\n",
    "    )\n",
    "    cosine_min_value_wrong: float = field(\n",
    "        default=-0.5,\n",
    "        metadata={\"help\": \"Minimum reward for cosine scaling for wrong answers\"},\n",
    "    )\n",
    "    cosine_max_value_wrong: float = field(\n",
    "        default=-0.1,\n",
    "        metadata={\"help\": \"Maximum reward for cosine scaling for wrong answers\"},\n",
    "    )\n",
    "    cosine_min_value_correct: float = field(\n",
    "        default=0.8,\n",
    "        metadata={\"help\": \"Minimum reward for cosine scaling for correct answers\"},\n",
    "    )\n",
    "    cosine_max_value_correct: float = field(\n",
    "        default=1.0,\n",
    "        metadata={\"help\": \"Maximum reward for cosine scaling for correct answers\"},\n",
    "    )\n",
    "    cosine_max_len: int = field(\n",
    "        default=1000,\n",
    "        metadata={\"help\": \"Maximum length for cosine scaling\"},\n",
    "    )\n",
    "\n",
    "    repetition_n_grams: int = field(\n",
    "        default=3,\n",
    "        metadata={\"help\": \"Number of n-grams for repetition penalty reward\"},\n",
    "    )\n",
    "    repetition_max_penalty: float = field(\n",
    "        default=-0.1,\n",
    "        metadata={\"help\": \"Maximum (negative) penalty for for repetition penalty reward\"},\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our `@dataclass` decorator makes it easy to create a class for storing data. WhileGRPOScriptArguments class holds reward settings.\n",
    "\n",
    "The reward_funcs list decides which rewards to use, starting with [\"accuracy\", \"format\"], but you can add more like \"reasoning_steps\", \"cosine\", \"repetition_penalty\".\n",
    "\n",
    "Some settings control how the cosine_scaled_reward and repetition_penalty_reward work, letting you adjust how rewards are given.\n",
    "\n",
    "Next up, we have TrainingArguments from the transformers library. This is the **main** configuration object that controls almost **everything** about the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define TrainingArguments from transformers\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,          # Output directory for checkpoints and logs\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,             # Total number of training epochs\n",
    "    per_device_train_batch_size=8,  # Batch size per device during training\n",
    "    per_device_eval_batch_size=16,   # Batch size for evaluation\n",
    "    gradient_accumulation_steps=2,  # Accumulate gradients to simulate larger batch size\n",
    "    learning_rate=5e-5,            # Initial learning rate for AdamW optimizer\n",
    "    warmup_ratio=0.1,              # Linear warmup over warmup_ratio fraction of training steps\n",
    "    weight_decay=0.01,             # Apply weight decay to all layers except bias and LayerNorm weights\n",
    "    logging_steps=10,              # Log every X updates steps\n",
    "    evaluation_strategy=\"steps\",    # Evaluate every `eval_steps`\n",
    "    eval_steps=50,                 # Evaluation and logging steps\n",
    "    save_strategy=\"steps\",         # Save checkpoint every `save_steps`\n",
    "    save_steps=50,                 # Save checkpoint every X updates steps\n",
    "    save_total_limit=2,            # Limit the total amount of checkpoints. Deletes the older checkpoints.\n",
    "    dataloader_num_workers=2,      # Number of subprocesses to use for data loading\n",
    "    seed=42,                       # Random seed for reproducibility\n",
    "    bf16=True,                     # Use mixed precision BFP16 training\n",
    "    push_to_hub=False,             # Whether to push the final model to Hugging Face Hub\n",
    "    gradient_checkpointing=True,   # Enable gradient checkpointing\n",
    "    report_to=\"none\",              # Reporting to no one\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to have a ModelConfig. This is where we put settings that are specific to the **model itself**, like which pre-trained model to use, what data type to use (like bfloat16), and whether to trust remote code or not and so.\n",
    "\n",
    "Let‚Äôs define our ModelConfig:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"\n",
    "    Configuration for the model.\n",
    "    \"\"\"\n",
    "    model_name_or_path: str = field(\n",
    "        default=MODEL_NAME, metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n",
    "    )\n",
    "    model_revision: Optional[str] = field(\n",
    "        default=\"main\", metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"}\n",
    "    )\n",
    "    torch_dtype: Optional[str] = field(\n",
    "        default=\"bfloat16\", metadata={\"help\": \"Override the default `torch_dtype` and load the model under this dtype.\"}\n",
    "    )\n",
    "    trust_remote_code: bool = field(\n",
    "        default=True, metadata={\"help\": \"Trust remote code when loading model and tokenizer.\"}\n",
    "    )\n",
    "    attn_implementation: Optional[str] = field(\n",
    "        default=\"flash_attention_2\", metadata={\"help\": \"Attention implementation to use. 'flash_attention_2' or None\"}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our **ModelConfig** class holds key settings, including model_name_or_path, which defaults to **Qwen 0.5B Instruct**. We use torch_dtype=\"bfloat16\" for efficiency and set trust_remote_code=True for safe remote loading. Additionally, attn_implementation=\"flash_attention_2\" is enabled for potentially faster training if supported.\n",
    "\n",
    "Now we need to actually **create** instances of these configuration classes so we can use them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate configuration objects\n",
    "script_args = GRPOScriptArguments()\n",
    "model_args = ModelConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to get our list of reward functions and any ‚Äúcallbacks‚Äù we want to use during training.\n",
    "\n",
    "Callbacks are like little helpers that can do things at different points in the training process (like logging progress, saving models, etc.). For now, we‚Äôll just use a simple logging callback.\n",
    "\n",
    "Getting our reward functions in one place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to get reward functions based on script arguments\n",
    "def get_reward_functions(script_args):\n",
    "    \"\"\"\n",
    "    Returns a list of reward functions based on the script arguments.\n",
    "    \"\"\"\n",
    "    reward_funcs_list = []\n",
    "    reward_funcs_registry = {\n",
    "        \"accuracy\": accuracy_reward,  # Assuming accuracy_reward is defined in previous steps\n",
    "        \"format\": format_reward,      # Assuming format_reward is defined in previous steps\n",
    "        \"reasoning_steps\": reasoning_steps_reward, # Assuming reasoning_steps_reward is defined\n",
    "        \"cosine\": get_cosine_scaled_reward( # Assuming get_cosine_scaled_reward is defined\n",
    "            min_value_wrong=script_args.cosine_min_value_wrong,\n",
    "            max_value_wrong=script_args.cosine_max_value_wrong,\n",
    "            min_value_correct=script_args.cosine_min_value_correct,\n",
    "            max_value_correct=script_args.cosine_max_value_correct,\n",
    "            max_len=script_args.cosine_max_len,\n",
    "        ),\n",
    "        \"repetition_penalty\": get_repetition_penalty_reward( # Assuming get_repetition_penalty_reward is defined\n",
    "            ngram_size=script_args.repetition_n_grams,\n",
    "            max_penalty=script_args.repetition_max_penalty,\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    for func_name in script_args.reward_funcs:\n",
    "        if func_name not in reward_funcs_registry:\n",
    "            raise ValueError(f\"Reward function '{func_name}' not found in registry.\")\n",
    "        reward_funcs_list.append(reward_funcs_registry[func_name])\n",
    "\n",
    "    return reward_funcs_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our callback function which will track loss and other important info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class LoggingCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    A simple callback for logging training information at specific steps.\n",
    "    \"\"\"\n",
    "    def on_step_end(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n",
    "        if state.global_step % args.logging_steps == 0:\n",
    "            logger.info(f\"Step {state.global_step}: Loss = {state.log_history[-1].get('loss', None)}, Learning Rate = {state.log_history[-1].get('learning_rate', None)}\")\n",
    "\n",
    "def get_callbacks(training_args, model_args, script_args):\n",
    "    \"\"\"\n",
    "    Returns a list of callbacks to be used during training.\n",
    "    For now, it includes only the LoggingCallback. You can extend this to add more callbacks.\n",
    "    \"\"\"\n",
    "    callbacks = [LoggingCallback()] # Instantiate our LoggingCallback\n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, initializing these function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get reward functions and callbacks\n",
    "reward_functions = get_reward_functions(script_args)\n",
    "callbacks = get_callbacks(training_args, model_args, script_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRPO Training Loop\n",
    "\n",
    "This is the engine that will actually drive our GRPO training. We need to initialize it, giving it all the pieces we‚Äôve prepared: our model, reward functions, training arguments, dataset, and callbacks!\n",
    "\n",
    "Let‚Äôs initialize the GRPOTrainer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grpo_trainer_override_tmp import GRPOTrainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GRPOConfig from TrainingArguments\n",
    "grpo_config = GRPOConfig(\n",
    "    **training_args.to_dict(), # Convert TrainingArguments to dictionary and unpack\n",
    "    **{ \n",
    "       # REMOVED model_init_kwargs here \n",
    "       # We are passing the instantiated 'model' object, so GRPOTrainer doesn't need model_init_kwargs\n",
    "    }\n",
    ")\n",
    "\n",
    "grpo_trainer = GRPOTrainer(\n",
    "    model=model,                      # Our initialized Qwen model\n",
    "    reward_funcs=reward_functions,    # List of reward functions from previous step\n",
    "    args=grpo_config,                # GRPOConfig (created from TrainingArguments)\n",
    "    train_dataset=dataset['train'],   # Training dataset\n",
    "    eval_dataset=dataset['test'],    # Evaluation dataset\n",
    "    callbacks=callbacks              # List of callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now start the **Training Loop**! This is as simple as calling the train() method on our grpo_trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the GRPO Training Loop\n",
    "train_result = grpo_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you run this cell, you should see the training process begin.\n",
    "\n",
    "Training will take some time but we set **num_train_epochs = 1** and are using a small model, it shouldn‚Äôt take *too* long for this example.\n",
    "\n",
    "But for real-world GRPO DeepSeek R1 Zero training, you‚Äôd likely train for many more epochs and steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Tiny R1 Zero LLM\n",
    "\n",
    "Once the training completed, we can save our trained model which can be used for inferencing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to your trained model (same as OUTPUT_DIR)\n",
    "TRAINED_MODEL_PATH = \"data/Qwen-GRPO-training\"\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(TRAINED_MODEL_PATH)\n",
    "\n",
    "# Save the trained model\n",
    "grpo_trainer.save_model(TRAINED_MODEL_PATH)\n",
    "\n",
    "print(f\"GRPO Trained model saved to {TRAINED_MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can simply load the trained model using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer - make sure to use trust_remote_code=True if needed\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    TRAINED_MODEL_PATH,\n",
    "    trust_remote_code=True, # If your model config requires it\n",
    "    padding_side=\"right\" # Ensure consistent padding side\n",
    ")\n",
    "\n",
    "# Set pad token if it wasn't saved or loaded correctly\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load the trained model itself\n",
    "trained_model = AutoModelForCausalLM.from_pretrained(\n",
    "    TRAINED_MODEL_PATH,\n",
    "    trust_remote_code=True, # If your model architecture requires it\n",
    "    torch_dtype=torch.bfloat16 # Keep the same dtype as training for consistency\n",
    ")\n",
    "\n",
    "# Move the loaded model to your device (GPU if available)\n",
    "trained_model.to(device) # 'device' is still our CUDA device from before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use it for inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Inference with the Trained Model\n",
    "def test_trained_model_inference(user_input: str):\n",
    "    \"\"\"Test inference with the loaded trained model and tokenizer.\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT}, # Re-use our system prompt\n",
    "        {\"role\": \"user\", \"content\": user_input}\n",
    "    ]\n",
    "\n",
    "    # Apply chat template using our tokenizer\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Generate output using our *trained_model*\n",
    "    outputs = trained_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=200, # Maybe generate a bit longer now\n",
    "        do_sample=True,\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "    # Decode the generated tokens back to text\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Test the model\n",
    "test_input = \"how are you?\"\n",
    "response = test_trained_model_inference(test_input)\n",
    "print(f\"Test Input: {test_input}\")\n",
    "print(f\"Trained Model Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two main problems with R1 Zero\n",
    "\n",
    "Now that we have completed our R1 zero training approach using our base model Qwen2‚Äì0.5B instead of their DeepSeek V3 (original base model).\n",
    "\n",
    "We cannot identify our trained model problems but researches of DeepSeek saw the R1 Zero model performed really well on reasoning tests, even scoring similarly to more advanced models like **OpenAI-01‚Äì0912** on tasks like **AIME 2024**.\n",
    "\n",
    "This showed that using reinforcement learning (RL) to encourage reasoning in language models is a promising approach.\n",
    "\n",
    "But they also noticed DeepSeek-R1-Zero had some key issues that needed fixing for real world use and wider research.\n",
    "\n",
    "![Problem with R1 Zero (Created by [Fareed Khan](undefined))](https://cdn-images-1.medium.com/max/6378/1*_NdVhpb9cgT3-8o3Qn7mMA.png)\n",
    "\n",
    "Researchers of DeepSeek states that the template is *intentionally simple and structurally focused*. It *avoids* imposing any *content-specific* constraints on the *reasoning process itself*. For example, it doesn‚Äôt say:\n",
    "\n",
    "* ‚ÄúYou *must* use step-by-step reasoning‚Äù (It just says ‚Äúreasoning process‚Äù leaving it open to the model to define what that means).\n",
    "\n",
    "* ‚ÄúYou *must* use reflective reasoning‚Äù\n",
    "\n",
    "* ‚ÄúYou *must* use a specific problem-solving strategy‚Äù\n",
    "\n",
    "The main problem was that the reasoning processes inside the `<think>` tags were hard to read, making it tough for humans to follow and analyze.\n",
    "\n",
    "Another issue was language mixing, when asked multi-lingual questions, the model sometimes mixed languages in the same response, leading to inconsistent and confusing outputs.\n",
    "\n",
    "If you asked it questions in, say, Spanish. Suddenly, its ‚Äúthinking‚Äù would be a jumbled mix of **English and Spanish, **not exactly polished! These problems, messy reasoning and language confusion, were the clear roadblocks.\n",
    "> These are the two main reasons they transformed their initial R1 Zero Model into the R1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Cold Start Data for SFT\n",
    "\n",
    "So to fix R1 Zero issues and really get DeepSeek reasoning properly, researchers performed a **Cold Start Data Collection and included Supervised Fine Tuning**.\n",
    "\n",
    "You can think of it as giving the model a good foundation in reasoning before the really intense RL training. Basically, they wanted to teach **DeepSeek-V3 Base** what good reasoning looks like and how to present it clearly.\n",
    "\n",
    "One of the example of cold start data is [Bespoke-Stratos-17k](https://huggingface.co/datasets/bespokelabs/Bespoke-Stratos-17k) that we see earlier and will be using for creating R1, but **we need to understand how cold dataset is created so we wont skip any part from the actual training**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-shot Prompting with Long CoT\n",
    "\n",
    "One technique is **Few-shot Prompting with Long Chain-of-Thought (CoT),** in which we try to show DeepSeek-V3 Base (or in our case, Qwen2‚Äì0.5B) few examples of questions paired with super detailed, step-by-step solutions. This is Chain-of-Thought (CoT).\n",
    "\n",
    "![Long CoT (Created by [Fareed Khan](undefined))](https://cdn-images-1.medium.com/max/4068/1*SAhvB0JqaK4d45IiIcj1Ow.png)\n",
    "\n",
    "Goal of this approach is to make the model learn by example and start mimicking this thorough reasoning style.\n",
    "\n",
    "For our example problem ‚ÄúWhat is 2 + 3 * 4?‚Äù, we can create prompts that include a few solved problems as examples. Let‚Äôs see how this looks in Python:\n",
    "```python\n",
    "# Loading Model and Tokenizer\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True, padding_side=\"right\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, trust_remote_code=True, torch_dtype=torch.bfloat16).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Generate Long COT Response\n",
    "def generate_response(prompt_text):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that provides step-by-step solutions.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt_text}\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=200, do_sample=False) # Keep it deterministic for example\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response.split(\"<|im_start|>assistant\\n\")[-1].strip() # Extract assistant's response\n",
    "```\n",
    "Let‚Äôs define the few shot examples accordingly for our asked question:\n",
    "```python\n",
    "# Example problems with solutions (using | special_token | as delimiter)\n",
    "few_shot_prompt = \"\"\"\n",
    "Problem: What's the square root of 9 plus 5?\n",
    "Solution: <|special_token|> First, find the square root of 9, which is 3. Then, add 5 to 3.  3 + 5 equals 8. <|special_token|> Summary: The answer is 8.\n",
    "\n",
    "Problem: Train travels at 60 mph for 2 hours, how far?\n",
    "Solution: <|special_token|> Use the formula: Distance = Speed times Time. Speed is 60 mph, Time is 2 hours. Distance = 60 * 2 = 120 miles. <|special_token|> Summary: Train travels 120 miles.\n",
    "\n",
    "Problem: What is 2 + 3 * 4?\n",
    "Solution:\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "Now using our base model our sample generations looks like this:\n",
    "```python\n",
    "# Generate response for the target problem using few-shot examples\n",
    "target_problem_prompt = few_shot_prompt + \"What is 2 + 3 * 4?\"\n",
    "model_response_few_shot = generate_response(target_problem_prompt)\n",
    "\n",
    "print(\"Few-shot Prompt:\")\n",
    "print(target_problem_prompt)\n",
    "print(\"\\nModel Response (Few-shot CoT):\")\n",
    "print(model_response_few_shot)\n",
    "```\n",
    "\n",
    "It output this structured data\n",
    "\n",
    "```\n",
    "Few-shot Prompt:\n",
    "Problem: What's the square root of 9 plus 5?\n",
    "Solution: <|special_token|> First, find the square root of 9, \n",
    "which is 3. Then, add 5 to 3.  3 + 5 equals 8. \n",
    "<|special_token|> Summary: The answer is 8.\n",
    "\n",
    "Problem: Train travels at 60 mph for 2 hours, how far?\n",
    "Solution: <|special_token|> Use the formula: Distance = Speed times Time. \n",
    "Speed is 60 mph, Time is 2 hours. Distance = 60 * 2 = 120 miles. \n",
    "<|special_token|> Summary: Train travels 120 miles.\n",
    "\n",
    "Problem: What is 2 + 3 * 4?\n",
    "Solution: \n",
    "\n",
    "Model Response (Few-shot CoT):\n",
    "<|special_token|> To solve 2 + 3 * 4, we need to follow the order \n",
    "of operations (PEMDAS/BODMAS). Multiplication should be performed \n",
    "before addition.\n",
    "Step 1: Multiply 3 by 4, which equals 12.\n",
    "Step 2: Add 2 to the result from Step 1: 2 + 12 = 14.\n",
    "<|special_token|> Summary: The answer is 14.\n",
    "```\n",
    "\n",
    "See how the model, after seeing examples, starts to structure its answer with <|special_token|> delimiters and provides step-by-step reasoning leading to the summary and final answer!\n",
    "\n",
    "This is the power of few-shot learning guiding the model towards the desired output format.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct Prompting\n",
    "\n",
    "Another method is **Direct Prompting**. Here, we directly instruct the model to not just solve the problem, but also to explicitly show its reasoning step-by-step and then verify its answer. \n",
    "\n",
    "This is about encouraging a more deliberate and thoughtful problem-solving approach.\n",
    "\n",
    "![Example based learning (Created by [Fareed Khan](undefined))](https://cdn-images-1.medium.com/max/4656/1*IYyk7UWgDNADFe_djWcXow.png)\n",
    "\n",
    "Let‚Äôs craft a prompt for ‚ÄúWhat is 2 + 3 * 4?‚Äù that explicitly asks for reasoning and verification. Here‚Äôs the Python code to see it in action:\n",
    "```python\n",
    "# Direct prompting example\n",
    "direct_prompt_text = \"\"\"\n",
    "Problem: Solve this, show reasoning step-by-step, and verify:\n",
    "What is 2 + 3 * 4?\n",
    "\"\"\"\n",
    "\n",
    "model_response_direct = generate_response(direct_prompt_text)\n",
    "\n",
    "print(\"Direct Prompt:\")\n",
    "print(direct_prompt_text)\n",
    "print(\"\\nModel Response (Direct Prompting):\")\n",
    "print(model_response_direct)\n",
    "```\n",
    "The direct prompting output is very easy to understand and this is what it looks like:\n",
    "```\n",
    "Direct Prompt:\n",
    "Problem: Solve this, show reasoning step-by-step, and verify:\n",
    "What is 2 + 3 * 4?\n",
    "\n",
    "Model Response (Direct Prompting):\n",
    "<|special_token|> Reasoning: To solve 2 + 3 * 4, I need to follow \n",
    "the order of operations, which states that multiplication should \n",
    "be done before addition.\n",
    "Step 1: Multiply 3 by 4, which equals 12.\n",
    "Step 2: Add 2 to the result from Step 1: 2 + 12 = 14.\n",
    "Verification: To verify the answer, I can double-check the \n",
    "order of operations and the calculations. Multiplication is \n",
    "indeed performed before addition, and the calculations are correct.\n",
    "<|special_token|> Summary: The answer is 14.\n",
    "```\n",
    "As you can see, by directly asking for reasoning and verification, the model provides a more comprehensive output, including a ‚ÄúVerification‚Äù section.\n",
    "\n",
    "This method directly guides the model to produce the kind of detailed reasoning we are looking for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post Processing Refinement\n",
    "\n",
    "The final technique involves **Post-Processing Refinement**. Interestingly, they even used the outputs from the already trained R1 Zero model for this!\n",
    "\n",
    "Even with its issues, R1 Zero could reason somewhat. So, they took R1 Zero outputs and had human annotators refine them, making them cleaner, more structured, and correcting any mistakes.\n",
    "\n",
    "![Processing Refnement (Created by [Fareed Khan](undefined))](https://cdn-images-1.medium.com/max/4388/1*-GR29EAnTOVBarQ2JrF5sA.png)\n",
    "\n",
    "Imagine a messy R1 Zero output like this:\n",
    "```\n",
    "<think>  ummm... multiply 3 and 4... get 12... then add 2...</think>\n",
    "<answer> 14 </answer>\n",
    "```\n",
    "\n",
    "Human annotators would then refine it to something much clearer and better formatted:\n",
    "```\n",
    "<|special_token|> Reasoning: To solve this, we use order of operations, doing multiplication before addition.\n",
    "Step 1: Multiply 3 by 4, which is 12.\n",
    "Step 2: Add 2 to the result: 2 + 12 = 14.\n",
    "<|special_token|> Summary: The answer is 14.\n",
    "```\n",
    "\n",
    "While we can‚Äôt perfectly simulate human refinement in code, we can demonstrate a basic idea of how you might programmatically reformat and structure a potentially messy output. \n",
    "\n",
    "Let‚Äôs take a simulated ‚Äúmessy‚Äù output and show how we could refine it:\n",
    "```python\n",
    "# Simulated messy R1 Zero output\n",
    "messy_output = \"<think>  ummm... multiply 3 and 4... get 12... then add 2...</think>\\n<answer> 14 </answer>\"\n",
    "\n",
    "def refine_output(messy_text):\n",
    "    think_content = messy_text.split(\"<think>\")[1].split(\"</think>\")[0].strip()\n",
    "    answer_content = messy_text.split(\"<answer>\")[1].split(\"</answer>\")[0].strip()\n",
    "\n",
    "    refined_text = f\"\"\"<|special_token|> Reasoning: {think_content.replace('umm...', '').strip().capitalize()}.\n",
    "<|special_token|> Summary: The answer is {answer_content}.\"\"\"\n",
    "    return refined_text\n",
    "\n",
    "refined_output_text = refine_output(messy_output)\n",
    "\n",
    "print(\"Messy Output (Simulated R1 Zero):\")\n",
    "print(messy_output)\n",
    "print(\"\\nRefined Output:\")\n",
    "print(refined_output_text)\n",
    "```\n",
    "\n",
    "This will output:\n",
    "```\n",
    "Messy Output (Simulated R1 Zero):\n",
    "<think>  ummm... multiply 3 and 4... get 12... then add 2...</think>\n",
    "<answer> 14 </answer>\n",
    "\n",
    "Refined Output:\n",
    "<|special_token|> Reasoning: Multiply 3 and 4... get 12... then add 2.\n",
    "<|special_token|> Summary: The answer is 14.\n",
    "```\n",
    "\n",
    "This simple refine_output function is just a basic example. Real refinement by humans involves much more nuanced understanding and correction of reasoning steps.\n",
    "\n",
    "However, it shows the core idea: taking initial model outputs and improving their quality and structure to create better training data.\n",
    "> After generating this Cold Start Data, the next crucial step was **Supervised Fine-Tuning (SFT)**, which we‚Äôll explore in the next section!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SFT Stage 1 With Cold Start Data\n",
    "\n",
    "To generate proper cold start data to build R1 using Supervised fine-tuning, we obviously need a proper team along with an excessive amount of code, but thankfully, we already have data ([Bespoke-Stratos-17k](https://huggingface.co/datasets/bespokelabs/Bespoke-Stratos-17k)) that is similar to the cold start form.\n",
    "> We need to know what and how training happens inside the SFT Trainer as it processes our training data?\n",
    "\n",
    "SFT is a form of supervised learning. This means we‚Äôre giving the model pairs of inputs and *desired* outputs.\n",
    "\n",
    "In our case, the input might be a problem prompt, and the desired output is the well-reasoned, step-by-step solution from our training dataset. **I hope this point gives a clear view of why there is a need of cold data.**\n",
    "\n",
    "It takes our tokenized training data and feeds it to the model in batches. For each batch, a important set of operations happens, Let‚Äôs visualize this internal process:\n",
    "\n",
    "![SFT WorkFlow (Created by [Fareed Khan](undefined))](https://cdn-images-1.medium.com/max/6838/1*EsEgATw1aSYPjfGtpId2mQ.png)\n",
    "\n",
    "First, the model takes an input, a problem prompt, for instance. It processes this input and generates its best guess for the solution, token by token. These are the *predicted tokens*.\n",
    "\n",
    "Next, the SFT Trainer needs to know how good (or bad) these predictions are. It uses a *loss function*, typically Cross-Entropy Loss. This function mathematically compares the model‚Äôs predicted tokens to the *correct* tokens from our training data. Think of it as calculating the ‚Äúerror‚Äù of the model‚Äôs answer.\n",
    "\n",
    "This ‚Äúerror‚Äù isn‚Äôt just discarded. It‚Äôs the crucial signal for learning. Through a process called *backpropagation*, this error is used to calculate *gradients*. Gradients are like guides, pointing in the direction of parameter adjustments that would reduce the error.\n",
    "\n",
    "Finally, an *optimizer*, like **AdamW** uses these gradients to subtly tweak the model‚Äôs internal settings ‚Äî its parameters. These tweaks are designed to make the model‚Äôs next prediction a little bit closer to the correct answer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1 SFT Trainer Configs for R1\n",
    "\n",
    "Remember those problems we had with R1 Zero messy reasoning and language mixing? SFT is designed to fix exactly that. By training on high-quality, refined data, we‚Äôre teaching the model:\n",
    "\n",
    "* **Clear Reasoning Style**: To structure its ‚Äúthinking‚Äù in a way that‚Äôs easy to read and follow.\n",
    "\n",
    "* **Consistent Language**: To stick to one language within a response, avoiding confusing mixes.\n",
    "\n",
    "We‚Äôre using the Bespoke-Stratos-17k dataset for SFT. As we saw earlier, it‚Äôs got 17,000 problems focused on math and code, with a format that looks pretty good for our needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let‚Äôs quickly remind ourselves of a sample from Bespoke-Stratos-17k:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the \"Bespoke-Stratos-17k\" dataset from bespokelabs\n",
    "bespoke_rl = load_dataset(\"bespokelabs/Bespoke-Stratos-17k\", \"default\")\n",
    "\n",
    "# Access the first sample in the training set\n",
    "bespoke_rl['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset, with its system prompts and user-assistant conversations, is perfect for showing our model how conversations with reasoning should look.\n",
    "\n",
    "We‚Äôll use the trl library again, which makes SFT training super easy.\n",
    "\n",
    "First, we need to set up our configurations, similar to what we did for GRPO, but this time for SFT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and Output Configuration (same as before, or adjust as needed)\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "OUTPUT_DIR = \"data/Qwen-SFT-training\" # New output directory for SFT model\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Training Arguments - similar to GRPO, but adjust for SFT\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,         # Adjust epochs as needed\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=2e-5,        # Adjust learning rate for SFT\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"no\",\n",
    "    eval_steps=50,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,\n",
    "    save_total_limit=2,\n",
    "    dataloader_num_workers=2,\n",
    "    seed=42,\n",
    "    bf16=True,\n",
    "    push_to_hub=False,\n",
    "    gradient_checkpointing=True,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# Model Configuration - same as before\n",
    "model_args = ModelConfig(\n",
    "    model_name_or_path=MODEL_NAME,\n",
    "    model_revision=\"main\",\n",
    "    torch_dtype=\"bfloat16\",\n",
    "    trust_remote_code=True,\n",
    "    attn_implementation=\"flash_attention_2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These TrainingArguments and ModelConfig are quite similar to what we used for GRPO, but with a few tweaks that are more suitable for SFT (like a slightly different learning rate, and importantly, packing=True and max_seq_length=4096 for efficient training on longer sequences)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1 STF Training Loop\n",
    "\n",
    "Now, let‚Äôs load our dataset and tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Bespoke-Stratos-17k dataset\n",
    "dataset_sft = load_dataset(\"HuggingFaceH4/Bespoke-Stratos-17k\", split='train') # Only using train split for simplicity\n",
    "\n",
    "# Initialize tokenizer - same as before\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    trust_remote_code=True,\n",
    "    padding_side=\"right\"\n",
    ")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, we initialize the SFTTrainer and start training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize base model for SFT - same as before\n",
    "model_sft = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Initialize the SFT Trainer\n",
    "sft_trainer = SFTTrainer(\n",
    "    model=model_sft,                     # Our initialized Qwen model\n",
    "    train_dataset=dataset_sft,           # Bespoke-Stratos-17k dataset\n",
    "    tokenizer=tokenizer,                 # Tokenizer\n",
    "    args=training_args,                  # Training arguments\n",
    ")\n",
    "\n",
    "# Start the SFT Training Loop\n",
    "sft_train_result = sft_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you run this code, you‚Äôll see the SFT training process start. It will look similar to the GRPO training output, showing loss and learning rate at each logging step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like with GRPO, training time will depend on your hardware and chosen epochs. Since we‚Äôre still using a small model and only 1 epoch for this example, it should be reasonably quick."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Tiny R1 LLM\n",
    "\n",
    "After SFT is done, we save our newly fine-tuned model (R1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the Trained SFT Model\n",
    "TRAINED_SFT_MODEL_PATH = \"data/Qwen-SFT-training\" # Same as OUTPUT_DIR\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(TRAINED_SFT_MODEL_PATH)\n",
    "\n",
    "# Save the trained model\n",
    "sft_trainer.save_model(TRAINED_SFT_MODEL_PATH)\n",
    "\n",
    "print(f\"SFT Trained model saved to {TRAINED_SFT_MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that‚Äôs it for the SFT part! We‚Äôve now taken our base model, shown it lots of examples of good reasoning, and fine-tuned it to be better at producing clear, structured responses.\n",
    "> This finetuned model using SFT is what we called R1 after SFT stage 1\n",
    "\n",
    "The steps after SFT, especially the RL stages and rejection sampling, are complex to implement from scratch in Python. Focusing on the theoretical understanding is key to understand the overall process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reasoning-Oriented RL\n",
    "\n",
    "After SFT, the model can reason better, but we want to *really* focus on reasoning quality and fix language mixing. This stage uses RL again, but with a smarter reward system.\n",
    "\n",
    "This new reward checks if the model reasoning and answer are in the same language as the question. If you ask in English, the *whole* response should be in English. This fixes language mixing issues.\n",
    "\n",
    "![Reasoning Oriented RL (Created by [Fareed Khan](undefined))](https://cdn-images-1.medium.com/max/7468/1*Z2oHDdkWb7RnO5uVHPSvMg.png)\n",
    "\n",
    "It adds a **Language Consistency Reward** alongside accuracy to ensure the SFT model reasons and answers in the same language as the input.\n",
    "\n",
    "The GRPO algorithm and training loop from R1 Zero are reused, but the reward signals are improved to specifically target better reasoning and consistent language output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rejection Sampling\n",
    "\n",
    "To get super high-quality reasoning data, DeepSeek uses **Rejection Sampling**. Think of it as a filter to keep only the *best* examples.\n",
    "\n",
    "![Rejection Sampling (Created by [Fareed Khan](undefined))](https://cdn-images-1.medium.com/max/8520/1*obG-BrhwtIuOv7YBZIpSwg.png)\n",
    "\n",
    "The model generates many reasoning examples. These are then evaluated for correctness and reasoning quality (often using a generative reward model and human checks).\n",
    "\n",
    "Only the *best*, high-quality reasoning examples are kept. Combined with non-reasoning data, this refined dataset is used for a second **SFT Stage 2**, further improving reasoning and general abilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SFT Stage 2 Training\n",
    "\n",
    "The final RL stage focuses on making the model a helpful and safe AI assistant for *all* situations, not just reasoning problems. This is about alignment with human values.\n",
    "\n",
    "**Key Focus: Helpfulness & Harmlessness Rewards**\n",
    "\n",
    "Not just accuracy, the reward system now includes:\n",
    "\n",
    "* **Helpfulness:** Is the response useful and informative?\n",
    "\n",
    "* **Harmlessness:** Is the response safe, unbiased, and ethical?\n",
    "\n",
    "![SFT Stage 2 (Created by [Fareed Khan](undefined))](https://cdn-images-1.medium.com/max/7086/1*_u5ALx4VYQpsSgT_0s10HQ.png)\n",
    "\n",
    "The training data becomes diverse, including reasoning tasks and human preference data (which output is better ‚Äî more helpful, less harmful?).\n",
    "\n",
    "The reward system now balances accuracy with **helpfulness and harmlessness**. Iterative RL training (likely GRPO again) optimizes the model to be not just good at reasoning, but also a safe and helpful AI assistant for general use, resulting in DeepSeek R1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distillation\n",
    "\n",
    "To make DeepSeek R1 accessible, they **distilled** its knowledge into smaller models.\n",
    "\n",
    "![Distillation Process (Created by [Fareed Khan](undefined))](https://cdn-images-1.medium.com/max/2500/0*QdOxtvuKaEASreK0.png)\n",
    "\n",
    "Distillation takes the knowledge of a large, powerful ‚Äúteacher‚Äù model (DeepSeek R1) and transfers it to smaller ‚Äústudent‚Äù models. Using a large dataset of reasoning examples, the outputs of DeepSeek R1 are used as the *target* answers.\n",
    "\n",
    "Smaller models are then trained (SFT) to mimic these outputs. This results in smaller, faster models that retain a significant portion of DeepSeek R1‚Äôs reasoning abilities, making them more practical for wider use."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-r10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
