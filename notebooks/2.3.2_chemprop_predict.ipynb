{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import json\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "from lightning import pytorch as pl\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "import numpy as np\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "from chemprop import data, featurizers, models, nn\n",
    "\n",
    "sys.path.insert(0, '../agenticadmet')\n",
    "from eval import eval_admet, extract_preds, extract_refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORKERS = 0 # number of workers for dataloader. 0 means using main process for data loading\n",
    "SMILES_COLUMN = 'smiles_std'\n",
    "TARGET_COLUMNS = ['LogHLM', 'LogMLM', 'LogD', 'LogKSOL', 'LogMDR1-MDCKII']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(input_df, smiles_column_for_pred: str = SMILES_COLUMN):\n",
    "    train_data, val_data = [], []\n",
    "    for _, row in input_df.iterrows():\n",
    "        dp = data.MoleculeDatapoint.from_smi(row[SMILES_COLUMN], row[TARGET_COLUMNS].values)\n",
    "        if row['split'] == 'train':\n",
    "            train_data.append(dp)\n",
    "        elif row['split'] == 'val':\n",
    "            val_data.append(dp)\n",
    "\n",
    "    pred_data = []\n",
    "    for _, row in input_df.iterrows():\n",
    "        dp = data.MoleculeDatapoint.from_smi(row[smiles_column_for_pred], row[TARGET_COLUMNS].values)\n",
    "        pred_data.append(dp)\n",
    "\n",
    "    featurizer = featurizers.SimpleMoleculeMolGraphFeaturizer()\n",
    "\n",
    "    train_dset = data.MoleculeDataset(train_data, featurizer)\n",
    "    # scaler = train_dset.normalize_targets()\n",
    "\n",
    "    val_dset = data.MoleculeDataset(val_data, featurizer)\n",
    "    # val_dset.normalize_targets(scaler)\n",
    "\n",
    "    pred_dset = data.MoleculeDataset(pred_data, featurizer)\n",
    "\n",
    "    return train_dset, val_dset, pred_dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config, train_dset, val_dset, num_workers, save_dir, run_idx, enable_logger=True):\n",
    "    # config is a dictionary containing hyperparameters used for the trial\n",
    "    depth = int(config[\"depth\"])\n",
    "    ffn_hidden_dim = int(config[\"ffn_hidden_dim\"])\n",
    "    ffn_num_layers = int(config[\"ffn_num_layers\"])\n",
    "    message_hidden_dim = int(config[\"message_hidden_dim\"])\n",
    "    dropout = float(config[\"dropout\"])\n",
    "\n",
    "    train_loader = data.build_dataloader(train_dset, num_workers=num_workers, shuffle=True)\n",
    "    val_loader = data.build_dataloader(val_dset, num_workers=num_workers, shuffle=False)\n",
    "\n",
    "    mp = nn.BondMessagePassing(d_h=message_hidden_dim, depth=depth, dropout=dropout)\n",
    "    agg = nn.MeanAggregation()\n",
    "    ffn = nn.RegressionFFN(\n",
    "        n_tasks=len(TARGET_COLUMNS),\n",
    "        output_transform=None, input_dim=message_hidden_dim, hidden_dim=ffn_hidden_dim, n_layers=ffn_num_layers,\n",
    "        dropout=dropout\n",
    "    )\n",
    "    batch_norm = True\n",
    "    metric_list = [nn.metrics.MAE(), nn.metrics.R2Score()]\n",
    "    model = models.MPNN(mp, agg, ffn, batch_norm, metric_list)\n",
    "\n",
    "    ckpt_callback = ModelCheckpoint(\n",
    "        save_top_k=0,\n",
    "        save_last=True\n",
    "    )\n",
    "\n",
    "    if enable_logger:\n",
    "        exp_name = f\"chemprop_run_{run_idx}\"\n",
    "        logger = WandbLogger(\n",
    "            project=\"admet-challenge\",\n",
    "            name=exp_name,\n",
    "            prefix=f\"{save_dir.stem}\",\n",
    "            save_dir=f\"../wandb/{exp_name}\"\n",
    "        )\n",
    "    else:\n",
    "        logger = None\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        accelerator=\"auto\",\n",
    "        devices=1,\n",
    "        max_epochs=200,\n",
    "        enable_progress_bar=False,\n",
    "        callbacks=[ckpt_callback],\n",
    "        default_root_dir=save_dir,\n",
    "        logger=logger\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        trainer.fit(model, train_loader, val_loader)\n",
    "    except Exception as e:\n",
    "        if logger is not None:\n",
    "            logger.finalize(\"failed\")\n",
    "            wandb.finish(exit_code=1)\n",
    "        raise e\n",
    "    else:\n",
    "        if logger is not None:\n",
    "            logger.finalize(\"success\")\n",
    "\n",
    "    return model\n",
    "\n",
    "def predict(model, pred_dset, num_workers):\n",
    "    pred_loader = data.build_dataloader(pred_dset, num_workers=num_workers, shuffle=False)\n",
    "    \n",
    "    trainer = pl.Trainer(\n",
    "        accelerator=\"auto\",\n",
    "        devices=1,\n",
    "        enable_progress_bar=False\n",
    "    )\n",
    "\n",
    "    model.eval()\n",
    "    preds = trainer.predict(model, pred_loader, return_predictions=True)\n",
    "    preds = torch.cat(preds)\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CONFIG = {\n",
    "    \"depth\": 4,\n",
    "    \"ffn_hidden_dim\": 500,\n",
    "    \"ffn_num_layers\": 1,\n",
    "    \"message_hidden_dim\": 1000,\n",
    "    \"dropout\": 0.2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval(input_paths, save_dirs, run_idx):\n",
    "    for input_path, save_dir in zip(input_paths, save_dirs):\n",
    "        print(f\"Training and predicting on {input_path}\")\n",
    "        input_df = pd.read_csv(input_path)\n",
    "        train_dset, val_dset, pred_dset = prepare_data(input_df)\n",
    "        model = train_model(MODEL_CONFIG, train_dset, val_dset, NUM_WORKERS, save_dir, run_idx)\n",
    "        preds = predict(model, pred_dset, NUM_WORKERS)\n",
    "\n",
    "        output_df = input_df.copy()\n",
    "        output_df[[\"pred_\" + t for t in TARGET_COLUMNS]] = preds\n",
    "        save_dir.mkdir(parents=True, exist_ok=True)\n",
    "        output_df.to_csv(save_dir / \"predictions.csv\", index=False)\n",
    "\n",
    "        train_preds = extract_preds(output_df[input_df[\"split\"] == \"train\"])\n",
    "        train_refs = extract_refs(input_df[input_df[\"split\"] == \"train\"])\n",
    "        val_preds = extract_preds(output_df[input_df[\"split\"] == \"val\"])\n",
    "        val_refs = extract_refs(input_df[input_df[\"split\"] == \"val\"])\n",
    "\n",
    "        metrics = eval_admet(train_preds, train_refs)\n",
    "        print(\"Train metrics:\")\n",
    "        print(json.dumps(metrics, indent=2))\n",
    "\n",
    "        metrics = eval_admet(val_preds, val_refs)\n",
    "        print(\"\\nVal metrics:\")\n",
    "        print(json.dumps(metrics, indent=2))\n",
    "    \n",
    "    wandb.finish(exit_code=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_paths = [Path(f'../data/asap/datasets/rnd_splits/split_{k}.csv') for k in range(5)]\n",
    "save_dirs = [Path(f'../output/asap/rnd_splits/chemprop/run_0/split_{k}') for k in range(5)]\n",
    "RUN_IDX = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predicting on ../data/asap/datasets/rnd_splits/split_0.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvladvin111\u001b[0m (\u001b[33mvladvin-org\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>../wandb/chemprop_run_0/wandb/run-20250311_210018-122z40s0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vladvin-org/admet-challenge/runs/122z40s0' target=\"_blank\">chemprop_run_0</a></strong> to <a href='https://wandb.ai/vladvin-org/admet-challenge' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/vladvin-org/admet-challenge' target=\"_blank\">https://wandb.ai/vladvin-org/admet-challenge</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/vladvin-org/admet-challenge/runs/122z40s0' target=\"_blank\">https://wandb.ai/vladvin-org/admet-challenge/runs/122z40s0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type               | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | message_passing | BondMessagePassing | 2.2 M  | train\n",
      "1 | agg             | MeanAggregation    | 0      | train\n",
      "2 | bn              | BatchNorm1d        | 2.0 K  | train\n",
      "3 | predictor       | RegressionFFN      | 503 K  | train\n",
      "4 | X_d_transform   | Identity           | 0      | train\n",
      "5 | metrics         | ModuleList         | 0      | train\n",
      "---------------------------------------------------------------\n",
      "2.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.7 M     Total params\n",
      "10.656    Total estimated model params size (MB)\n",
      "25        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/core/saving.py:363: Skipping 'metrics' parameter because it is not possible to safely dump to YAML.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.24934580146981947,\n",
      "    \"r2\": 0.7725539578721019\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.14027972797661595,\n",
      "    \"r2\": 0.7859417225647726\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.2213262028144005,\n",
      "    \"r2\": 0.8007038226326904\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.212618682984524,\n",
      "    \"r2\": 0.7750497069335556\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.27666164881640376,\n",
      "    \"r2\": 0.9166590204103382\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.2200464128123527,\n",
      "    \"macro_r2\": 0.8101816460826917\n",
      "  }\n",
      "}\n",
      "\n",
      "Val metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.45419421337664967,\n",
      "    \"r2\": 0.2237504750256415\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.2271353346828401,\n",
      "    \"r2\": 0.4693530487277412\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.3128545245774797,\n",
      "    \"r2\": 0.5870099105461248\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.36991042838739857,\n",
      "    \"r2\": 0.21709282186170376\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.45175654232501977,\n",
      "    \"r2\": 0.7929418951606493\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.3631702086698776,\n",
      "    \"macro_r2\": 0.45802963026437205\n",
      "  }\n",
      "}\n",
      "Training and predicting on ../data/asap/datasets/rnd_splits/split_1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type               | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | message_passing | BondMessagePassing | 2.2 M  | train\n",
      "1 | agg             | MeanAggregation    | 0      | train\n",
      "2 | bn              | BatchNorm1d        | 2.0 K  | train\n",
      "3 | predictor       | RegressionFFN      | 503 K  | train\n",
      "4 | X_d_transform   | Identity           | 0      | train\n",
      "5 | metrics         | ModuleList         | 0      | train\n",
      "---------------------------------------------------------------\n",
      "2.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.7 M     Total params\n",
      "10.656    Total estimated model params size (MB)\n",
      "25        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/core/saving.py:363: Skipping 'metrics' parameter because it is not possible to safely dump to YAML.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.2597171204692286,\n",
      "    \"r2\": 0.7659776510220662\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.14035346872335325,\n",
      "    \"r2\": 0.7707043996857273\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.22020261260545207,\n",
      "    \"r2\": 0.7822821667770143\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.19832394134362274,\n",
      "    \"r2\": 0.7932069513464481\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.3714193066576171,\n",
      "    \"r2\": 0.8537496324497851\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.23800328995985476,\n",
      "    \"macro_r2\": 0.7931841602562082\n",
      "  }\n",
      "}\n",
      "\n",
      "Val metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.40462325433754476,\n",
      "    \"r2\": 0.35663543802280806\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.1924591020089494,\n",
      "    \"r2\": 0.4205129316470587\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.38706234270826023,\n",
      "    \"r2\": 0.4200991579122768\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.35286920369671865,\n",
      "    \"r2\": 0.4250274157433831\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.5405113132072218,\n",
      "    \"r2\": 0.6212656217173432\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.37550504319173894,\n",
      "    \"macro_r2\": 0.44870811300857405\n",
      "  }\n",
      "}\n",
      "Training and predicting on ../data/asap/datasets/rnd_splits/split_2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type               | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | message_passing | BondMessagePassing | 2.2 M  | train\n",
      "1 | agg             | MeanAggregation    | 0      | train\n",
      "2 | bn              | BatchNorm1d        | 2.0 K  | train\n",
      "3 | predictor       | RegressionFFN      | 503 K  | train\n",
      "4 | X_d_transform   | Identity           | 0      | train\n",
      "5 | metrics         | ModuleList         | 0      | train\n",
      "---------------------------------------------------------------\n",
      "2.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.7 M     Total params\n",
      "10.656    Total estimated model params size (MB)\n",
      "25        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/core/saving.py:363: Skipping 'metrics' parameter because it is not possible to safely dump to YAML.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.22239809234134447,\n",
      "    \"r2\": 0.8010811162812254\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.14508306489088602,\n",
      "    \"r2\": 0.7616717931794632\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.18794078301477876,\n",
      "    \"r2\": 0.8612627619426025\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.19922002038038833,\n",
      "    \"r2\": 0.7940652741932519\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.25605372717515346,\n",
      "    \"r2\": 0.9239201983702174\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.20213913756051022,\n",
      "    \"macro_r2\": 0.8284002287933522\n",
      "  }\n",
      "}\n",
      "\n",
      "Val metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.41188724961656714,\n",
      "    \"r2\": 0.410622246540841\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.1933379131714535,\n",
      "    \"r2\": 0.531979215463223\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.4099410541752838,\n",
      "    \"r2\": 0.19285531565742398\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.36553200509293693,\n",
      "    \"r2\": 0.21284057376483267\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.5389142184998049,\n",
      "    \"r2\": 0.7215080286253273\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.3839224881112092,\n",
      "    \"macro_r2\": 0.4139610760103296\n",
      "  }\n",
      "}\n",
      "Training and predicting on ../data/asap/datasets/rnd_splits/split_3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type               | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | message_passing | BondMessagePassing | 2.2 M  | train\n",
      "1 | agg             | MeanAggregation    | 0      | train\n",
      "2 | bn              | BatchNorm1d        | 2.0 K  | train\n",
      "3 | predictor       | RegressionFFN      | 503 K  | train\n",
      "4 | X_d_transform   | Identity           | 0      | train\n",
      "5 | metrics         | ModuleList         | 0      | train\n",
      "---------------------------------------------------------------\n",
      "2.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.7 M     Total params\n",
      "10.656    Total estimated model params size (MB)\n",
      "25        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/core/saving.py:363: Skipping 'metrics' parameter because it is not possible to safely dump to YAML.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.19886306573587004,\n",
      "    \"r2\": 0.8430739581497567\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.14166601172282145,\n",
      "    \"r2\": 0.7832225861029515\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.21002362077586564,\n",
      "    \"r2\": 0.8217191246310506\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.19743713744578043,\n",
      "    \"r2\": 0.7895211403727999\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.30480210972927774,\n",
      "    \"r2\": 0.9055993503574271\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.21055838908192306,\n",
      "    \"macro_r2\": 0.8286272319227972\n",
      "  }\n",
      "}\n",
      "\n",
      "Val metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.28261446816765284,\n",
      "    \"r2\": 0.611528765537125\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.21922521733119443,\n",
      "    \"r2\": 0.3579301102233834\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.35622374330322265,\n",
      "    \"r2\": 0.3717045156913601\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.33394541963498114,\n",
      "    \"r2\": 0.41302280465186425\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.47401719380112795,\n",
      "    \"r2\": 0.7236394019569035\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.3332052084476358,\n",
      "    \"macro_r2\": 0.49556511961212724\n",
      "  }\n",
      "}\n",
      "Training and predicting on ../data/asap/datasets/rnd_splits/split_4.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type               | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | message_passing | BondMessagePassing | 2.2 M  | train\n",
      "1 | agg             | MeanAggregation    | 0      | train\n",
      "2 | bn              | BatchNorm1d        | 2.0 K  | train\n",
      "3 | predictor       | RegressionFFN      | 503 K  | train\n",
      "4 | X_d_transform   | Identity           | 0      | train\n",
      "5 | metrics         | ModuleList         | 0      | train\n",
      "---------------------------------------------------------------\n",
      "2.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.7 M     Total params\n",
      "10.656    Total estimated model params size (MB)\n",
      "25        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/core/saving.py:363: Skipping 'metrics' parameter because it is not possible to safely dump to YAML.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.2025360460620371,\n",
      "    \"r2\": 0.8368626139038552\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.12681379350238434,\n",
      "    \"r2\": 0.8156879900339248\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.197558901058282,\n",
      "    \"r2\": 0.8494635447756685\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.19031125405008548,\n",
      "    \"r2\": 0.8263062312788162\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.2234576534993889,\n",
      "    \"r2\": 0.9507129151952978\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.18813552963443553,\n",
      "    \"macro_r2\": 0.8558066590375125\n",
      "  }\n",
      "}\n",
      "\n",
      "Val metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.3533265182085088,\n",
      "    \"r2\": 0.35187100498644586\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.1914119530840047,\n",
      "    \"r2\": 0.5412721876625946\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.36125022152485914,\n",
      "    \"r2\": 0.37399337255815346\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.30024156225054494,\n",
      "    \"r2\": 0.37048478261976137\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.4732300555010637,\n",
      "    \"r2\": 0.6614379500709902\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.3358920621137963,\n",
      "    \"macro_r2\": 0.45981185957958903\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>split_0-epoch</td><td>▁▁▂▂▂▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▄▄▄▅▅▅▆▆▆▆▆▆▇▇▇████</td></tr><tr><td>split_0-train_loss_epoch</td><td>█▄▃▂▂▂▁▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_0-train_loss_step</td><td>▇█▅▅▄▁▄▂█▂▂▂▁▂▂▁▁▂▁▁▁▁▁▃</td></tr><tr><td>split_0-val/mae</td><td>▂▄█▃▃▆▄▁▂▂▂▁▂▂▅▁▂▂▁▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_0-val/r2</td><td>▇▆▆▁▇███████████████████████████████████</td></tr><tr><td>split_0-val_loss</td><td>▆▄▅█▄▆▄█▂▄▂▁▄▃▂▁▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_1-epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇████</td></tr><tr><td>split_1-train_loss_epoch</td><td>▇█▆▆▇▅▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_1-train_loss_step</td><td>▇▆█▅▃▃▃▃▆▂▂▁▂▂▂▂▂█▂▂▄▁▁▅</td></tr><tr><td>split_1-val/mae</td><td>▂▂▄▅▆█▆▃▃▄▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_1-val/r2</td><td>▇▆▅▃▁█▇███▇█████████████████████████████</td></tr><tr><td>split_1-val_loss</td><td>▃██▂▃▄▂▁▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_2-epoch</td><td>▁▁▂▂▂▂▂▂▂▃▃▄▄▄▄▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇███</td></tr><tr><td>split_2-train_loss_epoch</td><td>█▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_2-train_loss_step</td><td>█▅▆▄▄▅▃▃▃▂▂▅▂▂▂▁▁▅▁▁▁▁▁▂</td></tr><tr><td>split_2-val/mae</td><td>▃▄█▆▂▆▂▃▁▂▂▁▂▂▂▂▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_2-val/r2</td><td>▇▆▆▇▆▁▃▇█▆██████████████████████████████</td></tr><tr><td>split_2-val_loss</td><td>▂█▂▂▂▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_3-epoch</td><td>▁▁▂▂▂▂▂▂▂▂▃▃▃▃▃▃▃▃▃▄▄▅▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇██</td></tr><tr><td>split_3-train_loss_epoch</td><td>█▄▄▃▃▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_3-train_loss_step</td><td>█▇▁▄▆█▅▃▄▂▂▆▂▂▄▂▁▁▁▂▄▂▁▃</td></tr><tr><td>split_3-val/mae</td><td>█▅▅▆▃▅▃▂▄▂▂▂▃▂▂▂▂▃▂▁▁▁▁▁▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_3-val/r2</td><td>▅▆▆▃▁▇▅▆▄▆▆▄▆▇▇▇▇▇▇▇▇█▇▇████████████████</td></tr><tr><td>split_3-val_loss</td><td>█▅▄▃▇▃▄▃▄█▆▃▂▂▂▂▃▄▂▁▁▂▁▁▁▁▂▁▁▁▁▂▁▁▁▁▁▁▁▁</td></tr><tr><td>split_4-epoch</td><td>▁▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇██</td></tr><tr><td>split_4-train_loss_epoch</td><td>█▇▅▆▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_4-train_loss_step</td><td>▇▄█▃▃▂▂▂▁▂▂▂▂▂▂▁▂▆▁▁▂▁▁▂</td></tr><tr><td>split_4-val/mae</td><td>▃█▄▃▄▂▂▄▂▁▂▁▂▂▁▂▁▁▁▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_4-val/r2</td><td>▇▇▅▆▁▇▇▅██▆█████████████████████████████</td></tr><tr><td>split_4-val_loss</td><td>▄▃▃█▃▁▁▁▁▂▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▂▂▃▄▅▆▆▆██▁▁▁▁▃▄▄▅▅▅▆▆▆▁▅▆▇█▂▃▄▄▄▇▇█▂▂▂▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>split_0-epoch</td><td>199</td></tr><tr><td>split_0-train_loss_epoch</td><td>0.10783</td></tr><tr><td>split_0-train_loss_step</td><td>0.26311</td></tr><tr><td>split_0-val/mae</td><td>0.35689</td></tr><tr><td>split_0-val/r2</td><td>0.75855</td></tr><tr><td>split_0-val_loss</td><td>0.22458</td></tr><tr><td>split_1-epoch</td><td>199</td></tr><tr><td>split_1-train_loss_epoch</td><td>0.12155</td></tr><tr><td>split_1-train_loss_step</td><td>0.38707</td></tr><tr><td>split_1-val/mae</td><td>0.36949</td></tr><tr><td>split_1-val/r2</td><td>0.70497</td></tr><tr><td>split_1-val_loss</td><td>0.28008</td></tr><tr><td>split_2-epoch</td><td>199</td></tr><tr><td>split_2-train_loss_epoch</td><td>0.10074</td></tr><tr><td>split_2-train_loss_step</td><td>0.16949</td></tr><tr><td>split_2-val/mae</td><td>0.37653</td></tr><tr><td>split_2-val/r2</td><td>0.72933</td></tr><tr><td>split_2-val_loss</td><td>0.27507</td></tr><tr><td>split_3-epoch</td><td>199</td></tr><tr><td>split_3-train_loss_epoch</td><td>0.11351</td></tr><tr><td>split_3-train_loss_step</td><td>0.23127</td></tr><tr><td>split_3-val/mae</td><td>0.32607</td></tr><tr><td>split_3-val/r2</td><td>0.77909</td></tr><tr><td>split_3-val_loss</td><td>0.20608</td></tr><tr><td>split_4-epoch</td><td>199</td></tr><tr><td>split_4-train_loss_epoch</td><td>0.09088</td></tr><tr><td>split_4-train_loss_step</td><td>0.18125</td></tr><tr><td>split_4-val/mae</td><td>0.32924</td></tr><tr><td>split_4-val/r2</td><td>0.73087</td></tr><tr><td>split_4-val_loss</td><td>0.24764</td></tr><tr><td>trainer/global_step</td><td>1199</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">chemprop_run_0</strong> at: <a href='https://wandb.ai/vladvin-org/admet-challenge/runs/122z40s0' target=\"_blank\">https://wandb.ai/vladvin-org/admet-challenge/runs/122z40s0</a><br> View project at: <a href='https://wandb.ai/vladvin-org/admet-challenge' target=\"_blank\">https://wandb.ai/vladvin-org/admet-challenge</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>../wandb/chemprop_run_0/wandb/run-20250311_210018-122z40s0/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_and_eval(input_paths, save_dirs, RUN_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up + run 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(input_paths, save_dirs, output_dir, remove_worst_pct):\n",
    "    smiles_to_remove = defaultdict(set)\n",
    "\n",
    "    for input_path, save_dir in zip(input_paths, save_dirs):\n",
    "        input_df = pd.read_csv(input_path)\n",
    "        input_val_df = input_df[input_df[\"split\"] == \"val\"]\n",
    "        output_df = pd.read_csv(save_dir / \"predictions.csv\")\n",
    "        output_val_df = output_df[input_df[\"split\"] == \"val\"]\n",
    "\n",
    "        for t in TARGET_COLUMNS:\n",
    "            # Sort by absolute error\n",
    "            notna_mask = input_val_df[t].notna()\n",
    "            input_val_df = input_val_df[notna_mask]\n",
    "            output_val_df = output_val_df[notna_mask]\n",
    "\n",
    "            mae = np.abs(input_val_df[t] - output_val_df[f\"pred_{t}\"])\n",
    "            sorted_idx = np.argsort(mae)[::-1]\n",
    "            smiles_to_remove[t].update(\n",
    "                input_val_df.iloc[sorted_idx[:int(remove_worst_pct * len(sorted_idx))]][\"cxsmiles_std\"].tolist()\n",
    "            )\n",
    "\n",
    "    for input_path in input_paths:\n",
    "        input_df = pd.read_csv(input_path)\n",
    "        for t in TARGET_COLUMNS:\n",
    "            input_df.loc[input_df[\"cxsmiles_std\"].isin(smiles_to_remove[t]) & (input_df[\"split\"] == \"train\"), t] = np.nan\n",
    "\n",
    "        input_df.to_csv(output_dir / input_path.name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path(\"../output/asap/rnd_splits/chemprop/run_0/cleaned\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data(input_paths, save_dirs, output_dir, remove_worst_pct = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogHLM            150\n",
       "LogMLM            140\n",
       "LogD               86\n",
       "LogKSOL            74\n",
       "LogMDR1-MDCKII     15\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(input_paths[0])[TARGET_COLUMNS].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogHLM            189\n",
       "LogMLM            173\n",
       "LogD              116\n",
       "LogKSOL            99\n",
       "LogMDR1-MDCKII     40\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(output_dir / input_paths[0].name)[TARGET_COLUMNS].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_paths = [Path(f'../output/asap/rnd_splits/chemprop/run_0/cleaned/split_{k}.csv') for k in range(5)]\n",
    "save_dirs = [Path(f'../output/asap/rnd_splits/chemprop/run_1/split_{k}') for k in range(5)]\n",
    "RUN_IDX = \"1_clean_worst_pct_0.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predicting on ../output/asap/rnd_splits/chemprop/run_0/cleaned/split_0.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>../wandb/chemprop_run_1_clean_worst_pct_0.2/wandb/run-20250311_211011-8qgjizgi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vladvin-org/admet-challenge/runs/8qgjizgi' target=\"_blank\">chemprop_run_1_clean_worst_pct_0.2</a></strong> to <a href='https://wandb.ai/vladvin-org/admet-challenge' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/vladvin-org/admet-challenge' target=\"_blank\">https://wandb.ai/vladvin-org/admet-challenge</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/vladvin-org/admet-challenge/runs/8qgjizgi' target=\"_blank\">https://wandb.ai/vladvin-org/admet-challenge/runs/8qgjizgi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type               | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | message_passing | BondMessagePassing | 2.2 M  | train\n",
      "1 | agg             | MeanAggregation    | 0      | train\n",
      "2 | bn              | BatchNorm1d        | 2.0 K  | train\n",
      "3 | predictor       | RegressionFFN      | 503 K  | train\n",
      "4 | X_d_transform   | Identity           | 0      | train\n",
      "5 | metrics         | ModuleList         | 0      | train\n",
      "---------------------------------------------------------------\n",
      "2.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.7 M     Total params\n",
      "10.656    Total estimated model params size (MB)\n",
      "25        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/core/saving.py:363: Skipping 'metrics' parameter because it is not possible to safely dump to YAML.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.24015611684701654,\n",
      "    \"r2\": 0.7277680487621945\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.1534144232931043,\n",
      "    \"r2\": 0.7221598660470849\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.2871379576097833,\n",
      "    \"r2\": 0.5646091415023183\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.2782443940385389,\n",
      "    \"r2\": 0.5469261018652077\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.2991523738566474,\n",
      "    \"r2\": 0.8950816081063567\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.25162105312901806,\n",
      "    \"macro_r2\": 0.6913089532566324\n",
      "  }\n",
      "}\n",
      "\n",
      "Val metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.367016727560697,\n",
      "    \"r2\": 0.4084333295557705\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.2208954740841836,\n",
      "    \"r2\": 0.4583291930233465\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.32447410513977104,\n",
      "    \"r2\": 0.5392675434857483\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.31839985537969734,\n",
      "    \"r2\": 0.3832657549417898\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.5154423531108214,\n",
      "    \"r2\": 0.7343562247928184\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.3492457030550341,\n",
      "    \"macro_r2\": 0.5047304091598948\n",
      "  }\n",
      "}\n",
      "Training and predicting on ../output/asap/rnd_splits/chemprop/run_0/cleaned/split_1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type               | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | message_passing | BondMessagePassing | 2.2 M  | train\n",
      "1 | agg             | MeanAggregation    | 0      | train\n",
      "2 | bn              | BatchNorm1d        | 2.0 K  | train\n",
      "3 | predictor       | RegressionFFN      | 503 K  | train\n",
      "4 | X_d_transform   | Identity           | 0      | train\n",
      "5 | metrics         | ModuleList         | 0      | train\n",
      "---------------------------------------------------------------\n",
      "2.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.7 M     Total params\n",
      "10.656    Total estimated model params size (MB)\n",
      "25        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/core/saving.py:363: Skipping 'metrics' parameter because it is not possible to safely dump to YAML.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.239763939454739,\n",
      "    \"r2\": 0.718956939489979\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.16254419235724782,\n",
      "    \"r2\": 0.6610993841336104\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.27290543295096886,\n",
      "    \"r2\": 0.5547813315166519\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.25975063128350007,\n",
      "    \"r2\": 0.5585812406368915\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.2956405307656272,\n",
      "    \"r2\": 0.9048673559478665\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.2461209453624166,\n",
      "    \"macro_r2\": 0.6796572503449998\n",
      "  }\n",
      "}\n",
      "\n",
      "Val metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.3360772654228352,\n",
      "    \"r2\": 0.46826859316157676\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.21372770360833493,\n",
      "    \"r2\": 0.4977918329815898\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.3578487288641999,\n",
      "    \"r2\": 0.5228502619881628\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.33441213505979733,\n",
      "    \"r2\": 0.49423291118806245\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.5317780734621214,\n",
      "    \"r2\": 0.6939659436645726\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.35476878128345773,\n",
      "    \"macro_r2\": 0.535421908596793\n",
      "  }\n",
      "}\n",
      "Training and predicting on ../output/asap/rnd_splits/chemprop/run_0/cleaned/split_2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type               | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | message_passing | BondMessagePassing | 2.2 M  | train\n",
      "1 | agg             | MeanAggregation    | 0      | train\n",
      "2 | bn              | BatchNorm1d        | 2.0 K  | train\n",
      "3 | predictor       | RegressionFFN      | 503 K  | train\n",
      "4 | X_d_transform   | Identity           | 0      | train\n",
      "5 | metrics         | ModuleList         | 0      | train\n",
      "---------------------------------------------------------------\n",
      "2.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.7 M     Total params\n",
      "10.656    Total estimated model params size (MB)\n",
      "25        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/core/saving.py:363: Skipping 'metrics' parameter because it is not possible to safely dump to YAML.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.23936657175396683,\n",
      "    \"r2\": 0.7357898923761756\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.17470872390403855,\n",
      "    \"r2\": 0.6336310064210433\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.2741909687200981,\n",
      "    \"r2\": 0.6294886394326302\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.266847701113405,\n",
      "    \"r2\": 0.52591974747608\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.2257889774082495,\n",
      "    \"r2\": 0.9405456052427525\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.23618058857995158,\n",
      "    \"macro_r2\": 0.6930749781897363\n",
      "  }\n",
      "}\n",
      "\n",
      "Val metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.3985827886183202,\n",
      "    \"r2\": 0.35381222087719144\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.19643224773741708,\n",
      "    \"r2\": 0.5692178072998397\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.38419072464981757,\n",
      "    \"r2\": 0.2390758313166078\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.35399984197532197,\n",
      "    \"r2\": 0.3213320053374542\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.5820390567409268,\n",
      "    \"r2\": 0.6882290003411893\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.38304893194436074,\n",
      "    \"macro_r2\": 0.43433337303445646\n",
      "  }\n",
      "}\n",
      "Training and predicting on ../output/asap/rnd_splits/chemprop/run_0/cleaned/split_3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type               | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | message_passing | BondMessagePassing | 2.2 M  | train\n",
      "1 | agg             | MeanAggregation    | 0      | train\n",
      "2 | bn              | BatchNorm1d        | 2.0 K  | train\n",
      "3 | predictor       | RegressionFFN      | 503 K  | train\n",
      "4 | X_d_transform   | Identity           | 0      | train\n",
      "5 | metrics         | ModuleList         | 0      | train\n",
      "---------------------------------------------------------------\n",
      "2.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.7 M     Total params\n",
      "10.656    Total estimated model params size (MB)\n",
      "25        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/core/saving.py:363: Skipping 'metrics' parameter because it is not possible to safely dump to YAML.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.23873198642278523,\n",
      "    \"r2\": 0.7220947085170473\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.1681929594764098,\n",
      "    \"r2\": 0.6701224029138255\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.2875885808570273,\n",
      "    \"r2\": 0.5621061489877364\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.2545136414043143,\n",
      "    \"r2\": 0.5607239295233242\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.2791234872453416,\n",
      "    \"r2\": 0.9195677156953541\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.24563013108117565,\n",
      "    \"macro_r2\": 0.6869229811274575\n",
      "  }\n",
      "}\n",
      "\n",
      "Val metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.3466090227340552,\n",
      "    \"r2\": 0.38885276933383683\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.22923648777433067,\n",
      "    \"r2\": 0.2642125136987167\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.32128864770514737,\n",
      "    \"r2\": 0.479287750573418\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.32152775040593756,\n",
      "    \"r2\": 0.4646923445418015\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.40447581271941846,\n",
      "    \"r2\": 0.7831821808624198\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.32462754426777785,\n",
      "    \"macro_r2\": 0.4760455118020386\n",
      "  }\n",
      "}\n",
      "Training and predicting on ../output/asap/rnd_splits/chemprop/run_0/cleaned/split_4.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type               | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | message_passing | BondMessagePassing | 2.2 M  | train\n",
      "1 | agg             | MeanAggregation    | 0      | train\n",
      "2 | bn              | BatchNorm1d        | 2.0 K  | train\n",
      "3 | predictor       | RegressionFFN      | 503 K  | train\n",
      "4 | X_d_transform   | Identity           | 0      | train\n",
      "5 | metrics         | ModuleList         | 0      | train\n",
      "---------------------------------------------------------------\n",
      "2.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.7 M     Total params\n",
      "10.656    Total estimated model params size (MB)\n",
      "25        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/core/saving.py:363: Skipping 'metrics' parameter because it is not possible to safely dump to YAML.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.2049501458182054,\n",
      "    \"r2\": 0.7585603046925538\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.13932446158954812,\n",
      "    \"r2\": 0.7315145823252982\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.2791512257430599,\n",
      "    \"r2\": 0.5498500714616518\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.2791168777241887,\n",
      "    \"r2\": 0.5456720301206758\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.28398838515082997,\n",
      "    \"r2\": 0.9196906461972821\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.2373062192051664,\n",
      "    \"macro_r2\": 0.7010575269594923\n",
      "  }\n",
      "}\n",
      "\n",
      "Val metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.29527388987570835,\n",
      "    \"r2\": 0.3205603413159307\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.1933578214250906,\n",
      "    \"r2\": 0.5561999529093957\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.330339871319223,\n",
      "    \"r2\": 0.5875092958078285\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.2689108963354894,\n",
      "    \"r2\": 0.5294454536041988\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.5409009268681209,\n",
      "    \"r2\": 0.6495593955172009\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.32575668116472645,\n",
      "    \"macro_r2\": 0.528654887830911\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>split_0-epoch</td><td>▁▁▁▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>split_0-train_loss_epoch</td><td>█▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_0-train_loss_step</td><td>▇▃▄▃▃▃▂▃▁▂▂▅▂▂█▁▁▂▁▁▃▁▁▆</td></tr><tr><td>split_0-val/mae</td><td>▄█▇▄▆▂▃▄▂▂▂▂▂▁▁▂▁▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_0-val/r2</td><td>▁▄▂▆▇▇▇█▇▆███▇██████████████████████████</td></tr><tr><td>split_0-val_loss</td><td>▃▆█▃▃▃▆▁▄▂▃▂▂▂▂▄▃▂▂▂▂▁▁▃▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_1-epoch</td><td>▁▁▁▁▁▂▂▂▂▂▂▂▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>split_1-train_loss_epoch</td><td>█▆▇▆▆▅▅▅▅▄▃▃▃▃▃▃▃▂▂▂▂▂▃▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_1-train_loss_step</td><td>▆▄▃▄▄▄▃▂▆▂▂▁▂▂█▂▂▂▁▁▃▁▁▂</td></tr><tr><td>split_1-val/mae</td><td>▄▇▅█▅▃▅▂▃▂▅▃▂▂▂▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_1-val/r2</td><td>▇█▁▆▅▇██▇██▇████████████████████████████</td></tr><tr><td>split_1-val_loss</td><td>▃▄▂▇▃▂█▇▄▂▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_2-epoch</td><td>▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇████</td></tr><tr><td>split_2-train_loss_epoch</td><td>█▆▅▅▆▄▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_2-train_loss_step</td><td>▆█▅▃▃▃▃▃▂▂▂▅▂▂▁▁▂▁▂▁▁▁▁▁</td></tr><tr><td>split_2-val/mae</td><td>▃▄█▃▂▂▂▂▂▂▁▁▄▂▂▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_2-val/r2</td><td>▆▇▆▁████████████████████████████████████</td></tr><tr><td>split_2-val_loss</td><td>▃▃▂█▆▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_3-epoch</td><td>▁▁▂▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇██</td></tr><tr><td>split_3-train_loss_epoch</td><td>█▇▇▄▄▃▄▆▃▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_3-train_loss_step</td><td>▅▄▁▂▄█▂▂▃▁▁▁▁▁▃▁▁▃▁▂▁▁▁▃</td></tr><tr><td>split_3-val/mae</td><td>▂▄▃▃▃▅█▃▂▃▁▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_3-val/r2</td><td>█▅▆▆▁███▇▇▇▇████████████████████████████</td></tr><tr><td>split_3-val_loss</td><td>▆▄█▄▄▂▂▃▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_4-epoch</td><td>▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>split_4-train_loss_epoch</td><td>█▇▅▅▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_4-train_loss_step</td><td>█▅▄▃▃▁▂▃▂▂▂▇▂▁▄▁▁▂▁▁▅▁▁▂</td></tr><tr><td>split_4-val/mae</td><td>▆▆▅▃▂▅▂▂▂▂▂█▃▁▂▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_4-val/r2</td><td>▁▆▄█▇▇▆███▇█████████████████████████████</td></tr><tr><td>split_4-val_loss</td><td>▁▁▂▂█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▃▄▄▅▅▆▆▁▃▄▆▆▆▆▆▇▇▃▇▇▁▅▅▆▆▇███▂▄▄▄▆█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>split_0-epoch</td><td>199</td></tr><tr><td>split_0-train_loss_epoch</td><td>0.11152</td></tr><tr><td>split_0-train_loss_step</td><td>0.46332</td></tr><tr><td>split_0-val/mae</td><td>0.34401</td></tr><tr><td>split_0-val/r2</td><td>0.75845</td></tr><tr><td>split_0-val_loss</td><td>0.22467</td></tr><tr><td>split_1-epoch</td><td>199</td></tr><tr><td>split_1-train_loss_epoch</td><td>0.09389</td></tr><tr><td>split_1-train_loss_step</td><td>0.13279</td></tr><tr><td>split_1-val/mae</td><td>0.35022</td></tr><tr><td>split_1-val/r2</td><td>0.75603</td></tr><tr><td>split_1-val_loss</td><td>0.23161</td></tr><tr><td>split_2-epoch</td><td>199</td></tr><tr><td>split_2-train_loss_epoch</td><td>0.08405</td></tr><tr><td>split_2-train_loss_step</td><td>0.09638</td></tr><tr><td>split_2-val/mae</td><td>0.37634</td></tr><tr><td>split_2-val/r2</td><td>0.72545</td></tr><tr><td>split_2-val_loss</td><td>0.27901</td></tr><tr><td>split_3-epoch</td><td>199</td></tr><tr><td>split_3-train_loss_epoch</td><td>0.09435</td></tr><tr><td>split_3-train_loss_step</td><td>0.37256</td></tr><tr><td>split_3-val/mae</td><td>0.32104</td></tr><tr><td>split_3-val/r2</td><td>0.78634</td></tr><tr><td>split_3-val_loss</td><td>0.19932</td></tr><tr><td>split_4-epoch</td><td>199</td></tr><tr><td>split_4-train_loss_epoch</td><td>0.08087</td></tr><tr><td>split_4-train_loss_step</td><td>0.1405</td></tr><tr><td>split_4-val/mae</td><td>0.32112</td></tr><tr><td>split_4-val/r2</td><td>0.74613</td></tr><tr><td>split_4-val_loss</td><td>0.23359</td></tr><tr><td>trainer/global_step</td><td>1199</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">chemprop_run_1_clean_worst_pct_0.2</strong> at: <a href='https://wandb.ai/vladvin-org/admet-challenge/runs/8qgjizgi' target=\"_blank\">https://wandb.ai/vladvin-org/admet-challenge/runs/8qgjizgi</a><br> View project at: <a href='https://wandb.ai/vladvin-org/admet-challenge' target=\"_blank\">https://wandb.ai/vladvin-org/admet-challenge</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>../wandb/chemprop_run_1_clean_worst_pct_0.2/wandb/run-20250311_211011-8qgjizgi/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_and_eval(input_paths, save_dirs, RUN_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up + run 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path(\"../output/asap/rnd_splits/chemprop/run_1/cleaned\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data(input_paths, save_dirs, output_dir, remove_worst_pct = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_paths = [Path(f'../output/asap/rnd_splits/chemprop/run_1/cleaned/split_{k}.csv') for k in range(5)]\n",
    "save_dirs = [Path(f'../output/asap/rnd_splits/chemprop/run_2/split_{k}') for k in range(5)]\n",
    "RUN_IDX = \"2_clean_worst_pct_0.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predicting on ../output/asap/rnd_splits/chemprop/run_1/cleaned/split_0.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>../wandb/chemprop_run_2_clean_worst_pct_0.2/wandb/run-20250311_211949-ev1v18db</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vladvin-org/admet-challenge/runs/ev1v18db' target=\"_blank\">chemprop_run_2_clean_worst_pct_0.2</a></strong> to <a href='https://wandb.ai/vladvin-org/admet-challenge' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/vladvin-org/admet-challenge' target=\"_blank\">https://wandb.ai/vladvin-org/admet-challenge</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/vladvin-org/admet-challenge/runs/ev1v18db' target=\"_blank\">https://wandb.ai/vladvin-org/admet-challenge/runs/ev1v18db</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type               | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | message_passing | BondMessagePassing | 2.2 M  | train\n",
      "1 | agg             | MeanAggregation    | 0      | train\n",
      "2 | bn              | BatchNorm1d        | 2.0 K  | train\n",
      "3 | predictor       | RegressionFFN      | 503 K  | train\n",
      "4 | X_d_transform   | Identity           | 0      | train\n",
      "5 | metrics         | ModuleList         | 0      | train\n",
      "---------------------------------------------------------------\n",
      "2.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.7 M     Total params\n",
      "10.656    Total estimated model params size (MB)\n",
      "25        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/core/saving.py:363: Skipping 'metrics' parameter because it is not possible to safely dump to YAML.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.24346469708077964,\n",
      "    \"r2\": 0.7131768486305325\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.15842682701434382,\n",
      "    \"r2\": 0.6773955567462315\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.3058049375590547,\n",
      "    \"r2\": 0.46527251829799354\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.267993828477452,\n",
      "    \"r2\": 0.569569199686726\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.3013011177852054,\n",
      "    \"r2\": 0.8886175239581221\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.25539828158336714,\n",
      "    \"macro_r2\": 0.6628063294639212\n",
      "  }\n",
      "}\n",
      "\n",
      "Val metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.4006456045916631,\n",
      "    \"r2\": 0.3450002614749901\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.2434898010047162,\n",
      "    \"r2\": 0.34817773525262485\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.33359431687717434,\n",
      "    \"r2\": 0.5150008190102775\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.3319844486754105,\n",
      "    \"r2\": 0.2717838677660136\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.5433577235995747,\n",
      "    \"r2\": 0.6977678865138437\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.3706143789497077,\n",
      "    \"macro_r2\": 0.43554611400355\n",
      "  }\n",
      "}\n",
      "Training and predicting on ../output/asap/rnd_splits/chemprop/run_1/cleaned/split_1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type               | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | message_passing | BondMessagePassing | 2.2 M  | train\n",
      "1 | agg             | MeanAggregation    | 0      | train\n",
      "2 | bn              | BatchNorm1d        | 2.0 K  | train\n",
      "3 | predictor       | RegressionFFN      | 503 K  | train\n",
      "4 | X_d_transform   | Identity           | 0      | train\n",
      "5 | metrics         | ModuleList         | 0      | train\n",
      "---------------------------------------------------------------\n",
      "2.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.7 M     Total params\n",
      "10.656    Total estimated model params size (MB)\n",
      "25        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/core/saving.py:363: Skipping 'metrics' parameter because it is not possible to safely dump to YAML.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.2454548955976915,\n",
      "    \"r2\": 0.6907670629360063\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.16194189071765344,\n",
      "    \"r2\": 0.6588450402124902\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.29570088901290087,\n",
      "    \"r2\": 0.4939481534996508\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.2815622016162154,\n",
      "    \"r2\": 0.46986338244226167\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.28059066100981817,\n",
      "    \"r2\": 0.9071803260675587\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.2530501075908559,\n",
      "    \"macro_r2\": 0.6441207930315935\n",
      "  }\n",
      "}\n",
      "\n",
      "Val metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.35114818942470877,\n",
      "    \"r2\": 0.4864082272532795\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.21829884748399783,\n",
      "    \"r2\": 0.3995759875826008\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.3774881402469697,\n",
      "    \"r2\": 0.5323176519858472\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.30583415203503134,\n",
      "    \"r2\": 0.526827869020662\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.5299111646052562,\n",
      "    \"r2\": 0.6764697557260957\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.35653609875919273,\n",
      "    \"macro_r2\": 0.5243198983136971\n",
      "  }\n",
      "}\n",
      "Training and predicting on ../output/asap/rnd_splits/chemprop/run_1/cleaned/split_2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type               | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | message_passing | BondMessagePassing | 2.2 M  | train\n",
      "1 | agg             | MeanAggregation    | 0      | train\n",
      "2 | bn              | BatchNorm1d        | 2.0 K  | train\n",
      "3 | predictor       | RegressionFFN      | 503 K  | train\n",
      "4 | X_d_transform   | Identity           | 0      | train\n",
      "5 | metrics         | ModuleList         | 0      | train\n",
      "---------------------------------------------------------------\n",
      "2.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.7 M     Total params\n",
      "10.656    Total estimated model params size (MB)\n",
      "25        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/core/saving.py:363: Skipping 'metrics' parameter because it is not possible to safely dump to YAML.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.23118985424088492,\n",
      "    \"r2\": 0.7313856654611756\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.16329314750233623,\n",
      "    \"r2\": 0.6755545910157368\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.2876429816178054,\n",
      "    \"r2\": 0.5933415909727404\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.2680435869908554,\n",
      "    \"r2\": 0.516722104815347\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.22792288968064603,\n",
      "    \"r2\": 0.9342549713844119\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.2356184920065056,\n",
      "    \"macro_r2\": 0.6902517847298825\n",
      "  }\n",
      "}\n",
      "\n",
      "Val metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.39503960558916795,\n",
      "    \"r2\": 0.42926683055567094\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.18596303590285448,\n",
      "    \"r2\": 0.6159194712627734\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.3927535490065106,\n",
      "    \"r2\": 0.18374885901038274\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.34404719537536965,\n",
      "    \"r2\": 0.3951809129436581\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.5514187950076478,\n",
      "    \"r2\": 0.7234030584347525\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.3738444361763101,\n",
      "    \"macro_r2\": 0.46950382644144756\n",
      "  }\n",
      "}\n",
      "Training and predicting on ../output/asap/rnd_splits/chemprop/run_1/cleaned/split_3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type               | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | message_passing | BondMessagePassing | 2.2 M  | train\n",
      "1 | agg             | MeanAggregation    | 0      | train\n",
      "2 | bn              | BatchNorm1d        | 2.0 K  | train\n",
      "3 | predictor       | RegressionFFN      | 503 K  | train\n",
      "4 | X_d_transform   | Identity           | 0      | train\n",
      "5 | metrics         | ModuleList         | 0      | train\n",
      "---------------------------------------------------------------\n",
      "2.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.7 M     Total params\n",
      "10.656    Total estimated model params size (MB)\n",
      "25        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/core/saving.py:363: Skipping 'metrics' parameter because it is not possible to safely dump to YAML.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.24340159706790385,\n",
      "    \"r2\": 0.6756498243255233\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.16227727556321905,\n",
      "    \"r2\": 0.6831073685549913\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.2924313931602176,\n",
      "    \"r2\": 0.5529260657521982\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.27514881356809756,\n",
      "    \"r2\": 0.5293073533901498\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.24593859137107546,\n",
      "    \"r2\": 0.9300150242132186\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.24383953414610268,\n",
      "    \"macro_r2\": 0.6742011272472163\n",
      "  }\n",
      "}\n",
      "\n",
      "Val metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.3269344534760019,\n",
      "    \"r2\": 0.45515596165625916\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.22588045580928393,\n",
      "    \"r2\": 0.2997300518555833\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.36924385144528776,\n",
      "    \"r2\": 0.3560280653733774\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.3691283084644681,\n",
      "    \"r2\": 0.37389176175586847\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.4028308919209701,\n",
      "    \"r2\": 0.7858176044738695\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.33880359222320233,\n",
      "    \"macro_r2\": 0.4541246890229916\n",
      "  }\n",
      "}\n",
      "Training and predicting on ../output/asap/rnd_splits/chemprop/run_1/cleaned/split_4.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type               | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | message_passing | BondMessagePassing | 2.2 M  | train\n",
      "1 | agg             | MeanAggregation    | 0      | train\n",
      "2 | bn              | BatchNorm1d        | 2.0 K  | train\n",
      "3 | predictor       | RegressionFFN      | 503 K  | train\n",
      "4 | X_d_transform   | Identity           | 0      | train\n",
      "5 | metrics         | ModuleList         | 0      | train\n",
      "---------------------------------------------------------------\n",
      "2.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.7 M     Total params\n",
      "10.656    Total estimated model params size (MB)\n",
      "25        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/core/saving.py:363: Skipping 'metrics' parameter because it is not possible to safely dump to YAML.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.2287156700593006,\n",
      "    \"r2\": 0.6923940862089433\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.15664005456949467,\n",
      "    \"r2\": 0.6753168596390544\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.2946000325031204,\n",
      "    \"r2\": 0.4975313640715525\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.2700407740982986,\n",
      "    \"r2\": 0.5452323337784695\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.24246397003427494,\n",
      "    \"r2\": 0.9346670834396402\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.23849210025289785,\n",
      "    \"macro_r2\": 0.6690283454275321\n",
      "  }\n",
      "}\n",
      "\n",
      "Val metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.2965531525227481,\n",
      "    \"r2\": 0.5934587103305118\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.18132861378242054,\n",
      "    \"r2\": 0.6173007017801453\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.32030707243915624,\n",
      "    \"r2\": 0.5523443437524773\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.23201874473591116,\n",
      "    \"r2\": 0.6056849768586514\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.4787126799722513,\n",
      "    \"r2\": 0.6745659445663279\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.30178405269049746,\n",
      "    \"macro_r2\": 0.6086709354576227\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>split_0-epoch</td><td>▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▄▄▄▄▅▅▅▆▆▇▇▇▇▇▇▇▇██</td></tr><tr><td>split_0-train_loss_epoch</td><td>█▇▆▆▄▅▃▃▃▃▂▂▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_0-train_loss_step</td><td>▄▃▇▂▃▅▂▂█▂▂▁▂▁▂▂▁▃▁▁▂▁▁▂</td></tr><tr><td>split_0-val/mae</td><td>█▇▃▃▂▃▄▂▃▄▄▃▂▂▂▄▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_0-val/r2</td><td>▆▃▆▁▄▇▅▇▅▇▅▇▇▇██▇▇█▇████████████████████</td></tr><tr><td>split_0-val_loss</td><td>▄█▇▅▂▂▃▂▃▄▄▁▂▂▆▂▃▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_1-epoch</td><td>▁▁▁▂▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇█████</td></tr><tr><td>split_1-train_loss_epoch</td><td>█▃▄▃▃▃▃▂▂▂▂▂▂▂▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_1-train_loss_step</td><td>▃▃█▂▂▂▂▂▃▂▁▂▁▁▂▁▁▁▁▁▂▁▁▁</td></tr><tr><td>split_1-val/mae</td><td>▂▃▆▃▃█▃▂▂▅▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_1-val/r2</td><td>▅▆▇▄▆▄▁█████▇███████████████████████████</td></tr><tr><td>split_1-val_loss</td><td>▂█▂▃▂▂▂▆▅▃▁▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_2-epoch</td><td>▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇██</td></tr><tr><td>split_2-train_loss_epoch</td><td>█▇▆▇▄▄▃▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_2-train_loss_step</td><td>▇█▆▃▃▂▂▃▄▂▂▁▂▂▃▁▁▄▂▁▅▁▁▂</td></tr><tr><td>split_2-val/mae</td><td>▆▅█▆▄▃▃▃▆▂▃▂▂▂▂▂▃▂▂▂▂▂▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_2-val/r2</td><td>▁█▆▇▇█████▇█████████████████████████████</td></tr><tr><td>split_2-val_loss</td><td>▆▃█▃▇▅▂▂▂▂▂▂▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_3-epoch</td><td>▁▁▂▂▂▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇███</td></tr><tr><td>split_3-train_loss_epoch</td><td>█▆▆▅▆▄▄▅▄▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_3-train_loss_step</td><td>▄▄▂▂▃▇▂▂▂▂▂▂▁▁▁▁▁▂▁▁▂▁▁█</td></tr><tr><td>split_3-val/mae</td><td>▅▄█▇▆▄▄▅▂▃▃▃▂▂▂▂▂▁▁▁▁▁▁▂▁▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_3-val/r2</td><td>▇▁▃▅▇▇▆▅▆▇▇▇▇▇▇███▇█████████████████████</td></tr><tr><td>split_3-val_loss</td><td>██▄▅▃▂▂▂▃▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_4-epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇███</td></tr><tr><td>split_4-train_loss_epoch</td><td>█▆▆▆▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_4-train_loss_step</td><td>▆█▂▃▄▇▂▃▅▂▂▁▂▁▂▂▂▁▁▁▂▁▁▄</td></tr><tr><td>split_4-val/mae</td><td>▃▅▅█▂▁▂▁▂▂▁▂▁▂▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_4-val/r2</td><td>▁███▇█▇█████████████████████████████████</td></tr><tr><td>split_4-val_loss</td><td>▃██▁▂▂▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▃▃▅▇▇▄▄▅▅▆▆▄▆▆▇▇█▄▅▅▆▇▇█▂▂▂▄▄▄▅▆▇▇▇▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>split_0-epoch</td><td>199</td></tr><tr><td>split_0-train_loss_epoch</td><td>0.0836</td></tr><tr><td>split_0-train_loss_step</td><td>0.15101</td></tr><tr><td>split_0-val/mae</td><td>0.3661</td></tr><tr><td>split_0-val/r2</td><td>0.72699</td></tr><tr><td>split_0-val_loss</td><td>0.25393</td></tr><tr><td>split_1-epoch</td><td>199</td></tr><tr><td>split_1-train_loss_epoch</td><td>0.08088</td></tr><tr><td>split_1-train_loss_step</td><td>0.11246</td></tr><tr><td>split_1-val/mae</td><td>0.35316</td></tr><tr><td>split_1-val/r2</td><td>0.75063</td></tr><tr><td>split_1-val_loss</td><td>0.23673</td></tr><tr><td>split_2-epoch</td><td>199</td></tr><tr><td>split_2-train_loss_epoch</td><td>0.09113</td></tr><tr><td>split_2-train_loss_step</td><td>0.15112</td></tr><tr><td>split_2-val/mae</td><td>0.36689</td></tr><tr><td>split_2-val/r2</td><td>0.74525</td></tr><tr><td>split_2-val_loss</td><td>0.25889</td></tr><tr><td>split_3-epoch</td><td>199</td></tr><tr><td>split_3-train_loss_epoch</td><td>0.10729</td></tr><tr><td>split_3-train_loss_step</td><td>1.08624</td></tr><tr><td>split_3-val/mae</td><td>0.3308</td></tr><tr><td>split_3-val/r2</td><td>0.78059</td></tr><tr><td>split_3-val_loss</td><td>0.20468</td></tr><tr><td>split_4-epoch</td><td>199</td></tr><tr><td>split_4-train_loss_epoch</td><td>0.07557</td></tr><tr><td>split_4-train_loss_step</td><td>0.25132</td></tr><tr><td>split_4-val/mae</td><td>0.29798</td></tr><tr><td>split_4-val/r2</td><td>0.78831</td></tr><tr><td>split_4-val_loss</td><td>0.19478</td></tr><tr><td>trainer/global_step</td><td>1199</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">chemprop_run_2_clean_worst_pct_0.2</strong> at: <a href='https://wandb.ai/vladvin-org/admet-challenge/runs/ev1v18db' target=\"_blank\">https://wandb.ai/vladvin-org/admet-challenge/runs/ev1v18db</a><br> View project at: <a href='https://wandb.ai/vladvin-org/admet-challenge' target=\"_blank\">https://wandb.ai/vladvin-org/admet-challenge</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>../wandb/chemprop_run_2_clean_worst_pct_0.2/wandb/run-20250311_211949-ev1v18db/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_and_eval(input_paths, save_dirs, RUN_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up stero impure + run 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(input_paths, save_dirs, output_dir, remove_worst_pct):\n",
    "    smiles_to_remove = defaultdict(set)\n",
    "\n",
    "    for input_path, save_dir in zip(input_paths, save_dirs):\n",
    "        input_df = pd.read_csv(input_path)\n",
    "        input_val_df = input_df[input_df[\"split\"] == \"val\"]\n",
    "        output_df = pd.read_csv(save_dir / \"predictions.csv\")\n",
    "        output_val_df = output_df[input_df[\"split\"] == \"val\"]\n",
    "\n",
    "        for t in TARGET_COLUMNS:\n",
    "            # Sort by absolute error\n",
    "            notna_mask = input_val_df[t].notna()\n",
    "            input_val_df = input_val_df[notna_mask]\n",
    "            output_val_df = output_val_df[notna_mask]\n",
    "\n",
    "            mae = np.abs(input_val_df[t] - output_val_df[f\"pred_{t}\"])\n",
    "            sorted_idx = np.argsort(mae)[::-1]\n",
    "            smiles_to_remove[t].update(\n",
    "                input_val_df.iloc[sorted_idx[:int(remove_worst_pct * len(sorted_idx))]][\"cxsmiles_std\"].tolist()\n",
    "            )\n",
    "\n",
    "    for input_path in input_paths:\n",
    "        input_df = pd.read_csv(input_path)\n",
    "        for t in TARGET_COLUMNS:\n",
    "            input_df.loc[\n",
    "                input_df[\"cxsmiles_std\"].isin(smiles_to_remove[t]) & \\\n",
    "                    ~input_df[\"smiles_ext\"].isna() & \\\n",
    "                    (input_df[\"split\"] == \"train\"),\n",
    "                t\n",
    "            ] = np.nan\n",
    "\n",
    "        input_df.to_csv(output_dir / input_path.name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path(\"../output/asap/rnd_splits/chemprop/run_0/cleaned\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data(input_paths, save_dirs, output_dir, remove_worst_pct = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogHLM            203\n",
       "LogMLM            186\n",
       "LogD              129\n",
       "LogKSOL           107\n",
       "LogMDR1-MDCKII     50\n",
       "dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(input_paths[0])[TARGET_COLUMNS].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogHLM            103\n",
       "LogMLM             85\n",
       "LogD               67\n",
       "LogKSOL            57\n",
       "LogMDR1-MDCKII     32\n",
       "dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = pd.read_csv(input_paths[0])\n",
    "tmp = tmp[tmp[\"smiles_ext\"].isna()]\n",
    "tmp[TARGET_COLUMNS].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogHLM            206\n",
       "LogMLM            187\n",
       "LogD              134\n",
       "LogKSOL           107\n",
       "LogMDR1-MDCKII     52\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(output_dir / input_paths[0].name)[TARGET_COLUMNS].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogHLM            103\n",
       "LogMLM             85\n",
       "LogD               67\n",
       "LogKSOL            57\n",
       "LogMDR1-MDCKII     32\n",
       "dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = pd.read_csv(output_dir / input_paths[0].name)\n",
    "tmp = tmp[tmp[\"smiles_ext\"].isna()]\n",
    "tmp[TARGET_COLUMNS].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_paths = [Path(f'../output/asap/rnd_splits/chemprop/run_0/cleaned/split_{k}.csv') for k in range(5)]\n",
    "save_dirs = [Path(f'../output/asap/rnd_splits/chemprop/run_1/split_{k}') for k in range(5)]\n",
    "RUN_IDX = \"1_clean_worst_pct_0.2_stereo_impure\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predicting on ../output/asap/rnd_splits/chemprop/run_0/cleaned/split_0.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>../wandb/chemprop_run_1_clean_worst_pct_0.2_stereo_impure/wandb/run-20250311_212920-pwmxfqik</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vladvin-org/admet-challenge/runs/pwmxfqik' target=\"_blank\">chemprop_run_1_clean_worst_pct_0.2_stereo_impure</a></strong> to <a href='https://wandb.ai/vladvin-org/admet-challenge' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/vladvin-org/admet-challenge' target=\"_blank\">https://wandb.ai/vladvin-org/admet-challenge</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/vladvin-org/admet-challenge/runs/pwmxfqik' target=\"_blank\">https://wandb.ai/vladvin-org/admet-challenge/runs/pwmxfqik</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type               | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | message_passing | BondMessagePassing | 2.2 M  | train\n",
      "1 | agg             | MeanAggregation    | 0      | train\n",
      "2 | bn              | BatchNorm1d        | 2.0 K  | train\n",
      "3 | predictor       | RegressionFFN      | 503 K  | train\n",
      "4 | X_d_transform   | Identity           | 0      | train\n",
      "5 | metrics         | ModuleList         | 0      | train\n",
      "---------------------------------------------------------------\n",
      "2.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.7 M     Total params\n",
      "10.656    Total estimated model params size (MB)\n",
      "25        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/core/saving.py:363: Skipping 'metrics' parameter because it is not possible to safely dump to YAML.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.21632927197585794,\n",
      "    \"r2\": 0.7294461362807467\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.15007996537792595,\n",
      "    \"r2\": 0.7143767832557743\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.2922816246366181,\n",
      "    \"r2\": 0.512002868460343\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.2826572382615165,\n",
      "    \"r2\": 0.5328559838451612\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.23924801416231686,\n",
      "    \"r2\": 0.9212684263972968\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.23611922288284704,\n",
      "    \"macro_r2\": 0.6819900396478644\n",
      "  }\n",
      "}\n",
      "\n",
      "Val metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.34918324683856716,\n",
      "    \"r2\": 0.40315730186642784\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.22938675015512244,\n",
      "    \"r2\": 0.4006908298752314\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.3491726612623635,\n",
      "    \"r2\": 0.5152997396276542\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.344541862135246,\n",
      "    \"r2\": 0.21959466204864186\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.4468315945687841,\n",
      "    \"r2\": 0.7984909454983287\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.3438232229920167,\n",
      "    \"macro_r2\": 0.46744669578325676\n",
      "  }\n",
      "}\n",
      "Training and predicting on ../output/asap/rnd_splits/chemprop/run_0/cleaned/split_1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type               | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | message_passing | BondMessagePassing | 2.2 M  | train\n",
      "1 | agg             | MeanAggregation    | 0      | train\n",
      "2 | bn              | BatchNorm1d        | 2.0 K  | train\n",
      "3 | predictor       | RegressionFFN      | 503 K  | train\n",
      "4 | X_d_transform   | Identity           | 0      | train\n",
      "5 | metrics         | ModuleList         | 0      | train\n",
      "---------------------------------------------------------------\n",
      "2.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.7 M     Total params\n",
      "10.656    Total estimated model params size (MB)\n",
      "25        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/core/saving.py:363: Skipping 'metrics' parameter because it is not possible to safely dump to YAML.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.2254630497212826,\n",
      "    \"r2\": 0.7137079744943886\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.16145584634618565,\n",
      "    \"r2\": 0.6643658046468527\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.2941516681266022,\n",
      "    \"r2\": 0.5120290491750867\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.2662975315536405,\n",
      "    \"r2\": 0.5180148370129751\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.25007193498385766,\n",
      "    \"r2\": 0.9273841682604402\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.2394880061463137,\n",
      "    \"macro_r2\": 0.6671003667179487\n",
      "  }\n",
      "}\n",
      "\n",
      "Val metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.33741185782485306,\n",
      "    \"r2\": 0.42437636812382307\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.20475108683600546,\n",
      "    \"r2\": 0.45212774608877504\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.3809211992688627,\n",
      "    \"r2\": 0.5004446594165916\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.3232393982649962,\n",
      "    \"r2\": 0.5528326862998493\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.5546582854200494,\n",
      "    \"r2\": 0.6386666684474069\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.3601963655229533,\n",
      "    \"macro_r2\": 0.5136896256752892\n",
      "  }\n",
      "}\n",
      "Training and predicting on ../output/asap/rnd_splits/chemprop/run_0/cleaned/split_2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type               | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | message_passing | BondMessagePassing | 2.2 M  | train\n",
      "1 | agg             | MeanAggregation    | 0      | train\n",
      "2 | bn              | BatchNorm1d        | 2.0 K  | train\n",
      "3 | predictor       | RegressionFFN      | 503 K  | train\n",
      "4 | X_d_transform   | Identity           | 0      | train\n",
      "5 | metrics         | ModuleList         | 0      | train\n",
      "---------------------------------------------------------------\n",
      "2.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.7 M     Total params\n",
      "10.656    Total estimated model params size (MB)\n",
      "25        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/core/saving.py:363: Skipping 'metrics' parameter because it is not possible to safely dump to YAML.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.23700609736264228,\n",
      "    \"r2\": 0.7093467328806708\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.1667431391334219,\n",
      "    \"r2\": 0.6256158638489431\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.30877770531057763,\n",
      "    \"r2\": 0.5562916001824039\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.2971938161616641,\n",
      "    \"r2\": 0.458058791228806\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.2623995648504456,\n",
      "    \"r2\": 0.9133256811781736\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.2544240645637503,\n",
      "    \"macro_r2\": 0.6525277338637995\n",
      "  }\n",
      "}\n",
      "\n",
      "Val metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.40301672406482647,\n",
      "    \"r2\": 0.393031262101718\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.19122622948686493,\n",
      "    \"r2\": 0.5621169252746985\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.4019877973703594,\n",
      "    \"r2\": 0.2552873850044083\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.3537648122347687,\n",
      "    \"r2\": 0.3892462230744399\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.5076524715419068,\n",
      "    \"r2\": 0.7554384146469659\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.37152960693974524,\n",
      "    \"macro_r2\": 0.4710240420204461\n",
      "  }\n",
      "}\n",
      "Training and predicting on ../output/asap/rnd_splits/chemprop/run_0/cleaned/split_3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type               | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | message_passing | BondMessagePassing | 2.2 M  | train\n",
      "1 | agg             | MeanAggregation    | 0      | train\n",
      "2 | bn              | BatchNorm1d        | 2.0 K  | train\n",
      "3 | predictor       | RegressionFFN      | 503 K  | train\n",
      "4 | X_d_transform   | Identity           | 0      | train\n",
      "5 | metrics         | ModuleList         | 0      | train\n",
      "---------------------------------------------------------------\n",
      "2.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.7 M     Total params\n",
      "10.656    Total estimated model params size (MB)\n",
      "25        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/core/saving.py:363: Skipping 'metrics' parameter because it is not possible to safely dump to YAML.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.2767953331915166,\n",
      "    \"r2\": 0.6301204039984402\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.16028745761682114,\n",
      "    \"r2\": 0.663433321432642\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.29833642665877813,\n",
      "    \"r2\": 0.5471326171089472\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.2769237734504908,\n",
      "    \"r2\": 0.5264360197369312\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.25114376276804534,\n",
      "    \"r2\": 0.9280584517132433\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.2526973507371304,\n",
      "    \"macro_r2\": 0.6590361627980408\n",
      "  }\n",
      "}\n",
      "\n",
      "Val metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.3253866818640561,\n",
      "    \"r2\": 0.492893794248499\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.21150653203036016,\n",
      "    \"r2\": 0.37354413875784864\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.3318158526468605,\n",
      "    \"r2\": 0.45550338997885087\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.31968172401595774,\n",
      "    \"r2\": 0.46536534357509096\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.42737466935011054,\n",
      "    \"r2\": 0.7608927316730365\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.323153091981469,\n",
      "    \"macro_r2\": 0.5096398796466651\n",
      "  }\n",
      "}\n",
      "Training and predicting on ../output/asap/rnd_splits/chemprop/run_0/cleaned/split_4.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type               | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | message_passing | BondMessagePassing | 2.2 M  | train\n",
      "1 | agg             | MeanAggregation    | 0      | train\n",
      "2 | bn              | BatchNorm1d        | 2.0 K  | train\n",
      "3 | predictor       | RegressionFFN      | 503 K  | train\n",
      "4 | X_d_transform   | Identity           | 0      | train\n",
      "5 | metrics         | ModuleList         | 0      | train\n",
      "---------------------------------------------------------------\n",
      "2.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.7 M     Total params\n",
      "10.656    Total estimated model params size (MB)\n",
      "25        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/core/saving.py:363: Skipping 'metrics' parameter because it is not possible to safely dump to YAML.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.24210467201511093,\n",
      "    \"r2\": 0.7016332287647102\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.15116976418184577,\n",
      "    \"r2\": 0.6932880332035559\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.314101288664142,\n",
      "    \"r2\": 0.4758054160296268\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.3148615278888134,\n",
      "    \"r2\": 0.45912712842205194\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.23585673976796012,\n",
      "    \"r2\": 0.9387410778123851\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.25161879850357444,\n",
      "    \"macro_r2\": 0.6537189768464661\n",
      "  }\n",
      "}\n",
      "\n",
      "Val metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.3258917271420149,\n",
      "    \"r2\": 0.447921992942513\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.20576545878646824,\n",
      "    \"r2\": 0.5563936798379261\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.31862421782253275,\n",
      "    \"r2\": 0.5426390957304861\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.2678625225794306,\n",
      "    \"r2\": 0.551097278305885\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.49125565780202557,\n",
      "    \"r2\": 0.6872739444818152\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.3218799168264944,\n",
      "    \"macro_r2\": 0.557065198259725\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>split_0-epoch</td><td>▁▁▁▁▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇███</td></tr><tr><td>split_0-train_loss_epoch</td><td>█▅▅▅▄▄▄▄▃▄▂▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_0-train_loss_step</td><td>▇▅█▃▂▃▂▄▆▃▂█▂▂▂▁▁▄▁▂▃▁▁█</td></tr><tr><td>split_0-val/mae</td><td>▂▂▂▄▄▃█▄▄▂▂▂▂▁▂▁▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_0-val/r2</td><td>▇█▄▇▁████████▇██████████████████████████</td></tr><tr><td>split_0-val_loss</td><td>▁▂█▂▃▅▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_1-epoch</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>split_1-train_loss_epoch</td><td>█▆▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_1-train_loss_step</td><td>█▆▄▇▃▄▃▃▅▃▃▇▂▂▂▂▂▃▂▁▂▁▂▃</td></tr><tr><td>split_1-val/mae</td><td>▃▂█▄▄▄▂▂▂▃▁▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_1-val/r2</td><td>▆▅▄▅▇▁█▇█▆████▇▇▇███████████████████████</td></tr><tr><td>split_1-val_loss</td><td>▃▂▄█▁▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_2-epoch</td><td>▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>split_2-train_loss_epoch</td><td>█▇▆▅▅▄▄▄▃▃▃▃▃▂▂▂▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_2-train_loss_step</td><td>▆▇█▃▅▆▃▃▅▂▂▁▃▂▅▂▂▃▁▁▂▁▁▂</td></tr><tr><td>split_2-val/mae</td><td>▃▇█▄█▅▅▂▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_2-val/r2</td><td>▄▁▇▆▄█▇▇████████████████████████████████</td></tr><tr><td>split_2-val_loss</td><td>█▃▇▁▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_3-epoch</td><td>▁▁▁▁▁▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇█████</td></tr><tr><td>split_3-train_loss_epoch</td><td>█▂▂▂▂▂▂▂▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_3-train_loss_step</td><td>█▄█▃▃▃▂▂▅▂▂▂▁▂▂▁▁▂▁▁▂▁▁▂</td></tr><tr><td>split_3-val/mae</td><td>▅▆▅▄▆▅█▃▂▂▂▂▂▁▁▁▁▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_3-val/r2</td><td>▆█▁▇▆▅██████████████████████████████████</td></tr><tr><td>split_3-val_loss</td><td>▂█▄▃▂▂▁▁▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_4-epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▆████</td></tr><tr><td>split_4-train_loss_epoch</td><td>█▆▇▇▆▆▆▅▄▄▃▃▃▂▂▂▂▂▂▁▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_4-train_loss_step</td><td>█▇█▅▆▃▃▃▃▂▂▂▂▂▆▂▂▂▁▁▂▁▂▁</td></tr><tr><td>split_4-val/mae</td><td>▅█▃▂▄▂▂▂▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_4-val/r2</td><td>▇▁▆▇▇▇▅▆▅███▇▇█████▇████████████████████</td></tr><tr><td>split_4-val_loss</td><td>█▂▂▂▁▃▂▂▃▃▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▂▃▃▅▅▆▆▇██▁▁▂▃▃▄▅▅▃▄▅▅▆▆▂▂▂▄▅▅▆▇▇▂▃▃▄▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>split_0-epoch</td><td>199</td></tr><tr><td>split_0-train_loss_epoch</td><td>0.09074</td></tr><tr><td>split_0-train_loss_step</td><td>0.57636</td></tr><tr><td>split_0-val/mae</td><td>0.33734</td></tr><tr><td>split_0-val/r2</td><td>0.76842</td></tr><tr><td>split_0-val_loss</td><td>0.21539</td></tr><tr><td>split_1-epoch</td><td>199</td></tr><tr><td>split_1-train_loss_epoch</td><td>0.08518</td></tr><tr><td>split_1-train_loss_step</td><td>0.15682</td></tr><tr><td>split_1-val/mae</td><td>0.35515</td></tr><tr><td>split_1-val/r2</td><td>0.73273</td></tr><tr><td>split_1-val_loss</td><td>0.25372</td></tr><tr><td>split_2-epoch</td><td>199</td></tr><tr><td>split_2-train_loss_epoch</td><td>0.08569</td></tr><tr><td>split_2-train_loss_step</td><td>0.17989</td></tr><tr><td>split_2-val/mae</td><td>0.36448</td></tr><tr><td>split_2-val/r2</td><td>0.75621</td></tr><tr><td>split_2-val_loss</td><td>0.24775</td></tr><tr><td>split_3-epoch</td><td>199</td></tr><tr><td>split_3-train_loss_epoch</td><td>0.09093</td></tr><tr><td>split_3-train_loss_step</td><td>0.15379</td></tr><tr><td>split_3-val/mae</td><td>0.31789</td></tr><tr><td>split_3-val/r2</td><td>0.79005</td></tr><tr><td>split_3-val_loss</td><td>0.19585</td></tr><tr><td>split_4-epoch</td><td>199</td></tr><tr><td>split_4-train_loss_epoch</td><td>0.07387</td></tr><tr><td>split_4-train_loss_step</td><td>0.08753</td></tr><tr><td>split_4-val/mae</td><td>0.31848</td></tr><tr><td>split_4-val/r2</td><td>0.77016</td></tr><tr><td>split_4-val_loss</td><td>0.21148</td></tr><tr><td>trainer/global_step</td><td>1199</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">chemprop_run_1_clean_worst_pct_0.2_stereo_impure</strong> at: <a href='https://wandb.ai/vladvin-org/admet-challenge/runs/pwmxfqik' target=\"_blank\">https://wandb.ai/vladvin-org/admet-challenge/runs/pwmxfqik</a><br> View project at: <a href='https://wandb.ai/vladvin-org/admet-challenge' target=\"_blank\">https://wandb.ai/vladvin-org/admet-challenge</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>../wandb/chemprop_run_1_clean_worst_pct_0.2_stereo_impure/wandb/run-20250311_212920-pwmxfqik/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_and_eval(input_paths, save_dirs, RUN_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing all stereo impure + run 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_paths = [Path(f'../data/asap/datasets/rnd_splits/split_{k}.csv') for k in range(5)]\n",
    "output_dir = Path(\"../data/asap/datasets/rnd_splits/stereo_pure\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for input_path in input_paths:\n",
    "    input_df = pd.read_csv(input_path)\n",
    "    input_df = pd.concat([\n",
    "        input_df[(input_df[\"split\"] == \"train\") & input_df[\"smiles_ext\"].isna()],\n",
    "        input_df[input_df[\"split\"] == \"val\"]\n",
    "    ])\n",
    "    input_df.to_csv(output_dir / input_path.name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predicting on ../data/asap/datasets/rnd_splits/stereo_pure/split_0.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>../wandb/chemprop_run_0_stereo_pure/wandb/run-20250311_213932-wl3x9tnc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vladvin-org/admet-challenge/runs/wl3x9tnc' target=\"_blank\">chemprop_run_0_stereo_pure</a></strong> to <a href='https://wandb.ai/vladvin-org/admet-challenge' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/vladvin-org/admet-challenge' target=\"_blank\">https://wandb.ai/vladvin-org/admet-challenge</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/vladvin-org/admet-challenge/runs/wl3x9tnc' target=\"_blank\">https://wandb.ai/vladvin-org/admet-challenge/runs/wl3x9tnc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type               | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | message_passing | BondMessagePassing | 2.2 M  | train\n",
      "1 | agg             | MeanAggregation    | 0      | train\n",
      "2 | bn              | BatchNorm1d        | 2.0 K  | train\n",
      "3 | predictor       | RegressionFFN      | 503 K  | train\n",
      "4 | X_d_transform   | Identity           | 0      | train\n",
      "5 | metrics         | ModuleList         | 0      | train\n",
      "---------------------------------------------------------------\n",
      "2.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.7 M     Total params\n",
      "10.656    Total estimated model params size (MB)\n",
      "25        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/core/saving.py:363: Skipping 'metrics' parameter because it is not possible to safely dump to YAML.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.12030690768355291,\n",
      "    \"r2\": 0.9563927467311923\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.09372683182009414,\n",
      "    \"r2\": 0.8877512309931737\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.12718344824401792,\n",
      "    \"r2\": 0.9416204741891571\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.12545981667105635,\n",
      "    \"r2\": 0.9356275576482681\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.15688534669692702,\n",
      "    \"r2\": 0.9730673835140098\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.12471247022312966,\n",
      "    \"macro_r2\": 0.9388918786151603\n",
      "  }\n",
      "}\n",
      "\n",
      "Val metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.5680448231825244,\n",
      "    \"r2\": -0.09786599411764163\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.29273192821805477,\n",
      "    \"r2\": 0.2424324464193509\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.4911846075775767,\n",
      "    \"r2\": 0.1361321479552149\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.5211672928871471,\n",
      "    \"r2\": -0.4134754428766063\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.626462386941812,\n",
      "    \"r2\": 0.6238653953298569\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.499918207761423,\n",
      "    \"macro_r2\": 0.09821771054203494\n",
      "  }\n",
      "}\n",
      "Training and predicting on ../data/asap/datasets/rnd_splits/stereo_pure/split_1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type               | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | message_passing | BondMessagePassing | 2.2 M  | train\n",
      "1 | agg             | MeanAggregation    | 0      | train\n",
      "2 | bn              | BatchNorm1d        | 2.0 K  | train\n",
      "3 | predictor       | RegressionFFN      | 503 K  | train\n",
      "4 | X_d_transform   | Identity           | 0      | train\n",
      "5 | metrics         | ModuleList         | 0      | train\n",
      "---------------------------------------------------------------\n",
      "2.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.7 M     Total params\n",
      "10.656    Total estimated model params size (MB)\n",
      "25        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/core/saving.py:363: Skipping 'metrics' parameter because it is not possible to safely dump to YAML.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.11660677789413042,\n",
      "    \"r2\": 0.9591897634784995\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.10178770163604664,\n",
      "    \"r2\": 0.8740749461495877\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.11841434787636623,\n",
      "    \"r2\": 0.9445401677974787\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.13853918386000208,\n",
      "    \"r2\": 0.922922121334044\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.13979969267042414,\n",
      "    \"r2\": 0.9809356262718117\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.12302954078739389,\n",
      "    \"macro_r2\": 0.9363325250062843\n",
      "  }\n",
      "}\n",
      "\n",
      "Val metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.4357920451919993,\n",
      "    \"r2\": 0.2025251509514966\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.2650045787792353,\n",
      "    \"r2\": 0.14335606208567642\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.5269354235940544,\n",
      "    \"r2\": 0.020800870876167665\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.5451256776020096,\n",
      "    \"r2\": -0.24332393318536716\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.7866876216345665,\n",
      "    \"r2\": 0.21822647096431702\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.5119090693603731,\n",
      "    \"macro_r2\": 0.06831692433845811\n",
      "  }\n",
      "}\n",
      "Training and predicting on ../data/asap/datasets/rnd_splits/stereo_pure/split_2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type               | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | message_passing | BondMessagePassing | 2.2 M  | train\n",
      "1 | agg             | MeanAggregation    | 0      | train\n",
      "2 | bn              | BatchNorm1d        | 2.0 K  | train\n",
      "3 | predictor       | RegressionFFN      | 503 K  | train\n",
      "4 | X_d_transform   | Identity           | 0      | train\n",
      "5 | metrics         | ModuleList         | 0      | train\n",
      "---------------------------------------------------------------\n",
      "2.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.7 M     Total params\n",
      "10.656    Total estimated model params size (MB)\n",
      "25        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/core/saving.py:363: Skipping 'metrics' parameter because it is not possible to safely dump to YAML.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.14152796655820626,\n",
      "    \"r2\": 0.9371535655117621\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.09380656610819096,\n",
      "    \"r2\": 0.888514819396159\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.18963686476001324,\n",
      "    \"r2\": 0.8670893259528928\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.15079814920057377,\n",
      "    \"r2\": 0.9056831186521972\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.21696616546975245,\n",
      "    \"r2\": 0.9610252910927546\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.15854714241934734,\n",
      "    \"macro_r2\": 0.9118932241211531\n",
      "  }\n",
      "}\n",
      "\n",
      "Val metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.49318758265430235,\n",
      "    \"r2\": 0.0911451892790337\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.2699714709493397,\n",
      "    \"r2\": 0.2192983815940759\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.45964326788790866,\n",
      "    \"r2\": -0.09534373750856018\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.403706195131317,\n",
      "    \"r2\": -0.03093494549489484\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.7326988833217006,\n",
      "    \"r2\": 0.5066300589699301\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.47184147998891374,\n",
      "    \"macro_r2\": 0.13815898936791693\n",
      "  }\n",
      "}\n",
      "Training and predicting on ../data/asap/datasets/rnd_splits/stereo_pure/split_3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type               | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | message_passing | BondMessagePassing | 2.2 M  | train\n",
      "1 | agg             | MeanAggregation    | 0      | train\n",
      "2 | bn              | BatchNorm1d        | 2.0 K  | train\n",
      "3 | predictor       | RegressionFFN      | 503 K  | train\n",
      "4 | X_d_transform   | Identity           | 0      | train\n",
      "5 | metrics         | ModuleList         | 0      | train\n",
      "---------------------------------------------------------------\n",
      "2.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.7 M     Total params\n",
      "10.656    Total estimated model params size (MB)\n",
      "25        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/core/saving.py:363: Skipping 'metrics' parameter because it is not possible to safely dump to YAML.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.1125530165610916,\n",
      "    \"r2\": 0.9562336711686011\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.0848403883746084,\n",
      "    \"r2\": 0.90891759236915\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.0933274110996453,\n",
      "    \"r2\": 0.9656599027784813\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.10275760771433629,\n",
      "    \"r2\": 0.951066384191527\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.11379170615709464,\n",
      "    \"r2\": 0.9873025748477414\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.10145402598135525,\n",
      "    \"macro_r2\": 0.9538360250711001\n",
      "  }\n",
      "}\n",
      "\n",
      "Val metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.4858513129826885,\n",
      "    \"r2\": -0.1264214246580686\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.23480826473811126,\n",
      "    \"r2\": 0.22987918666071872\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.4350341699256539,\n",
      "    \"r2\": 0.11532076174268113\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.49753429991076276,\n",
      "    \"r2\": -0.058802434348966814\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.7951624211072922,\n",
      "    \"r2\": 0.2480423905510195\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.4896780937329018,\n",
      "    \"macro_r2\": 0.08160369598947678\n",
      "  }\n",
      "}\n",
      "Training and predicting on ../data/asap/datasets/rnd_splits/stereo_pure/split_4.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type               | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | message_passing | BondMessagePassing | 2.2 M  | train\n",
      "1 | agg             | MeanAggregation    | 0      | train\n",
      "2 | bn              | BatchNorm1d        | 2.0 K  | train\n",
      "3 | predictor       | RegressionFFN      | 503 K  | train\n",
      "4 | X_d_transform   | Identity           | 0      | train\n",
      "5 | metrics         | ModuleList         | 0      | train\n",
      "---------------------------------------------------------------\n",
      "2.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.7 M     Total params\n",
      "10.656    Total estimated model params size (MB)\n",
      "25        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/core/saving.py:363: Skipping 'metrics' parameter because it is not possible to safely dump to YAML.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.11477006558459803,\n",
      "    \"r2\": 0.9576309482033944\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.0875738935878281,\n",
      "    \"r2\": 0.9043522224232208\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.1051815400347513,\n",
      "    \"r2\": 0.9575025534695657\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.09288331901201524,\n",
      "    \"r2\": 0.9642531960825176\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.14471215070102567,\n",
      "    \"r2\": 0.9780445088373652\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.10902419378404367,\n",
      "    \"macro_r2\": 0.9523566858032128\n",
      "  }\n",
      "}\n",
      "\n",
      "Val metrics:\n",
      "{\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.4783119779500371,\n",
      "    \"r2\": 0.1757673292914611\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.25400166066229396,\n",
      "    \"r2\": 0.3244032566585551\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.4151491230803711,\n",
      "    \"r2\": 0.1434760611657272\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.35107380090087575,\n",
      "    \"r2\": 0.13784652405377185\n",
      "  },\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.6285614242951074,\n",
      "    \"r2\": 0.5383199674003585\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.4254195973777371,\n",
      "    \"macro_r2\": 0.2639626277139747\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>split_0-epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇█████</td></tr><tr><td>split_0-train_loss_epoch</td><td>█▅▄▄▄▃▃▃▂▂▂▃▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_0-train_loss_step</td><td>█▄▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>split_0-val/mae</td><td>█▃▄▃▂▂▃▁▂▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_0-val/r2</td><td>▂▂▁▆▅▇▆▇▆▆▇▇█▇██████████████████████████</td></tr><tr><td>split_0-val_loss</td><td>█▃▂▂▂▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_1-epoch</td><td>▁▁▁▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇██</td></tr><tr><td>split_1-train_loss_epoch</td><td>█▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_1-train_loss_step</td><td>█▅▃▂▂▂▁▁▂▂▁▁</td></tr><tr><td>split_1-val/mae</td><td>█▄▃▃▃▂▂▂▁▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_1-val/r2</td><td>▁▆▆▃▆▇▆▇▇▇▇▇▇█▇██▇██▇▇██████████████████</td></tr><tr><td>split_1-val_loss</td><td>█▆▂▂▃▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_2-epoch</td><td>▁▁▁▁▁▂▂▂▂▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇███</td></tr><tr><td>split_2-train_loss_epoch</td><td>█▆▆▅▅▄▃▂▂▂▂▂▁▁▂▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_2-train_loss_step</td><td>█▄▃▂▂▃▂▁▁▁▁▂</td></tr><tr><td>split_2-val/mae</td><td>▇▅█▅▅▇▆▃▄▃▃▂▆▂▂▂▂▂▃▃▂▃▂▃▂▂▁▁▂▁▂▂▂▂▂▂▁▂▂▂</td></tr><tr><td>split_2-val/r2</td><td>▁▆▇▇▇▇▇▇████████████████████████████████</td></tr><tr><td>split_2-val_loss</td><td>█▄▃▇▃▂▄▃▄▃▁▁▂▂▁▁▂▁▂▂▂▂▁▂▂▂▁▁▁▁▁▁▂▂▁▁▂▂▁▁</td></tr><tr><td>split_3-epoch</td><td>▁▁▁▁▂▂▂▂▃▃▄▄▅▅▅▅▅▅▅▅▅▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇██</td></tr><tr><td>split_3-train_loss_epoch</td><td>█▅▅▄▄▄▃▃▂▂▂▂▂▂▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_3-train_loss_step</td><td>█▄▃▂▂▁▁▁▂▁▁▁</td></tr><tr><td>split_3-val/mae</td><td>▄▄▅█▂▁▂▂▂▁▁▁▁▂▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_3-val/r2</td><td>▁▆▇▇▇█▇▇███▇████████████████████████████</td></tr><tr><td>split_3-val_loss</td><td>▃▃▁█▃▁▂▂▁▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_4-epoch</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇█████</td></tr><tr><td>split_4-train_loss_epoch</td><td>██▇▄▅▅▃▃▂▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_4-train_loss_step</td><td>█▅▄▃▂▂▂▁▂▁▁▁</td></tr><tr><td>split_4-val/mae</td><td>█▇▄▄▃▂▄▃▂▂▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_4-val/r2</td><td>▆▁▄▆▆▇▆▇▇▅▇▇█▇█████▇█▇██████████████████</td></tr><tr><td>split_4-val_loss</td><td>▃▂▃▃█▄▃▂▄▃▂▂▂▂▂▂▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▂▃▅▆▇▁▁▃▄▅▅▆▂▂▂▃▃▃▄▆▇▇▁▁▂▅▆█▁▁▃▄▄▄▅▆▇▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>split_0-epoch</td><td>199</td></tr><tr><td>split_0-train_loss_epoch</td><td>0.04666</td></tr><tr><td>split_0-train_loss_step</td><td>0.04502</td></tr><tr><td>split_0-val/mae</td><td>0.48808</td></tr><tr><td>split_0-val/r2</td><td>0.59129</td></tr><tr><td>split_0-val_loss</td><td>0.38015</td></tr><tr><td>split_1-epoch</td><td>199</td></tr><tr><td>split_1-train_loss_epoch</td><td>0.04955</td></tr><tr><td>split_1-train_loss_step</td><td>0.04722</td></tr><tr><td>split_1-val/mae</td><td>0.4988</td></tr><tr><td>split_1-val/r2</td><td>0.46755</td></tr><tr><td>split_1-val_loss</td><td>0.50547</td></tr><tr><td>split_2-epoch</td><td>199</td></tr><tr><td>split_2-train_loss_epoch</td><td>0.06676</td></tr><tr><td>split_2-train_loss_step</td><td>0.09995</td></tr><tr><td>split_2-val/mae</td><td>0.46552</td></tr><tr><td>split_2-val/r2</td><td>0.58324</td></tr><tr><td>split_2-val_loss</td><td>0.42354</td></tr><tr><td>split_3-epoch</td><td>199</td></tr><tr><td>split_3-train_loss_epoch</td><td>0.06135</td></tr><tr><td>split_3-train_loss_step</td><td>0.08285</td></tr><tr><td>split_3-val/mae</td><td>0.48094</td></tr><tr><td>split_3-val/r2</td><td>0.50848</td></tr><tr><td>split_3-val_loss</td><td>0.45853</td></tr><tr><td>split_4-epoch</td><td>199</td></tr><tr><td>split_4-train_loss_epoch</td><td>0.05384</td></tr><tr><td>split_4-train_loss_step</td><td>0.04478</td></tr><tr><td>split_4-val/mae</td><td>0.42069</td></tr><tr><td>split_4-val/r2</td><td>0.63718</td></tr><tr><td>split_4-val_loss</td><td>0.33385</td></tr><tr><td>trainer/global_step</td><td>599</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">chemprop_run_0_stereo_pure</strong> at: <a href='https://wandb.ai/vladvin-org/admet-challenge/runs/wl3x9tnc' target=\"_blank\">https://wandb.ai/vladvin-org/admet-challenge/runs/wl3x9tnc</a><br> View project at: <a href='https://wandb.ai/vladvin-org/admet-challenge' target=\"_blank\">https://wandb.ai/vladvin-org/admet-challenge</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>../wandb/chemprop_run_0_stereo_pure/wandb/run-20250311_213932-wl3x9tnc/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_paths = [Path(f'../data/asap/datasets/rnd_splits/stereo_pure/split_{k}.csv') for k in range(5)]\n",
    "save_dirs = [Path(f'../output/asap/rnd_splits/chemprop/run_0/stereo_pure/split_{k}') for k in range(5)]\n",
    "RUN_IDX = \"0_stereo_pure\"\n",
    "train_and_eval(input_paths, save_dirs, RUN_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up unstable predictions for stero impure + run 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "      <th>HLM</th>\n",
       "      <th>KSOL</th>\n",
       "      <th>LogD</th>\n",
       "      <th>MLM</th>\n",
       "      <th>MDR1-MDCKII</th>\n",
       "      <th>smiles_std</th>\n",
       "      <th>cxsmiles_std</th>\n",
       "      <th>mol_idx</th>\n",
       "      <th>smiles_ext</th>\n",
       "      <th>LogHLM</th>\n",
       "      <th>LogMLM</th>\n",
       "      <th>LogKSOL</th>\n",
       "      <th>LogMDR1-MDCKII</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>COC1=CC=CC(Cl)=C1NC(=O)N1CCC[C@H](C(N)=O)C1 |a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>COc1cccc(Cl)c1NC(=O)N1CCC[C@H](C(N)=O)C1</td>\n",
       "      <td>COc1cccc(Cl)c1NC(=O)N1CCC[C@H](C(N)=O)C1 |a:16|</td>\n",
       "      <td>191</td>\n",
       "      <td>|a:16|</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.477121</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>O=C(NCC(F)F)[C@H](NC1=CC2=C(C=C1Br)CNC2)C1=CC(...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>333.0</td>\n",
       "      <td>2.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2</td>\n",
       "      <td>O=C(NCC(F)F)[C@H](Nc1cc2c(cc1Br)CNC2)c1cc(Cl)c...</td>\n",
       "      <td>O=C(NCC(F)F)[C@H](Nc1cc2c(cc1Br)CNC2)c1cc(Cl)c...</td>\n",
       "      <td>335</td>\n",
       "      <td>|&amp;1:7|</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.523746</td>\n",
       "      <td>0.079181</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>O=C(NCC(F)F)[C@H](NC1=CC=C2CNCC2=C1)C1=CC(Br)=...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5</td>\n",
       "      <td>O=C(NCC(F)F)[C@H](Nc1ccc2c(c1)CNC2)c1cc(Br)cc2...</td>\n",
       "      <td>O=C(NCC(F)F)[C@H](Nc1ccc2c(c1)CNC2)c1cc(Br)cc2...</td>\n",
       "      <td>336</td>\n",
       "      <td>|&amp;1:7|</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.176091</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NC(=O)[C@H]1CCCN(C(=O)CC2=CC=CC3=C2C=CO3)C1 |&amp;...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>376.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.5</td>\n",
       "      <td>NC(=O)[C@H]1CCCN(C(=O)Cc2cccc3occc23)C1</td>\n",
       "      <td>NC(=O)[C@H]1CCCN(C(=O)Cc2cccc3occc23)C1 |&amp;1:3|</td>\n",
       "      <td>300</td>\n",
       "      <td>|&amp;1:3|</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.576341</td>\n",
       "      <td>0.977724</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CC1=CC(CC(=O)N2CCC[C@H](C(N)=O)C2)=CC=N1 |&amp;1:11|</td>\n",
       "      <td>NaN</td>\n",
       "      <td>375.0</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9</td>\n",
       "      <td>Cc1cc(CC(=O)N2CCC[C@H](C(N)=O)C2)ccn1</td>\n",
       "      <td>Cc1cc(CC(=O)N2CCC[C@H](C(N)=O)C2)ccn1 |&amp;1:11|</td>\n",
       "      <td>249</td>\n",
       "      <td>|&amp;1:11|</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.575188</td>\n",
       "      <td>0.278754</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>CC(C)NC[C@H](O)COC1=CC=CC2=CC=CC=C12 |&amp;1:5|</td>\n",
       "      <td>25.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>63.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CC(C)NC[C@H](O)COc1cccc2ccccc12</td>\n",
       "      <td>CC(C)NC[C@H](O)COc1cccc2ccccc12 |&amp;1:5|</td>\n",
       "      <td>22</td>\n",
       "      <td>|&amp;1:5|</td>\n",
       "      <td>1.423246</td>\n",
       "      <td>1.806180</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>O=C(O)CC1=CC=CC=C1NC1=C(Cl)C=CC=C1Cl</td>\n",
       "      <td>216.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>386.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>O=C(O)Cc1ccccc1Nc1c(Cl)cccc1Cl</td>\n",
       "      <td>O=C(O)Cc1ccccc1Nc1c(Cl)cccc1Cl</td>\n",
       "      <td>380</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.336460</td>\n",
       "      <td>2.587711</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>NCC1=CC(Cl)=CC(C(=O)NC2=CC=C3CNCC3=C2)=C1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NCc1cc(Cl)cc(C(=O)Nc2ccc3c(c2)CNC3)c1</td>\n",
       "      <td>NCc1cc(Cl)cc(C(=O)Nc2ccc3c(c2)CNC3)c1</td>\n",
       "      <td>303</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>COC(=O)NC1=NC2=CC=C(C(=O)C3=CC=CC=C3)C=C2N1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>COC(=O)Nc1nc2ccc(C(=O)c3ccccc3)cc2[nH]1</td>\n",
       "      <td>COC(=O)Nc1nc2ccc(C(=O)c3ccccc3)cc2[nH]1</td>\n",
       "      <td>166</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>CC1=NC=CN1C[C@H]1CCC2=C(C1=O)C1=CC=CC=C1N2C |&amp;...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>127.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cc1nccn1C[C@H]1CCc2c(c3ccccc3n2C)C1=O</td>\n",
       "      <td>Cc1nccn1C[C@H]1CCc2c(c3ccccc3n2C)C1=O |&amp;1:7|</td>\n",
       "      <td>268</td>\n",
       "      <td>|&amp;1:7|</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.107210</td>\n",
       "      <td>NaN</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>404 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                smiles    HLM   KSOL  LogD  \\\n",
       "0    COC1=CC=CC(Cl)=C1NC(=O)N1CCC[C@H](C(N)=O)C1 |a...    NaN    NaN   0.3   \n",
       "1    O=C(NCC(F)F)[C@H](NC1=CC2=C(C=C1Br)CNC2)C1=CC(...    NaN  333.0   2.9   \n",
       "2    O=C(NCC(F)F)[C@H](NC1=CC=C2CNCC2=C1)C1=CC(Br)=...    NaN    NaN   0.4   \n",
       "3    NC(=O)[C@H]1CCCN(C(=O)CC2=CC=CC3=C2C=CO3)C1 |&...    NaN  376.0   1.0   \n",
       "4     CC1=CC(CC(=O)N2CCC[C@H](C(N)=O)C2)=CC=N1 |&1:11|    NaN  375.0  -0.3   \n",
       "..                                                 ...    ...    ...   ...   \n",
       "399        CC(C)NC[C@H](O)COC1=CC=CC2=CC=CC=C12 |&1:5|   25.5    NaN   NaN   \n",
       "400               O=C(O)CC1=CC=CC=C1NC1=C(Cl)C=CC=C1Cl  216.0    NaN   NaN   \n",
       "401          NCC1=CC(Cl)=CC(C(=O)NC2=CC=C3CNCC3=C2)=C1    NaN    NaN   2.0   \n",
       "402        COC(=O)NC1=NC2=CC=C(C(=O)C3=CC=CC=C3)C=C2N1    NaN    NaN   2.9   \n",
       "403  CC1=NC=CN1C[C@H]1CCC2=C(C1=O)C1=CC=CC=C1N2C |&...    NaN  127.0   NaN   \n",
       "\n",
       "       MLM  MDR1-MDCKII                                         smiles_std  \\\n",
       "0      NaN          2.0           COc1cccc(Cl)c1NC(=O)N1CCC[C@H](C(N)=O)C1   \n",
       "1      NaN          0.2  O=C(NCC(F)F)[C@H](Nc1cc2c(cc1Br)CNC2)c1cc(Cl)c...   \n",
       "2      NaN          0.5  O=C(NCC(F)F)[C@H](Nc1ccc2c(c1)CNC2)c1cc(Br)cc2...   \n",
       "3      NaN          8.5            NC(=O)[C@H]1CCCN(C(=O)Cc2cccc3occc23)C1   \n",
       "4      NaN          0.9              Cc1cc(CC(=O)N2CCC[C@H](C(N)=O)C2)ccn1   \n",
       "..     ...          ...                                                ...   \n",
       "399   63.0          NaN                    CC(C)NC[C@H](O)COc1cccc2ccccc12   \n",
       "400  386.0          NaN                     O=C(O)Cc1ccccc1Nc1c(Cl)cccc1Cl   \n",
       "401    NaN          NaN              NCc1cc(Cl)cc(C(=O)Nc2ccc3c(c2)CNC3)c1   \n",
       "402    NaN          NaN            COC(=O)Nc1nc2ccc(C(=O)c3ccccc3)cc2[nH]1   \n",
       "403    NaN          NaN              Cc1nccn1C[C@H]1CCc2c(c3ccccc3n2C)C1=O   \n",
       "\n",
       "                                          cxsmiles_std  mol_idx smiles_ext  \\\n",
       "0      COc1cccc(Cl)c1NC(=O)N1CCC[C@H](C(N)=O)C1 |a:16|      191     |a:16|   \n",
       "1    O=C(NCC(F)F)[C@H](Nc1cc2c(cc1Br)CNC2)c1cc(Cl)c...      335     |&1:7|   \n",
       "2    O=C(NCC(F)F)[C@H](Nc1ccc2c(c1)CNC2)c1cc(Br)cc2...      336     |&1:7|   \n",
       "3       NC(=O)[C@H]1CCCN(C(=O)Cc2cccc3occc23)C1 |&1:3|      300     |&1:3|   \n",
       "4        Cc1cc(CC(=O)N2CCC[C@H](C(N)=O)C2)ccn1 |&1:11|      249    |&1:11|   \n",
       "..                                                 ...      ...        ...   \n",
       "399             CC(C)NC[C@H](O)COc1cccc2ccccc12 |&1:5|       22     |&1:5|   \n",
       "400                     O=C(O)Cc1ccccc1Nc1c(Cl)cccc1Cl      380        NaN   \n",
       "401              NCc1cc(Cl)cc(C(=O)Nc2ccc3c(c2)CNC3)c1      303        NaN   \n",
       "402            COC(=O)Nc1nc2ccc(C(=O)c3ccccc3)cc2[nH]1      166        NaN   \n",
       "403       Cc1nccn1C[C@H]1CCc2c(c3ccccc3n2C)C1=O |&1:7|      268     |&1:7|   \n",
       "\n",
       "       LogHLM    LogMLM   LogKSOL  LogMDR1-MDCKII  split  \n",
       "0         NaN       NaN       NaN        0.477121    val  \n",
       "1         NaN       NaN  2.523746        0.079181  train  \n",
       "2         NaN       NaN       NaN        0.176091  train  \n",
       "3         NaN       NaN  2.576341        0.977724  train  \n",
       "4         NaN       NaN  2.575188        0.278754  train  \n",
       "..        ...       ...       ...             ...    ...  \n",
       "399  1.423246  1.806180       NaN             NaN    val  \n",
       "400  2.336460  2.587711       NaN             NaN    val  \n",
       "401       NaN       NaN       NaN             NaN  train  \n",
       "402       NaN       NaN       NaN             NaN  train  \n",
       "403       NaN       NaN  2.107210             NaN  train  \n",
       "\n",
       "[404 rows x 15 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_df = pd.read_csv(\"../data/asap/datasets/rnd_splits/split_0.csv\")\n",
    "input_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "404"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_df[\"mol_idx\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import EnumerateStereoisomers\n",
    "opts = EnumerateStereoisomers.StereoEnumerationOptions()\n",
    "opts.onlyStereoGroups = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_isomers(smiles: str) -> list[str]:\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    enumerated = EnumerateStereoisomers.EnumerateStereoisomers(mol, opts)\n",
    "\n",
    "    return [Chem.MolToSmiles(isomer) for isomer in enumerated]\n",
    "\n",
    "def calculate_num_isomers(input_df: pd.DataFrame):\n",
    "    input_df = input_df.copy()\n",
    "    input_df[\"n_isomers\"] = input_df[\"cxsmiles_std\"].apply(lambda x: len(get_all_isomers(x)))\n",
    "    \n",
    "    return input_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "      <th>HLM</th>\n",
       "      <th>KSOL</th>\n",
       "      <th>LogD</th>\n",
       "      <th>MLM</th>\n",
       "      <th>MDR1-MDCKII</th>\n",
       "      <th>smiles_std</th>\n",
       "      <th>cxsmiles_std</th>\n",
       "      <th>mol_idx</th>\n",
       "      <th>smiles_ext</th>\n",
       "      <th>LogHLM</th>\n",
       "      <th>LogMLM</th>\n",
       "      <th>LogKSOL</th>\n",
       "      <th>LogMDR1-MDCKII</th>\n",
       "      <th>split</th>\n",
       "      <th>n_isomers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>COC1=CC=CC(Cl)=C1NC(=O)N1CCC[C@H](C(N)=O)C1 |a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>COc1cccc(Cl)c1NC(=O)N1CCC[C@H](C(N)=O)C1</td>\n",
       "      <td>COc1cccc(Cl)c1NC(=O)N1CCC[C@H](C(N)=O)C1 |a:16|</td>\n",
       "      <td>191</td>\n",
       "      <td>|a:16|</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.477121</td>\n",
       "      <td>val</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>O=C(NCC(F)F)[C@H](NC1=CC2=C(C=C1Br)CNC2)C1=CC(...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>333.0</td>\n",
       "      <td>2.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2</td>\n",
       "      <td>O=C(NCC(F)F)[C@H](Nc1cc2c(cc1Br)CNC2)c1cc(Cl)c...</td>\n",
       "      <td>O=C(NCC(F)F)[C@H](Nc1cc2c(cc1Br)CNC2)c1cc(Cl)c...</td>\n",
       "      <td>335</td>\n",
       "      <td>|&amp;1:7|</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.523746</td>\n",
       "      <td>0.079181</td>\n",
       "      <td>train</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>O=C(NCC(F)F)[C@H](NC1=CC=C2CNCC2=C1)C1=CC(Br)=...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5</td>\n",
       "      <td>O=C(NCC(F)F)[C@H](Nc1ccc2c(c1)CNC2)c1cc(Br)cc2...</td>\n",
       "      <td>O=C(NCC(F)F)[C@H](Nc1ccc2c(c1)CNC2)c1cc(Br)cc2...</td>\n",
       "      <td>336</td>\n",
       "      <td>|&amp;1:7|</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.176091</td>\n",
       "      <td>train</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NC(=O)[C@H]1CCCN(C(=O)CC2=CC=CC3=C2C=CO3)C1 |&amp;...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>376.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.5</td>\n",
       "      <td>NC(=O)[C@H]1CCCN(C(=O)Cc2cccc3occc23)C1</td>\n",
       "      <td>NC(=O)[C@H]1CCCN(C(=O)Cc2cccc3occc23)C1 |&amp;1:3|</td>\n",
       "      <td>300</td>\n",
       "      <td>|&amp;1:3|</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.576341</td>\n",
       "      <td>0.977724</td>\n",
       "      <td>train</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CC1=CC(CC(=O)N2CCC[C@H](C(N)=O)C2)=CC=N1 |&amp;1:11|</td>\n",
       "      <td>NaN</td>\n",
       "      <td>375.0</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9</td>\n",
       "      <td>Cc1cc(CC(=O)N2CCC[C@H](C(N)=O)C2)ccn1</td>\n",
       "      <td>Cc1cc(CC(=O)N2CCC[C@H](C(N)=O)C2)ccn1 |&amp;1:11|</td>\n",
       "      <td>249</td>\n",
       "      <td>|&amp;1:11|</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.575188</td>\n",
       "      <td>0.278754</td>\n",
       "      <td>train</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>CC(C)NC[C@H](O)COC1=CC=CC2=CC=CC=C12 |&amp;1:5|</td>\n",
       "      <td>25.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>63.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CC(C)NC[C@H](O)COc1cccc2ccccc12</td>\n",
       "      <td>CC(C)NC[C@H](O)COc1cccc2ccccc12 |&amp;1:5|</td>\n",
       "      <td>22</td>\n",
       "      <td>|&amp;1:5|</td>\n",
       "      <td>1.423246</td>\n",
       "      <td>1.806180</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>val</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>O=C(O)CC1=CC=CC=C1NC1=C(Cl)C=CC=C1Cl</td>\n",
       "      <td>216.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>386.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>O=C(O)Cc1ccccc1Nc1c(Cl)cccc1Cl</td>\n",
       "      <td>O=C(O)Cc1ccccc1Nc1c(Cl)cccc1Cl</td>\n",
       "      <td>380</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.336460</td>\n",
       "      <td>2.587711</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>val</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>NCC1=CC(Cl)=CC(C(=O)NC2=CC=C3CNCC3=C2)=C1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NCc1cc(Cl)cc(C(=O)Nc2ccc3c(c2)CNC3)c1</td>\n",
       "      <td>NCc1cc(Cl)cc(C(=O)Nc2ccc3c(c2)CNC3)c1</td>\n",
       "      <td>303</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>COC(=O)NC1=NC2=CC=C(C(=O)C3=CC=CC=C3)C=C2N1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>COC(=O)Nc1nc2ccc(C(=O)c3ccccc3)cc2[nH]1</td>\n",
       "      <td>COC(=O)Nc1nc2ccc(C(=O)c3ccccc3)cc2[nH]1</td>\n",
       "      <td>166</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>CC1=NC=CN1C[C@H]1CCC2=C(C1=O)C1=CC=CC=C1N2C |&amp;...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>127.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cc1nccn1C[C@H]1CCc2c(c3ccccc3n2C)C1=O</td>\n",
       "      <td>Cc1nccn1C[C@H]1CCc2c(c3ccccc3n2C)C1=O |&amp;1:7|</td>\n",
       "      <td>268</td>\n",
       "      <td>|&amp;1:7|</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.107210</td>\n",
       "      <td>NaN</td>\n",
       "      <td>train</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>404 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                smiles    HLM   KSOL  LogD  \\\n",
       "0    COC1=CC=CC(Cl)=C1NC(=O)N1CCC[C@H](C(N)=O)C1 |a...    NaN    NaN   0.3   \n",
       "1    O=C(NCC(F)F)[C@H](NC1=CC2=C(C=C1Br)CNC2)C1=CC(...    NaN  333.0   2.9   \n",
       "2    O=C(NCC(F)F)[C@H](NC1=CC=C2CNCC2=C1)C1=CC(Br)=...    NaN    NaN   0.4   \n",
       "3    NC(=O)[C@H]1CCCN(C(=O)CC2=CC=CC3=C2C=CO3)C1 |&...    NaN  376.0   1.0   \n",
       "4     CC1=CC(CC(=O)N2CCC[C@H](C(N)=O)C2)=CC=N1 |&1:11|    NaN  375.0  -0.3   \n",
       "..                                                 ...    ...    ...   ...   \n",
       "399        CC(C)NC[C@H](O)COC1=CC=CC2=CC=CC=C12 |&1:5|   25.5    NaN   NaN   \n",
       "400               O=C(O)CC1=CC=CC=C1NC1=C(Cl)C=CC=C1Cl  216.0    NaN   NaN   \n",
       "401          NCC1=CC(Cl)=CC(C(=O)NC2=CC=C3CNCC3=C2)=C1    NaN    NaN   2.0   \n",
       "402        COC(=O)NC1=NC2=CC=C(C(=O)C3=CC=CC=C3)C=C2N1    NaN    NaN   2.9   \n",
       "403  CC1=NC=CN1C[C@H]1CCC2=C(C1=O)C1=CC=CC=C1N2C |&...    NaN  127.0   NaN   \n",
       "\n",
       "       MLM  MDR1-MDCKII                                         smiles_std  \\\n",
       "0      NaN          2.0           COc1cccc(Cl)c1NC(=O)N1CCC[C@H](C(N)=O)C1   \n",
       "1      NaN          0.2  O=C(NCC(F)F)[C@H](Nc1cc2c(cc1Br)CNC2)c1cc(Cl)c...   \n",
       "2      NaN          0.5  O=C(NCC(F)F)[C@H](Nc1ccc2c(c1)CNC2)c1cc(Br)cc2...   \n",
       "3      NaN          8.5            NC(=O)[C@H]1CCCN(C(=O)Cc2cccc3occc23)C1   \n",
       "4      NaN          0.9              Cc1cc(CC(=O)N2CCC[C@H](C(N)=O)C2)ccn1   \n",
       "..     ...          ...                                                ...   \n",
       "399   63.0          NaN                    CC(C)NC[C@H](O)COc1cccc2ccccc12   \n",
       "400  386.0          NaN                     O=C(O)Cc1ccccc1Nc1c(Cl)cccc1Cl   \n",
       "401    NaN          NaN              NCc1cc(Cl)cc(C(=O)Nc2ccc3c(c2)CNC3)c1   \n",
       "402    NaN          NaN            COC(=O)Nc1nc2ccc(C(=O)c3ccccc3)cc2[nH]1   \n",
       "403    NaN          NaN              Cc1nccn1C[C@H]1CCc2c(c3ccccc3n2C)C1=O   \n",
       "\n",
       "                                          cxsmiles_std  mol_idx smiles_ext  \\\n",
       "0      COc1cccc(Cl)c1NC(=O)N1CCC[C@H](C(N)=O)C1 |a:16|      191     |a:16|   \n",
       "1    O=C(NCC(F)F)[C@H](Nc1cc2c(cc1Br)CNC2)c1cc(Cl)c...      335     |&1:7|   \n",
       "2    O=C(NCC(F)F)[C@H](Nc1ccc2c(c1)CNC2)c1cc(Br)cc2...      336     |&1:7|   \n",
       "3       NC(=O)[C@H]1CCCN(C(=O)Cc2cccc3occc23)C1 |&1:3|      300     |&1:3|   \n",
       "4        Cc1cc(CC(=O)N2CCC[C@H](C(N)=O)C2)ccn1 |&1:11|      249    |&1:11|   \n",
       "..                                                 ...      ...        ...   \n",
       "399             CC(C)NC[C@H](O)COc1cccc2ccccc12 |&1:5|       22     |&1:5|   \n",
       "400                     O=C(O)Cc1ccccc1Nc1c(Cl)cccc1Cl      380        NaN   \n",
       "401              NCc1cc(Cl)cc(C(=O)Nc2ccc3c(c2)CNC3)c1      303        NaN   \n",
       "402            COC(=O)Nc1nc2ccc(C(=O)c3ccccc3)cc2[nH]1      166        NaN   \n",
       "403       Cc1nccn1C[C@H]1CCc2c(c3ccccc3n2C)C1=O |&1:7|      268     |&1:7|   \n",
       "\n",
       "       LogHLM    LogMLM   LogKSOL  LogMDR1-MDCKII  split  n_isomers  \n",
       "0         NaN       NaN       NaN        0.477121    val          1  \n",
       "1         NaN       NaN  2.523746        0.079181  train          2  \n",
       "2         NaN       NaN       NaN        0.176091  train          2  \n",
       "3         NaN       NaN  2.576341        0.977724  train          2  \n",
       "4         NaN       NaN  2.575188        0.278754  train          2  \n",
       "..        ...       ...       ...             ...    ...        ...  \n",
       "399  1.423246  1.806180       NaN             NaN    val          2  \n",
       "400  2.336460  2.587711       NaN             NaN    val          1  \n",
       "401       NaN       NaN       NaN             NaN  train          1  \n",
       "402       NaN       NaN       NaN             NaN  train          1  \n",
       "403       NaN       NaN  2.107210             NaN  train          2  \n",
       "\n",
       "[404 rows x 16 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_df = calculate_num_isomers(input_df)\n",
    "input_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "      <th>HLM</th>\n",
       "      <th>KSOL</th>\n",
       "      <th>LogD</th>\n",
       "      <th>MLM</th>\n",
       "      <th>MDR1-MDCKII</th>\n",
       "      <th>smiles_std</th>\n",
       "      <th>cxsmiles_std</th>\n",
       "      <th>mol_idx</th>\n",
       "      <th>smiles_ext</th>\n",
       "      <th>LogHLM</th>\n",
       "      <th>LogMLM</th>\n",
       "      <th>LogKSOL</th>\n",
       "      <th>LogMDR1-MDCKII</th>\n",
       "      <th>split</th>\n",
       "      <th>n_isomers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [smiles, HLM, KSOL, LogD, MLM, MDR1-MDCKII, smiles_std, cxsmiles_std, mol_idx, smiles_ext, LogHLM, LogMLM, LogKSOL, LogMDR1-MDCKII, split, n_isomers]\n",
       "Index: []"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_df[(input_df[\"n_isomers\"] > 1) & (input_df[\"smiles_ext\"].isna())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "      <th>HLM</th>\n",
       "      <th>KSOL</th>\n",
       "      <th>LogD</th>\n",
       "      <th>MLM</th>\n",
       "      <th>MDR1-MDCKII</th>\n",
       "      <th>smiles_std</th>\n",
       "      <th>cxsmiles_std</th>\n",
       "      <th>mol_idx</th>\n",
       "      <th>smiles_ext</th>\n",
       "      <th>LogHLM</th>\n",
       "      <th>LogMLM</th>\n",
       "      <th>LogKSOL</th>\n",
       "      <th>LogMDR1-MDCKII</th>\n",
       "      <th>split</th>\n",
       "      <th>n_isomers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [smiles, HLM, KSOL, LogD, MLM, MDR1-MDCKII, smiles_std, cxsmiles_std, mol_idx, smiles_ext, LogHLM, LogMLM, LogKSOL, LogMDR1-MDCKII, split, n_isomers]\n",
       "Index: []"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = input_df[~input_df[\"smiles_ext\"].isna()]\n",
    "tmp[(tmp[\"n_isomers\"] > 1) & (tmp[\"smiles_ext\"].str.contains(\"a\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "n_isomers\n",
       "2    170\n",
       "1     20\n",
       "4      9\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp[\"n_isomers\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "      <th>HLM</th>\n",
       "      <th>KSOL</th>\n",
       "      <th>LogD</th>\n",
       "      <th>MLM</th>\n",
       "      <th>MDR1-MDCKII</th>\n",
       "      <th>smiles_std</th>\n",
       "      <th>cxsmiles_std</th>\n",
       "      <th>mol_idx</th>\n",
       "      <th>smiles_ext</th>\n",
       "      <th>LogHLM</th>\n",
       "      <th>LogMLM</th>\n",
       "      <th>LogKSOL</th>\n",
       "      <th>LogMDR1-MDCKII</th>\n",
       "      <th>split</th>\n",
       "      <th>n_isomers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>COC1=CC=CC(Cl)=C1NC(=O)N1CCC[C@H](C(N)=O)C1 |a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.000</td>\n",
       "      <td>COc1cccc(Cl)c1NC(=O)N1CCC[C@H](C(N)=O)C1</td>\n",
       "      <td>COc1cccc(Cl)c1NC(=O)N1CCC[C@H](C(N)=O)C1 |a:16|</td>\n",
       "      <td>191</td>\n",
       "      <td>|a:16|</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.477121</td>\n",
       "      <td>val</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>O=C(CC1=CN=CC2=CC=CC=C12)N1CC[C@@H](OC2=CC=C(F...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>324.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21.700</td>\n",
       "      <td>O=C(Cc1cncc2ccccc12)N1CC[C@@H](Oc2ccc(F)cc2)C1</td>\n",
       "      <td>O=C(Cc1cncc2ccccc12)N1CC[C@@H](Oc2ccc(F)cc2)C1...</td>\n",
       "      <td>326</td>\n",
       "      <td>|a:16|</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.511883</td>\n",
       "      <td>1.356026</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>CC1=CC(C2=NOC(C(F)(F)F)=N2)=CC=C1OCCCC1=CC(C(=...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2.090</td>\n",
       "      <td>Cc1cc(-c2noc(C(F)(F)F)n2)ccc1OCCCc1cc(C(=O)N2C...</td>\n",
       "      <td>Cc1cc(-c2noc(C(F)(F)F)n2)ccc1OCCCc1cc(C(=O)N2C...</td>\n",
       "      <td>231</td>\n",
       "      <td>|a:28|</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.146128</td>\n",
       "      <td>1.113943</td>\n",
       "      <td>0.489958</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>CC1=CC(C2=NOC(C(F)(F)F)=N2)=CC=C1OCCCC1=CC(C(=...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>12.3</td>\n",
       "      <td>2.470</td>\n",
       "      <td>Cc1cc(-c2noc(C(F)(F)F)n2)ccc1OCCCc1cc(C(=O)N2C...</td>\n",
       "      <td>Cc1cc(-c2noc(C(F)(F)F)n2)ccc1OCCCc1cc(C(=O)N2C...</td>\n",
       "      <td>232</td>\n",
       "      <td>|a:28|</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.123852</td>\n",
       "      <td>1.146128</td>\n",
       "      <td>0.540329</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>O=C1NC(=O)[C@@H](CC2=CC=C3OCOC3=C2)C2=CC=CC=C1...</td>\n",
       "      <td>32.4</td>\n",
       "      <td>263.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>82.7</td>\n",
       "      <td>6.560</td>\n",
       "      <td>O=C1NC(=O)[C@@H](Cc2ccc3c(c2)OCO3)c2ccccc21</td>\n",
       "      <td>O=C1NC(=O)[C@@H](Cc2ccc3c(c2)OCO3)c2ccccc21 |a:5|</td>\n",
       "      <td>391</td>\n",
       "      <td>|a:5|</td>\n",
       "      <td>1.523746</td>\n",
       "      <td>1.922725</td>\n",
       "      <td>2.421604</td>\n",
       "      <td>0.878522</td>\n",
       "      <td>val</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>NCC1=CC=CC(NC(=O)[C@@H](NC(=O)OCC2=CC=CC=C2)C2...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.530</td>\n",
       "      <td>NCc1cccc(NC(=O)[C@@H](NC(=O)OCc2ccccc2)c2ccc(O...</td>\n",
       "      <td>NCc1cccc(NC(=O)[C@@H](NC(=O)OCc2ccccc2)c2ccc(O...</td>\n",
       "      <td>308</td>\n",
       "      <td>|a:10|</td>\n",
       "      <td>1.079181</td>\n",
       "      <td>0.301030</td>\n",
       "      <td>1.380211</td>\n",
       "      <td>0.403121</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>C[C@H]1CN(C2=CN=CC3=CC=CC=C23)C(=O)[C@@]12CN(C...</td>\n",
       "      <td>19.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.2</td>\n",
       "      <td>57.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>C[C@H]1CN(c2cncc3ccccc23)C(=O)[C@@]12CN(Cc1ccn...</td>\n",
       "      <td>C[C@H]1CN(c2cncc3ccccc23)C(=O)[C@@]12CN(Cc1ccn...</td>\n",
       "      <td>214</td>\n",
       "      <td>|a:1,16|</td>\n",
       "      <td>1.301030</td>\n",
       "      <td>1.763428</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.301030</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>CNC(=O)CN1C[C@@]2(C(=O)N(C3=CN=CC4=CC=CC=C34)C...</td>\n",
       "      <td>70.0</td>\n",
       "      <td>390.0</td>\n",
       "      <td>2.1</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0.500</td>\n",
       "      <td>CNC(=O)CN1C[C@@]2(C(=O)N(c3cncc4ccccc34)C[C@@H...</td>\n",
       "      <td>CNC(=O)CN1C[C@@]2(C(=O)N(c3cncc4ccccc34)C[C@@H...</td>\n",
       "      <td>115</td>\n",
       "      <td>|a:7,22|</td>\n",
       "      <td>1.851258</td>\n",
       "      <td>1.880814</td>\n",
       "      <td>2.592177</td>\n",
       "      <td>0.176091</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>CNC(=O)CN1C[C@@]2(C(=O)N(C3=CN=CC4=CC=CC=C34)C...</td>\n",
       "      <td>39.0</td>\n",
       "      <td>397.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>77.0</td>\n",
       "      <td>0.600</td>\n",
       "      <td>CNC(=O)CN1C[C@@]2(C(=O)N(c3cncc4ccccc34)C[C@@H...</td>\n",
       "      <td>CNC(=O)CN1C[C@@]2(C(=O)N(c3cncc4ccccc34)C[C@@H...</td>\n",
       "      <td>120</td>\n",
       "      <td>|a:7,22|</td>\n",
       "      <td>1.602060</td>\n",
       "      <td>1.892095</td>\n",
       "      <td>2.599883</td>\n",
       "      <td>0.204120</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>C[C@H]1CN(C2=CN=CC3=CC=CC=C23)C(=O)[C@@]12CN(C...</td>\n",
       "      <td>237.0</td>\n",
       "      <td>376.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>237.0</td>\n",
       "      <td>1.330</td>\n",
       "      <td>C[C@H]1CN(c2cncc3ccccc23)C(=O)[C@@]12CN(CCN1CC...</td>\n",
       "      <td>C[C@H]1CN(c2cncc3ccccc23)C(=O)[C@@]12CN(CCN1CC...</td>\n",
       "      <td>206</td>\n",
       "      <td>|a:1,16|</td>\n",
       "      <td>2.376577</td>\n",
       "      <td>2.376577</td>\n",
       "      <td>2.576341</td>\n",
       "      <td>0.367356</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>O=C1C2=CC=C(Cl)C=C2[C@@]2(CN1CCN1CCOCC1)C(=O)N...</td>\n",
       "      <td>108.0</td>\n",
       "      <td>382.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>133.0</td>\n",
       "      <td>1.170</td>\n",
       "      <td>O=C1c2ccc(Cl)cc2[C@@]2(CN1CCN1CCOCC1)C(=O)N(c1...</td>\n",
       "      <td>O=C1c2ccc(Cl)cc2[C@@]2(CN1CCN1CCOCC1)C(=O)N(c1...</td>\n",
       "      <td>395</td>\n",
       "      <td>|a:9,34|</td>\n",
       "      <td>2.037426</td>\n",
       "      <td>2.127105</td>\n",
       "      <td>2.583199</td>\n",
       "      <td>0.336460</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>CNC(=O)CN1C[C@@]2(C(=O)N(C3=CN=CC4=CC=CC=C34)C...</td>\n",
       "      <td>17.0</td>\n",
       "      <td>383.0</td>\n",
       "      <td>2.1</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.600</td>\n",
       "      <td>CNC(=O)CN1C[C@@]2(C(=O)N(c3cncc4ccccc34)C[C@@H...</td>\n",
       "      <td>CNC(=O)CN1C[C@@]2(C(=O)N(c3cncc4ccccc34)C[C@@H...</td>\n",
       "      <td>122</td>\n",
       "      <td>|a:7,22|</td>\n",
       "      <td>1.255273</td>\n",
       "      <td>1.531479</td>\n",
       "      <td>2.584331</td>\n",
       "      <td>0.204120</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>CNC(=O)CN1C[C@@]2(C(=O)N(C3=CN=CC4=CC=CC=C34)C...</td>\n",
       "      <td>83.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.3</td>\n",
       "      <td>69.0</td>\n",
       "      <td>0.142</td>\n",
       "      <td>CNC(=O)CN1C[C@@]2(C(=O)N(c3cncc4ccccc34)C[C@@H...</td>\n",
       "      <td>CNC(=O)CN1C[C@@]2(C(=O)N(c3cncc4ccccc34)C[C@@H...</td>\n",
       "      <td>124</td>\n",
       "      <td>|a:7,22|</td>\n",
       "      <td>1.924279</td>\n",
       "      <td>1.845098</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.057666</td>\n",
       "      <td>val</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>CC1=NOC(CN2C[C@@]3(C(=O)N(C4=CN=CC5=CC=CC=C45)...</td>\n",
       "      <td>80.0</td>\n",
       "      <td>392.0</td>\n",
       "      <td>2.1</td>\n",
       "      <td>240.0</td>\n",
       "      <td>12.500</td>\n",
       "      <td>Cc1cc(CN2C[C@@]3(C(=O)N(c4cncc5ccccc45)C[C@@H]...</td>\n",
       "      <td>Cc1cc(CN2C[C@@]3(C(=O)N(c4cncc5ccccc45)C[C@@H]...</td>\n",
       "      <td>251</td>\n",
       "      <td>|a:8,23|</td>\n",
       "      <td>1.908485</td>\n",
       "      <td>2.382017</td>\n",
       "      <td>2.594393</td>\n",
       "      <td>1.130334</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>C[C@H]1CN(C2=CN=CC3=CC=CC=C23)C(=O)[C@@]12CN(C...</td>\n",
       "      <td>58.0</td>\n",
       "      <td>372.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>172.0</td>\n",
       "      <td>3.020</td>\n",
       "      <td>C[C@H]1CN(c2cncc3ccccc23)C(=O)[C@@]12CN(Cc1nnn...</td>\n",
       "      <td>C[C@H]1CN(c2cncc3ccccc23)C(=O)[C@@]12CN(Cc1nnn...</td>\n",
       "      <td>220</td>\n",
       "      <td>|a:1,16|</td>\n",
       "      <td>1.770852</td>\n",
       "      <td>2.238046</td>\n",
       "      <td>2.571709</td>\n",
       "      <td>0.604226</td>\n",
       "      <td>val</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>C[C@H]1CN(C2=CN=CC3=CC=CC=C23)C(=O)[C@@]12CN(C...</td>\n",
       "      <td>195.0</td>\n",
       "      <td>377.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>351.0</td>\n",
       "      <td>6.700</td>\n",
       "      <td>C[C@H]1CN(c2cncc3ccccc23)C(=O)[C@@]12CN(Cc1ncc...</td>\n",
       "      <td>C[C@H]1CN(c2cncc3ccccc23)C(=O)[C@@]12CN(Cc1ncc...</td>\n",
       "      <td>219</td>\n",
       "      <td>|a:1,16|</td>\n",
       "      <td>2.292256</td>\n",
       "      <td>2.546543</td>\n",
       "      <td>2.577492</td>\n",
       "      <td>0.886491</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>C[C@H]1CN(C2=CN=CC3=CC=CC=C23)C(=O)[C@@]12CN(C...</td>\n",
       "      <td>85.0</td>\n",
       "      <td>375.0</td>\n",
       "      <td>1.7</td>\n",
       "      <td>84.0</td>\n",
       "      <td>8.260</td>\n",
       "      <td>C[C@H]1CN(c2cncc3ccccc23)C(=O)[C@@]12CN(CC#N)C...</td>\n",
       "      <td>C[C@H]1CN(c2cncc3ccccc23)C(=O)[C@@]12CN(CC#N)C...</td>\n",
       "      <td>200</td>\n",
       "      <td>|a:1,16|</td>\n",
       "      <td>1.934498</td>\n",
       "      <td>1.929419</td>\n",
       "      <td>2.575188</td>\n",
       "      <td>0.966611</td>\n",
       "      <td>val</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>CNC(=O)CN1C[C@@]2(C(=O)N(C3=CN=CC4=CC=CC=C34)C...</td>\n",
       "      <td>44.0</td>\n",
       "      <td>387.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>46.0</td>\n",
       "      <td>0.290</td>\n",
       "      <td>CNC(=O)CN1C[C@@]2(C(=O)N(c3cncc4ccccc34)C[C@@H...</td>\n",
       "      <td>CNC(=O)CN1C[C@@]2(C(=O)N(c3cncc4ccccc34)C[C@@H...</td>\n",
       "      <td>130</td>\n",
       "      <td>|a:7,22|</td>\n",
       "      <td>1.653213</td>\n",
       "      <td>1.672098</td>\n",
       "      <td>2.588832</td>\n",
       "      <td>0.110590</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>CNC(=O)CN1C[C@@]2(C(=O)N(C3=CN=CC4=CC=CC=C34)C...</td>\n",
       "      <td>26.0</td>\n",
       "      <td>395.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>90.0</td>\n",
       "      <td>0.371</td>\n",
       "      <td>CNC(=O)CN1C[C@@]2(C(=O)N(c3cncc4ccccc34)C[C@@H...</td>\n",
       "      <td>CNC(=O)CN1C[C@@]2(C(=O)N(c3cncc4ccccc34)C[C@@H...</td>\n",
       "      <td>118</td>\n",
       "      <td>|a:7,22|</td>\n",
       "      <td>1.431364</td>\n",
       "      <td>1.959041</td>\n",
       "      <td>2.597695</td>\n",
       "      <td>0.137037</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>C=C[C@H]1CN2CC[C@H]1C[C@@H]2[C@@H](O)C1=CC=NC2...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.900</td>\n",
       "      <td>C=C[C@H]1CN2CC[C@H]1C[C@@H]2[C@@H](O)c1ccnc2cc...</td>\n",
       "      <td>C=C[C@H]1CN2CC[C@H]1C[C@@H]2[C@@H](O)c1ccnc2cc...</td>\n",
       "      <td>10</td>\n",
       "      <td>|a:2,7,9,10|</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.173186</td>\n",
       "      <td>val</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                smiles    HLM   KSOL  LogD  \\\n",
       "0    COC1=CC=CC(Cl)=C1NC(=O)N1CCC[C@H](C(N)=O)C1 |a...    NaN    NaN   0.3   \n",
       "51   O=C(CC1=CN=CC2=CC=CC=C12)N1CC[C@@H](OC2=CC=C(F...    NaN  324.0   NaN   \n",
       "175  CC1=CC(C2=NOC(C(F)(F)F)=N2)=CC=C1OCCCC1=CC(C(=...    NaN   12.0   4.4   \n",
       "193  CC1=CC(C2=NOC(C(F)(F)F)=N2)=CC=C1OCCCC1=CC(C(=...    NaN   13.0   4.4   \n",
       "195  O=C1NC(=O)[C@@H](CC2=CC=C3OCOC3=C2)C2=CC=CC=C1...   32.4  263.0   2.4   \n",
       "225  NCC1=CC=CC(NC(=O)[C@@H](NC(=O)OCC2=CC=CC=C2)C2...   11.0   23.0   3.2   \n",
       "241  C[C@H]1CN(C2=CN=CC3=CC=CC=C23)C(=O)[C@@]12CN(C...   19.0    NaN   1.2   \n",
       "242  CNC(=O)CN1C[C@@]2(C(=O)N(C3=CN=CC4=CC=CC=C34)C...   70.0  390.0   2.1   \n",
       "243  CNC(=O)CN1C[C@@]2(C(=O)N(C3=CN=CC4=CC=CC=C34)C...   39.0  397.0   1.8   \n",
       "244  C[C@H]1CN(C2=CN=CC3=CC=CC=C23)C(=O)[C@@]12CN(C...  237.0  376.0   1.8   \n",
       "245  O=C1C2=CC=C(Cl)C=C2[C@@]2(CN1CCN1CCOCC1)C(=O)N...  108.0  382.0   1.9   \n",
       "247  CNC(=O)CN1C[C@@]2(C(=O)N(C3=CN=CC4=CC=CC=C34)C...   17.0  383.0   2.1   \n",
       "250  CNC(=O)CN1C[C@@]2(C(=O)N(C3=CN=CC4=CC=CC=C34)C...   83.0    NaN   1.3   \n",
       "252  CC1=NOC(CN2C[C@@]3(C(=O)N(C4=CN=CC5=CC=CC=C45)...   80.0  392.0   2.1   \n",
       "253  C[C@H]1CN(C2=CN=CC3=CC=CC=C23)C(=O)[C@@]12CN(C...   58.0  372.0   1.5   \n",
       "254  C[C@H]1CN(C2=CN=CC3=CC=CC=C23)C(=O)[C@@]12CN(C...  195.0  377.0   2.0   \n",
       "255  C[C@H]1CN(C2=CN=CC3=CC=CC=C23)C(=O)[C@@]12CN(C...   85.0  375.0   1.7   \n",
       "256  CNC(=O)CN1C[C@@]2(C(=O)N(C3=CN=CC4=CC=CC=C34)C...   44.0  387.0   1.1   \n",
       "257  CNC(=O)CN1C[C@@]2(C(=O)N(C3=CN=CC4=CC=CC=C34)C...   26.0  395.0   1.9   \n",
       "380  C=C[C@H]1CN2CC[C@H]1C[C@@H]2[C@@H](O)C1=CC=NC2...    NaN    NaN   NaN   \n",
       "\n",
       "       MLM  MDR1-MDCKII                                         smiles_std  \\\n",
       "0      NaN        2.000           COc1cccc(Cl)c1NC(=O)N1CCC[C@H](C(N)=O)C1   \n",
       "51     NaN       21.700     O=C(Cc1cncc2ccccc12)N1CC[C@@H](Oc2ccc(F)cc2)C1   \n",
       "175   13.0        2.090  Cc1cc(-c2noc(C(F)(F)F)n2)ccc1OCCCc1cc(C(=O)N2C...   \n",
       "193   12.3        2.470  Cc1cc(-c2noc(C(F)(F)F)n2)ccc1OCCCc1cc(C(=O)N2C...   \n",
       "195   82.7        6.560        O=C1NC(=O)[C@@H](Cc2ccc3c(c2)OCO3)c2ccccc21   \n",
       "225    1.0        1.530  NCc1cccc(NC(=O)[C@@H](NC(=O)OCc2ccccc2)c2ccc(O...   \n",
       "241   57.0        1.000  C[C@H]1CN(c2cncc3ccccc23)C(=O)[C@@]12CN(Cc1ccn...   \n",
       "242   75.0        0.500  CNC(=O)CN1C[C@@]2(C(=O)N(c3cncc4ccccc34)C[C@@H...   \n",
       "243   77.0        0.600  CNC(=O)CN1C[C@@]2(C(=O)N(c3cncc4ccccc34)C[C@@H...   \n",
       "244  237.0        1.330  C[C@H]1CN(c2cncc3ccccc23)C(=O)[C@@]12CN(CCN1CC...   \n",
       "245  133.0        1.170  O=C1c2ccc(Cl)cc2[C@@]2(CN1CCN1CCOCC1)C(=O)N(c1...   \n",
       "247   33.0        0.600  CNC(=O)CN1C[C@@]2(C(=O)N(c3cncc4ccccc34)C[C@@H...   \n",
       "250   69.0        0.142  CNC(=O)CN1C[C@@]2(C(=O)N(c3cncc4ccccc34)C[C@@H...   \n",
       "252  240.0       12.500  Cc1cc(CN2C[C@@]3(C(=O)N(c4cncc5ccccc45)C[C@@H]...   \n",
       "253  172.0        3.020  C[C@H]1CN(c2cncc3ccccc23)C(=O)[C@@]12CN(Cc1nnn...   \n",
       "254  351.0        6.700  C[C@H]1CN(c2cncc3ccccc23)C(=O)[C@@]12CN(Cc1ncc...   \n",
       "255   84.0        8.260  C[C@H]1CN(c2cncc3ccccc23)C(=O)[C@@]12CN(CC#N)C...   \n",
       "256   46.0        0.290  CNC(=O)CN1C[C@@]2(C(=O)N(c3cncc4ccccc34)C[C@@H...   \n",
       "257   90.0        0.371  CNC(=O)CN1C[C@@]2(C(=O)N(c3cncc4ccccc34)C[C@@H...   \n",
       "380    NaN       13.900  C=C[C@H]1CN2CC[C@H]1C[C@@H]2[C@@H](O)c1ccnc2cc...   \n",
       "\n",
       "                                          cxsmiles_std  mol_idx    smiles_ext  \\\n",
       "0      COc1cccc(Cl)c1NC(=O)N1CCC[C@H](C(N)=O)C1 |a:16|      191        |a:16|   \n",
       "51   O=C(Cc1cncc2ccccc12)N1CC[C@@H](Oc2ccc(F)cc2)C1...      326        |a:16|   \n",
       "175  Cc1cc(-c2noc(C(F)(F)F)n2)ccc1OCCCc1cc(C(=O)N2C...      231        |a:28|   \n",
       "193  Cc1cc(-c2noc(C(F)(F)F)n2)ccc1OCCCc1cc(C(=O)N2C...      232        |a:28|   \n",
       "195  O=C1NC(=O)[C@@H](Cc2ccc3c(c2)OCO3)c2ccccc21 |a:5|      391         |a:5|   \n",
       "225  NCc1cccc(NC(=O)[C@@H](NC(=O)OCc2ccccc2)c2ccc(O...      308        |a:10|   \n",
       "241  C[C@H]1CN(c2cncc3ccccc23)C(=O)[C@@]12CN(Cc1ccn...      214      |a:1,16|   \n",
       "242  CNC(=O)CN1C[C@@]2(C(=O)N(c3cncc4ccccc34)C[C@@H...      115      |a:7,22|   \n",
       "243  CNC(=O)CN1C[C@@]2(C(=O)N(c3cncc4ccccc34)C[C@@H...      120      |a:7,22|   \n",
       "244  C[C@H]1CN(c2cncc3ccccc23)C(=O)[C@@]12CN(CCN1CC...      206      |a:1,16|   \n",
       "245  O=C1c2ccc(Cl)cc2[C@@]2(CN1CCN1CCOCC1)C(=O)N(c1...      395      |a:9,34|   \n",
       "247  CNC(=O)CN1C[C@@]2(C(=O)N(c3cncc4ccccc34)C[C@@H...      122      |a:7,22|   \n",
       "250  CNC(=O)CN1C[C@@]2(C(=O)N(c3cncc4ccccc34)C[C@@H...      124      |a:7,22|   \n",
       "252  Cc1cc(CN2C[C@@]3(C(=O)N(c4cncc5ccccc45)C[C@@H]...      251      |a:8,23|   \n",
       "253  C[C@H]1CN(c2cncc3ccccc23)C(=O)[C@@]12CN(Cc1nnn...      220      |a:1,16|   \n",
       "254  C[C@H]1CN(c2cncc3ccccc23)C(=O)[C@@]12CN(Cc1ncc...      219      |a:1,16|   \n",
       "255  C[C@H]1CN(c2cncc3ccccc23)C(=O)[C@@]12CN(CC#N)C...      200      |a:1,16|   \n",
       "256  CNC(=O)CN1C[C@@]2(C(=O)N(c3cncc4ccccc34)C[C@@H...      130      |a:7,22|   \n",
       "257  CNC(=O)CN1C[C@@]2(C(=O)N(c3cncc4ccccc34)C[C@@H...      118      |a:7,22|   \n",
       "380  C=C[C@H]1CN2CC[C@H]1C[C@@H]2[C@@H](O)c1ccnc2cc...       10  |a:2,7,9,10|   \n",
       "\n",
       "       LogHLM    LogMLM   LogKSOL  LogMDR1-MDCKII  split  n_isomers  \n",
       "0         NaN       NaN       NaN        0.477121    val          1  \n",
       "51        NaN       NaN  2.511883        1.356026  train          1  \n",
       "175       NaN  1.146128  1.113943        0.489958  train          1  \n",
       "193       NaN  1.123852  1.146128        0.540329  train          1  \n",
       "195  1.523746  1.922725  2.421604        0.878522    val          1  \n",
       "225  1.079181  0.301030  1.380211        0.403121  train          1  \n",
       "241  1.301030  1.763428       NaN        0.301030  train          1  \n",
       "242  1.851258  1.880814  2.592177        0.176091  train          1  \n",
       "243  1.602060  1.892095  2.599883        0.204120  train          1  \n",
       "244  2.376577  2.376577  2.576341        0.367356  train          1  \n",
       "245  2.037426  2.127105  2.583199        0.336460  train          1  \n",
       "247  1.255273  1.531479  2.584331        0.204120  train          1  \n",
       "250  1.924279  1.845098       NaN        0.057666    val          1  \n",
       "252  1.908485  2.382017  2.594393        1.130334  train          1  \n",
       "253  1.770852  2.238046  2.571709        0.604226    val          1  \n",
       "254  2.292256  2.546543  2.577492        0.886491  train          1  \n",
       "255  1.934498  1.929419  2.575188        0.966611    val          1  \n",
       "256  1.653213  1.672098  2.588832        0.110590  train          1  \n",
       "257  1.431364  1.959041  2.597695        0.137037  train          1  \n",
       "380       NaN       NaN       NaN        1.173186    val          1  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp[tmp[\"smiles_ext\"].str.contains(\"a\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enumerate_isomers(input_df: pd.DataFrame):\n",
    "    input_df = input_df.copy()\n",
    "    # we don't want to find isomers for compounds with absolute stereochemistry\n",
    "    # so exlude them and just leave the original compound as the only isomer\n",
    "    input_df[\"cxsmiles_std_isomer\"] = input_df[\"cxsmiles_std\"]\n",
    "    input_df.loc[input_df[\"smiles_ext\"].isna(), \"smiles_ext\"] = \"\"\n",
    "    stereo_impure_mask = ~input_df[\"smiles_ext\"].isna() & ~input_df[\"smiles_ext\"].str.contains(\"a\")\n",
    "    input_df.loc[stereo_impure_mask, \"cxsmiles_std_isomer\"] = \\\n",
    "        input_df.loc[stereo_impure_mask, \"cxsmiles_std\"].apply(lambda x: get_all_isomers(x))\n",
    "    \n",
    "    input_df = input_df.explode(\"cxsmiles_std_isomer\")\n",
    "    \n",
    "    return input_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "      <th>HLM</th>\n",
       "      <th>KSOL</th>\n",
       "      <th>LogD</th>\n",
       "      <th>MLM</th>\n",
       "      <th>MDR1-MDCKII</th>\n",
       "      <th>smiles_std</th>\n",
       "      <th>cxsmiles_std</th>\n",
       "      <th>mol_idx</th>\n",
       "      <th>smiles_ext</th>\n",
       "      <th>LogHLM</th>\n",
       "      <th>LogMLM</th>\n",
       "      <th>LogKSOL</th>\n",
       "      <th>LogMDR1-MDCKII</th>\n",
       "      <th>split</th>\n",
       "      <th>n_isomers</th>\n",
       "      <th>cxsmiles_std_isomer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>COC1=CC=CC(Cl)=C1NC(=O)N1CCC[C@H](C(N)=O)C1 |a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>COc1cccc(Cl)c1NC(=O)N1CCC[C@H](C(N)=O)C1</td>\n",
       "      <td>COc1cccc(Cl)c1NC(=O)N1CCC[C@H](C(N)=O)C1 |a:16|</td>\n",
       "      <td>191</td>\n",
       "      <td>|a:16|</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.477121</td>\n",
       "      <td>val</td>\n",
       "      <td>1</td>\n",
       "      <td>COc1cccc(Cl)c1NC(=O)N1CCC[C@H](C(N)=O)C1 |a:16|</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>O=C(NCC(F)F)[C@H](NC1=CC2=C(C=C1Br)CNC2)C1=CC(...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>333.0</td>\n",
       "      <td>2.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2</td>\n",
       "      <td>O=C(NCC(F)F)[C@H](Nc1cc2c(cc1Br)CNC2)c1cc(Cl)c...</td>\n",
       "      <td>O=C(NCC(F)F)[C@H](Nc1cc2c(cc1Br)CNC2)c1cc(Cl)c...</td>\n",
       "      <td>335</td>\n",
       "      <td>|&amp;1:7|</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.523746</td>\n",
       "      <td>0.079181</td>\n",
       "      <td>train</td>\n",
       "      <td>2</td>\n",
       "      <td>O=C(NCC(F)F)[C@@H](Nc1cc2c(cc1Br)CNC2)c1cc(Cl)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>O=C(NCC(F)F)[C@H](NC1=CC2=C(C=C1Br)CNC2)C1=CC(...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>333.0</td>\n",
       "      <td>2.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2</td>\n",
       "      <td>O=C(NCC(F)F)[C@H](Nc1cc2c(cc1Br)CNC2)c1cc(Cl)c...</td>\n",
       "      <td>O=C(NCC(F)F)[C@H](Nc1cc2c(cc1Br)CNC2)c1cc(Cl)c...</td>\n",
       "      <td>335</td>\n",
       "      <td>|&amp;1:7|</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.523746</td>\n",
       "      <td>0.079181</td>\n",
       "      <td>train</td>\n",
       "      <td>2</td>\n",
       "      <td>O=C(NCC(F)F)[C@H](Nc1cc2c(cc1Br)CNC2)c1cc(Cl)c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>O=C(NCC(F)F)[C@H](NC1=CC=C2CNCC2=C1)C1=CC(Br)=...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5</td>\n",
       "      <td>O=C(NCC(F)F)[C@H](Nc1ccc2c(c1)CNC2)c1cc(Br)cc2...</td>\n",
       "      <td>O=C(NCC(F)F)[C@H](Nc1ccc2c(c1)CNC2)c1cc(Br)cc2...</td>\n",
       "      <td>336</td>\n",
       "      <td>|&amp;1:7|</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.176091</td>\n",
       "      <td>train</td>\n",
       "      <td>2</td>\n",
       "      <td>O=C(NCC(F)F)[C@@H](Nc1ccc2c(c1)CNC2)c1cc(Br)cc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>O=C(NCC(F)F)[C@H](NC1=CC=C2CNCC2=C1)C1=CC(Br)=...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5</td>\n",
       "      <td>O=C(NCC(F)F)[C@H](Nc1ccc2c(c1)CNC2)c1cc(Br)cc2...</td>\n",
       "      <td>O=C(NCC(F)F)[C@H](Nc1ccc2c(c1)CNC2)c1cc(Br)cc2...</td>\n",
       "      <td>336</td>\n",
       "      <td>|&amp;1:7|</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.176091</td>\n",
       "      <td>train</td>\n",
       "      <td>2</td>\n",
       "      <td>O=C(NCC(F)F)[C@H](Nc1ccc2c(c1)CNC2)c1cc(Br)cc2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>O=C(O)CC1=CC=CC=C1NC1=C(Cl)C=CC=C1Cl</td>\n",
       "      <td>216.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>386.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>O=C(O)Cc1ccccc1Nc1c(Cl)cccc1Cl</td>\n",
       "      <td>O=C(O)Cc1ccccc1Nc1c(Cl)cccc1Cl</td>\n",
       "      <td>380</td>\n",
       "      <td></td>\n",
       "      <td>2.33646</td>\n",
       "      <td>2.587711</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>val</td>\n",
       "      <td>1</td>\n",
       "      <td>O=C(O)Cc1ccccc1Nc1c(Cl)cccc1Cl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>NCC1=CC(Cl)=CC(C(=O)NC2=CC=C3CNCC3=C2)=C1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NCc1cc(Cl)cc(C(=O)Nc2ccc3c(c2)CNC3)c1</td>\n",
       "      <td>NCc1cc(Cl)cc(C(=O)Nc2ccc3c(c2)CNC3)c1</td>\n",
       "      <td>303</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>NCc1cc(Cl)cc(C(=O)Nc2ccc3c(c2)CNC3)c1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>COC(=O)NC1=NC2=CC=C(C(=O)C3=CC=CC=C3)C=C2N1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>COC(=O)Nc1nc2ccc(C(=O)c3ccccc3)cc2[nH]1</td>\n",
       "      <td>COC(=O)Nc1nc2ccc(C(=O)c3ccccc3)cc2[nH]1</td>\n",
       "      <td>166</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>COC(=O)Nc1nc2ccc(C(=O)c3ccccc3)cc2[nH]1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>CC1=NC=CN1C[C@H]1CCC2=C(C1=O)C1=CC=CC=C1N2C |&amp;...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>127.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cc1nccn1C[C@H]1CCc2c(c3ccccc3n2C)C1=O</td>\n",
       "      <td>Cc1nccn1C[C@H]1CCc2c(c3ccccc3n2C)C1=O |&amp;1:7|</td>\n",
       "      <td>268</td>\n",
       "      <td>|&amp;1:7|</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.107210</td>\n",
       "      <td>NaN</td>\n",
       "      <td>train</td>\n",
       "      <td>2</td>\n",
       "      <td>Cc1nccn1C[C@@H]1CCc2c(c3ccccc3n2C)C1=O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>CC1=NC=CN1C[C@H]1CCC2=C(C1=O)C1=CC=CC=C1N2C |&amp;...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>127.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cc1nccn1C[C@H]1CCc2c(c3ccccc3n2C)C1=O</td>\n",
       "      <td>Cc1nccn1C[C@H]1CCc2c(c3ccccc3n2C)C1=O |&amp;1:7|</td>\n",
       "      <td>268</td>\n",
       "      <td>|&amp;1:7|</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.107210</td>\n",
       "      <td>NaN</td>\n",
       "      <td>train</td>\n",
       "      <td>2</td>\n",
       "      <td>Cc1nccn1C[C@H]1CCc2c(c3ccccc3n2C)C1=O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>601 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                smiles    HLM   KSOL  LogD  \\\n",
       "0    COC1=CC=CC(Cl)=C1NC(=O)N1CCC[C@H](C(N)=O)C1 |a...    NaN    NaN   0.3   \n",
       "1    O=C(NCC(F)F)[C@H](NC1=CC2=C(C=C1Br)CNC2)C1=CC(...    NaN  333.0   2.9   \n",
       "1    O=C(NCC(F)F)[C@H](NC1=CC2=C(C=C1Br)CNC2)C1=CC(...    NaN  333.0   2.9   \n",
       "2    O=C(NCC(F)F)[C@H](NC1=CC=C2CNCC2=C1)C1=CC(Br)=...    NaN    NaN   0.4   \n",
       "2    O=C(NCC(F)F)[C@H](NC1=CC=C2CNCC2=C1)C1=CC(Br)=...    NaN    NaN   0.4   \n",
       "..                                                 ...    ...    ...   ...   \n",
       "400               O=C(O)CC1=CC=CC=C1NC1=C(Cl)C=CC=C1Cl  216.0    NaN   NaN   \n",
       "401          NCC1=CC(Cl)=CC(C(=O)NC2=CC=C3CNCC3=C2)=C1    NaN    NaN   2.0   \n",
       "402        COC(=O)NC1=NC2=CC=C(C(=O)C3=CC=CC=C3)C=C2N1    NaN    NaN   2.9   \n",
       "403  CC1=NC=CN1C[C@H]1CCC2=C(C1=O)C1=CC=CC=C1N2C |&...    NaN  127.0   NaN   \n",
       "403  CC1=NC=CN1C[C@H]1CCC2=C(C1=O)C1=CC=CC=C1N2C |&...    NaN  127.0   NaN   \n",
       "\n",
       "       MLM  MDR1-MDCKII                                         smiles_std  \\\n",
       "0      NaN          2.0           COc1cccc(Cl)c1NC(=O)N1CCC[C@H](C(N)=O)C1   \n",
       "1      NaN          0.2  O=C(NCC(F)F)[C@H](Nc1cc2c(cc1Br)CNC2)c1cc(Cl)c...   \n",
       "1      NaN          0.2  O=C(NCC(F)F)[C@H](Nc1cc2c(cc1Br)CNC2)c1cc(Cl)c...   \n",
       "2      NaN          0.5  O=C(NCC(F)F)[C@H](Nc1ccc2c(c1)CNC2)c1cc(Br)cc2...   \n",
       "2      NaN          0.5  O=C(NCC(F)F)[C@H](Nc1ccc2c(c1)CNC2)c1cc(Br)cc2...   \n",
       "..     ...          ...                                                ...   \n",
       "400  386.0          NaN                     O=C(O)Cc1ccccc1Nc1c(Cl)cccc1Cl   \n",
       "401    NaN          NaN              NCc1cc(Cl)cc(C(=O)Nc2ccc3c(c2)CNC3)c1   \n",
       "402    NaN          NaN            COC(=O)Nc1nc2ccc(C(=O)c3ccccc3)cc2[nH]1   \n",
       "403    NaN          NaN              Cc1nccn1C[C@H]1CCc2c(c3ccccc3n2C)C1=O   \n",
       "403    NaN          NaN              Cc1nccn1C[C@H]1CCc2c(c3ccccc3n2C)C1=O   \n",
       "\n",
       "                                          cxsmiles_std  mol_idx smiles_ext  \\\n",
       "0      COc1cccc(Cl)c1NC(=O)N1CCC[C@H](C(N)=O)C1 |a:16|      191     |a:16|   \n",
       "1    O=C(NCC(F)F)[C@H](Nc1cc2c(cc1Br)CNC2)c1cc(Cl)c...      335     |&1:7|   \n",
       "1    O=C(NCC(F)F)[C@H](Nc1cc2c(cc1Br)CNC2)c1cc(Cl)c...      335     |&1:7|   \n",
       "2    O=C(NCC(F)F)[C@H](Nc1ccc2c(c1)CNC2)c1cc(Br)cc2...      336     |&1:7|   \n",
       "2    O=C(NCC(F)F)[C@H](Nc1ccc2c(c1)CNC2)c1cc(Br)cc2...      336     |&1:7|   \n",
       "..                                                 ...      ...        ...   \n",
       "400                     O=C(O)Cc1ccccc1Nc1c(Cl)cccc1Cl      380              \n",
       "401              NCc1cc(Cl)cc(C(=O)Nc2ccc3c(c2)CNC3)c1      303              \n",
       "402            COC(=O)Nc1nc2ccc(C(=O)c3ccccc3)cc2[nH]1      166              \n",
       "403       Cc1nccn1C[C@H]1CCc2c(c3ccccc3n2C)C1=O |&1:7|      268     |&1:7|   \n",
       "403       Cc1nccn1C[C@H]1CCc2c(c3ccccc3n2C)C1=O |&1:7|      268     |&1:7|   \n",
       "\n",
       "      LogHLM    LogMLM   LogKSOL  LogMDR1-MDCKII  split  n_isomers  \\\n",
       "0        NaN       NaN       NaN        0.477121    val          1   \n",
       "1        NaN       NaN  2.523746        0.079181  train          2   \n",
       "1        NaN       NaN  2.523746        0.079181  train          2   \n",
       "2        NaN       NaN       NaN        0.176091  train          2   \n",
       "2        NaN       NaN       NaN        0.176091  train          2   \n",
       "..       ...       ...       ...             ...    ...        ...   \n",
       "400  2.33646  2.587711       NaN             NaN    val          1   \n",
       "401      NaN       NaN       NaN             NaN  train          1   \n",
       "402      NaN       NaN       NaN             NaN  train          1   \n",
       "403      NaN       NaN  2.107210             NaN  train          2   \n",
       "403      NaN       NaN  2.107210             NaN  train          2   \n",
       "\n",
       "                                   cxsmiles_std_isomer  \n",
       "0      COc1cccc(Cl)c1NC(=O)N1CCC[C@H](C(N)=O)C1 |a:16|  \n",
       "1    O=C(NCC(F)F)[C@@H](Nc1cc2c(cc1Br)CNC2)c1cc(Cl)...  \n",
       "1    O=C(NCC(F)F)[C@H](Nc1cc2c(cc1Br)CNC2)c1cc(Cl)c...  \n",
       "2    O=C(NCC(F)F)[C@@H](Nc1ccc2c(c1)CNC2)c1cc(Br)cc...  \n",
       "2    O=C(NCC(F)F)[C@H](Nc1ccc2c(c1)CNC2)c1cc(Br)cc2...  \n",
       "..                                                 ...  \n",
       "400                     O=C(O)Cc1ccccc1Nc1c(Cl)cccc1Cl  \n",
       "401              NCc1cc(Cl)cc(C(=O)Nc2ccc3c(c2)CNC3)c1  \n",
       "402            COC(=O)Nc1nc2ccc(C(=O)c3ccccc3)cc2[nH]1  \n",
       "403             Cc1nccn1C[C@@H]1CCc2c(c3ccccc3n2C)C1=O  \n",
       "403              Cc1nccn1C[C@H]1CCc2c(c3ccccc3n2C)C1=O  \n",
       "\n",
       "[601 rows x 17 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enumerate_isomers(input_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROPERTY2STD = {\n",
    "    \"LogHLM\": 217.0507262281013,\n",
    "    \"LogMLM\": 318.4791749144294,\n",
    "    \"LogD\": 1.2280921175245259,\n",
    "    \"LogKSOL\": 157.6780176767439,\n",
    "    \"LogMDR1-MDCKII\": 6.433744322214307\n",
    "}\n",
    "\n",
    "def clean_data(input_paths, output_dir):\n",
    "    mols_to_remove = defaultdict(set)\n",
    "\n",
    "    for input_path in input_paths:\n",
    "        print(f\"Training and predicting on {input_path}\")\n",
    "        input_df = pd.read_csv(input_path)\n",
    "        input_df[\"cxsmiles_std_isomer\"] = input_df[\"cxsmiles_std\"]\n",
    "        input_df = pd.concat([\n",
    "            input_df[input_df[\"split\"] == \"train\"],\n",
    "            enumerate_isomers(input_df[input_df[\"split\"] == \"val\"])\n",
    "        ]).reset_index(drop=True)\n",
    "        train_dset, val_dset, pred_dset = prepare_data(input_df, smiles_column_for_pred=\"cxsmiles_std_isomer\")\n",
    "\n",
    "        save_dir = output_dir / f\"{input_path.stem}\"\n",
    "        model = train_model(MODEL_CONFIG, train_dset, val_dset, NUM_WORKERS, save_dir, run_idx=None, enable_logger=False)\n",
    "        preds = predict(model, pred_dset, NUM_WORKERS)\n",
    "\n",
    "        output_df = input_df.copy()\n",
    "        output_df[[\"pred_\" + t for t in TARGET_COLUMNS]] = preds\n",
    "        save_dir.mkdir(parents=True, exist_ok=True)\n",
    "        output_df.to_csv(save_dir / \"predictions.csv\", index=False)\n",
    "\n",
    "        output_val_df = output_df[input_df[\"split\"] == \"val\"].reset_index(drop=True)\n",
    "        for t in TARGET_COLUMNS:\n",
    "            if t in [\"LogHLM\", \"LogMLM\", \"LogKSOL\", \"LogMDR1-MDCKII\"]:\n",
    "                output_val_df[f\"diff_{t}\"] = output_val_df.groupby(\"mol_idx\")[f\"pred_{t}\"].transform(\n",
    "                    lambda x: np.power(10, x.max()) - np.power(10, x.min())\n",
    "                )\n",
    "            else:\n",
    "                output_val_df[f\"diff_{t}\"] = output_val_df.groupby(\"mol_idx\")[f\"pred_{t}\"].transform(\n",
    "                    lambda x: x.max() - x.min()\n",
    "                )\n",
    "            if t == \"LogHLM\":\n",
    "                output_val_df[f\"noisy_{t}\"] = output_val_df[f\"diff_{t}\"] > 0.2 * PROPERTY2STD[t]\n",
    "            elif t == \"LogMLM\":\n",
    "                output_val_df[f\"noisy_{t}\"] = output_val_df[f\"diff_{t}\"] > 0.2 * PROPERTY2STD[t]\n",
    "            elif t == \"LogD\":\n",
    "                output_val_df[f\"noisy_{t}\"] = output_val_df[f\"diff_{t}\"] > 0.2  # absolute error\n",
    "            elif t == \"LogKSOL\":\n",
    "                output_val_df[f\"noisy_{t}\"] = output_val_df[f\"diff_{t}\"] > 0.2 * PROPERTY2STD[t]\n",
    "            elif t == \"LogMDR1-MDCKII\":\n",
    "                output_val_df[f\"noisy_{t}\"] = output_val_df[f\"diff_{t}\"] > 0.2 * PROPERTY2STD[t]\n",
    "            \n",
    "            mols_to_remove[t].update(\n",
    "                output_val_df[output_val_df[f\"noisy_{t}\"]][\"mol_idx\"].tolist()\n",
    "            )\n",
    "\n",
    "    for t in TARGET_COLUMNS:\n",
    "        print(f\"[{input_path.stem}] Removing {len(mols_to_remove[t])} noisy mols out of each trainig set for {t}\")\n",
    "    \n",
    "    for input_path in input_paths:\n",
    "        input_df = pd.read_csv(input_path)\n",
    "        for t in TARGET_COLUMNS:\n",
    "            input_df.loc[\n",
    "                input_df[\"mol_idx\"].isin(mols_to_remove[t]) & \\\n",
    "                    (input_df[\"split\"] == \"train\"),\n",
    "                t\n",
    "            ] = np.nan\n",
    "\n",
    "        input_df.to_csv(output_dir / input_path.name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_paths = [Path(f'../data/asap/datasets/rnd_splits/split_{k}.csv') for k in range(5)]\n",
    "output_dir = Path(\"../output/asap/rnd_splits/chemprop/run_0/cleaned\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predicting on ../data/asap/datasets/rnd_splits/split_0.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type               | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | message_passing | BondMessagePassing | 2.2 M  | train\n",
      "1 | agg             | MeanAggregation    | 0      | train\n",
      "2 | bn              | BatchNorm1d        | 2.0 K  | train\n",
      "3 | predictor       | RegressionFFN      | 503 K  | train\n",
      "4 | X_d_transform   | Identity           | 0      | train\n",
      "5 | metrics         | ModuleList         | 0      | train\n",
      "---------------------------------------------------------------\n",
      "2.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.7 M     Total params\n",
      "10.656    Total estimated model params size (MB)\n",
      "25        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/core/saving.py:363: Skipping 'metrics' parameter because it is not possible to safely dump to YAML.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predicting on ../data/asap/datasets/rnd_splits/split_1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type               | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | message_passing | BondMessagePassing | 2.2 M  | train\n",
      "1 | agg             | MeanAggregation    | 0      | train\n",
      "2 | bn              | BatchNorm1d        | 2.0 K  | train\n",
      "3 | predictor       | RegressionFFN      | 503 K  | train\n",
      "4 | X_d_transform   | Identity           | 0      | train\n",
      "5 | metrics         | ModuleList         | 0      | train\n",
      "---------------------------------------------------------------\n",
      "2.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.7 M     Total params\n",
      "10.656    Total estimated model params size (MB)\n",
      "25        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/core/saving.py:363: Skipping 'metrics' parameter because it is not possible to safely dump to YAML.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predicting on ../data/asap/datasets/rnd_splits/split_2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type               | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | message_passing | BondMessagePassing | 2.2 M  | train\n",
      "1 | agg             | MeanAggregation    | 0      | train\n",
      "2 | bn              | BatchNorm1d        | 2.0 K  | train\n",
      "3 | predictor       | RegressionFFN      | 503 K  | train\n",
      "4 | X_d_transform   | Identity           | 0      | train\n",
      "5 | metrics         | ModuleList         | 0      | train\n",
      "---------------------------------------------------------------\n",
      "2.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.7 M     Total params\n",
      "10.656    Total estimated model params size (MB)\n",
      "25        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/core/saving.py:363: Skipping 'metrics' parameter because it is not possible to safely dump to YAML.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predicting on ../data/asap/datasets/rnd_splits/split_3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type               | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | message_passing | BondMessagePassing | 2.2 M  | train\n",
      "1 | agg             | MeanAggregation    | 0      | train\n",
      "2 | bn              | BatchNorm1d        | 2.0 K  | train\n",
      "3 | predictor       | RegressionFFN      | 503 K  | train\n",
      "4 | X_d_transform   | Identity           | 0      | train\n",
      "5 | metrics         | ModuleList         | 0      | train\n",
      "---------------------------------------------------------------\n",
      "2.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.7 M     Total params\n",
      "10.656    Total estimated model params size (MB)\n",
      "25        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/core/saving.py:363: Skipping 'metrics' parameter because it is not possible to safely dump to YAML.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predicting on ../data/asap/datasets/rnd_splits/split_4.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type               | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | message_passing | BondMessagePassing | 2.2 M  | train\n",
      "1 | agg             | MeanAggregation    | 0      | train\n",
      "2 | bn              | BatchNorm1d        | 2.0 K  | train\n",
      "3 | predictor       | RegressionFFN      | 503 K  | train\n",
      "4 | X_d_transform   | Identity           | 0      | train\n",
      "5 | metrics         | ModuleList         | 0      | train\n",
      "---------------------------------------------------------------\n",
      "2.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.7 M     Total params\n",
      "10.656    Total estimated model params size (MB)\n",
      "25        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/core/saving.py:363: Skipping 'metrics' parameter because it is not possible to safely dump to YAML.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[split_4] Removing 22 noisy mols out of each trainig set for LogHLM\n",
      "[split_4] Removing 18 noisy mols out of each trainig set for LogMLM\n",
      "[split_4] Removing 37 noisy mols out of each trainig set for LogD\n",
      "[split_4] Removing 101 noisy mols out of each trainig set for LogKSOL\n",
      "[split_4] Removing 23 noisy mols out of each trainig set for LogMDR1-MDCKII\n"
     ]
    }
   ],
   "source": [
    "clean_data(input_paths, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogHLM            150\n",
       "LogMLM            140\n",
       "LogD               86\n",
       "LogKSOL            74\n",
       "LogMDR1-MDCKII     15\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(input_paths[0])[TARGET_COLUMNS].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogHLM            155\n",
       "LogMLM            143\n",
       "LogD              101\n",
       "LogKSOL           135\n",
       "LogMDR1-MDCKII     37\n",
       "dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(output_dir / input_paths[0].name)[TARGET_COLUMNS].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_paths = [Path(f'../output/asap/rnd_splits/chemprop/run_0/cleaned/split_{k}.csv') for k in range(5)]\n",
    "save_dirs = [Path(f'../output/asap/rnd_splits/chemprop/run_1/split_{k}') for k in range(5)]\n",
    "RUN_IDX = \"1_clean_noisy_stereo_impure\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predicting on ../output/asap/rnd_splits/chemprop/run_0/cleaned/split_0.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvladvin111\u001b[0m (\u001b[33mvladvin-org\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Path ../wandb/chemprop_run_1_clean_noisy_stereo_impure/wandb/ wasn't writable, using system temp directory.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/var/tmp/wandb/run-20250312_010504-kgk9nvl8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vladvin-org/admet-challenge/runs/kgk9nvl8' target=\"_blank\">chemprop_run_1_clean_noisy_stereo_impure</a></strong> to <a href='https://wandb.ai/vladvin-org/admet-challenge' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/vladvin-org/admet-challenge' target=\"_blank\">https://wandb.ai/vladvin-org/admet-challenge</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/vladvin-org/admet-challenge/runs/kgk9nvl8' target=\"_blank\">https://wandb.ai/vladvin-org/admet-challenge/runs/kgk9nvl8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type               | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | message_passing | BondMessagePassing | 2.2 M  | train\n",
      "1 | agg             | MeanAggregation    | 0      | train\n",
      "2 | bn              | BatchNorm1d        | 2.0 K  | train\n",
      "3 | predictor       | RegressionFFN      | 503 K  | train\n",
      "4 | X_d_transform   | Identity           | 0      | train\n",
      "5 | metrics         | ModuleList         | 0      | train\n",
      "---------------------------------------------------------------\n",
      "2.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.7 M     Total params\n",
      "10.656    Total estimated model params size (MB)\n",
      "25        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/core/saving.py:363: Skipping 'metrics' parameter because it is not possible to safely dump to YAML.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics:\n",
      "{\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.41157330864470854,\n",
      "    \"r2\": 0.7793256730035836\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.2274635736890114,\n",
      "    \"r2\": 0.7692358810553236\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.15861471095255583,\n",
      "    \"r2\": 0.7039743137709552\n",
      "  },\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.30302947491732396,\n",
      "    \"r2\": 0.6639689553479851\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.22111798235844082,\n",
      "    \"r2\": 0.7415076219426853\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.2643598101124081,\n",
      "    \"macro_r2\": 0.7316024890241065\n",
      "  }\n",
      "}\n",
      "\n",
      "Val metrics:\n",
      "{\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.61606073803589,\n",
      "    \"r2\": 0.6217383236856941\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.3510808179304693,\n",
      "    \"r2\": 0.48925999458340796\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.2548134638389187,\n",
      "    \"r2\": 0.38469622915485413\n",
      "  },\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.519244693145987,\n",
      "    \"r2\": 0.08830698591379915\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.3225285099173066,\n",
      "    \"r2\": 0.43497863213132515\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.4127456445737144,\n",
      "    \"macro_r2\": 0.4037960330938161\n",
      "  }\n",
      "}\n",
      "Training and predicting on ../output/asap/rnd_splits/chemprop/run_0/cleaned/split_1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type               | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | message_passing | BondMessagePassing | 2.2 M  | train\n",
      "1 | agg             | MeanAggregation    | 0      | train\n",
      "2 | bn              | BatchNorm1d        | 2.0 K  | train\n",
      "3 | predictor       | RegressionFFN      | 503 K  | train\n",
      "4 | X_d_transform   | Identity           | 0      | train\n",
      "5 | metrics         | ModuleList         | 0      | train\n",
      "---------------------------------------------------------------\n",
      "2.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.7 M     Total params\n",
      "10.656    Total estimated model params size (MB)\n",
      "25        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/core/saving.py:363: Skipping 'metrics' parameter because it is not possible to safely dump to YAML.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics:\n",
      "{\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.2630360685667858,\n",
      "    \"r2\": 0.9162161709305012\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.2071444701859941,\n",
      "    \"r2\": 0.8206599253308432\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.15387229231635954,\n",
      "    \"r2\": 0.7103173968846648\n",
      "  },\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.27647263583003473,\n",
      "    \"r2\": 0.6947764461448322\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.1892485309606936,\n",
      "    \"r2\": 0.805741035686721\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.21795479957197356,\n",
      "    \"macro_r2\": 0.7895421949955124\n",
      "  }\n",
      "}\n",
      "\n",
      "Val metrics:\n",
      "{\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.5447231742739678,\n",
      "    \"r2\": 0.6625823624015863\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.3500520323848197,\n",
      "    \"r2\": 0.5204176868102546\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.20249838406466078,\n",
      "    \"r2\": 0.32469892370304343\n",
      "  },\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.43212247833779316,\n",
      "    \"r2\": 0.26626116679035916\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.32880266835404715,\n",
      "    \"r2\": 0.5230104332175883\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.3716397474830577,\n",
      "    \"macro_r2\": 0.45939411458456636\n",
      "  }\n",
      "}\n",
      "Training and predicting on ../output/asap/rnd_splits/chemprop/run_0/cleaned/split_2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type               | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | message_passing | BondMessagePassing | 2.2 M  | train\n",
      "1 | agg             | MeanAggregation    | 0      | train\n",
      "2 | bn              | BatchNorm1d        | 2.0 K  | train\n",
      "3 | predictor       | RegressionFFN      | 503 K  | train\n",
      "4 | X_d_transform   | Identity           | 0      | train\n",
      "5 | metrics         | ModuleList         | 0      | train\n",
      "---------------------------------------------------------------\n",
      "2.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.7 M     Total params\n",
      "10.656    Total estimated model params size (MB)\n",
      "25        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/core/saving.py:363: Skipping 'metrics' parameter because it is not possible to safely dump to YAML.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics:\n",
      "{\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.24629208330002403,\n",
      "    \"r2\": 0.9213291204314001\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.2226628849942504,\n",
      "    \"r2\": 0.7921801794411683\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.1594922086738758,\n",
      "    \"r2\": 0.6966986002336822\n",
      "  },\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.26707413667271906,\n",
      "    \"r2\": 0.7085725358388657\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.20348018369443777,\n",
      "    \"r2\": 0.7789510627362493\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.21980029946706142,\n",
      "    \"macro_r2\": 0.7795462997362732\n",
      "  }\n",
      "}\n",
      "\n",
      "Val metrics:\n",
      "{\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.5609929378962878,\n",
      "    \"r2\": 0.670569639483211\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.4419827322587787,\n",
      "    \"r2\": 0.05836532509549319\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.1941678700516716,\n",
      "    \"r2\": 0.5386003487941138\n",
      "  },\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.4325733212981746,\n",
      "    \"r2\": 0.28242950451161664\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.3557894725965894,\n",
      "    \"r2\": 0.25988863102946413\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.3971012668203004,\n",
      "    \"macro_r2\": 0.36197068978277974\n",
      "  }\n",
      "}\n",
      "Training and predicting on ../output/asap/rnd_splits/chemprop/run_0/cleaned/split_3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type               | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | message_passing | BondMessagePassing | 2.2 M  | train\n",
      "1 | agg             | MeanAggregation    | 0      | train\n",
      "2 | bn              | BatchNorm1d        | 2.0 K  | train\n",
      "3 | predictor       | RegressionFFN      | 503 K  | train\n",
      "4 | X_d_transform   | Identity           | 0      | train\n",
      "5 | metrics         | ModuleList         | 0      | train\n",
      "---------------------------------------------------------------\n",
      "2.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.7 M     Total params\n",
      "10.656    Total estimated model params size (MB)\n",
      "25        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/core/saving.py:363: Skipping 'metrics' parameter because it is not possible to safely dump to YAML.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics:\n",
      "{\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.33838355030188577,\n",
      "    \"r2\": 0.8864517752152922\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.2367176323636466,\n",
      "    \"r2\": 0.7737339550332083\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.1537083369722694,\n",
      "    \"r2\": 0.7443691982824767\n",
      "  },\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.2864583648744494,\n",
      "    \"r2\": 0.7008398852032374\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.21636523087618673,\n",
      "    \"r2\": 0.752404458574051\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.24632662307768757,\n",
      "    \"macro_r2\": 0.7715598544616531\n",
      "  }\n",
      "}\n",
      "\n",
      "Val metrics:\n",
      "{\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.5168429049299313,\n",
      "    \"r2\": 0.6808673873374208\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.39349699742893035,\n",
      "    \"r2\": 0.2770419730072251\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.22926762735587358,\n",
      "    \"r2\": 0.28365381248197274\n",
      "  },\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.3908191409532371,\n",
      "    \"r2\": 0.39676522672831727\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.3350627364630285,\n",
      "    \"r2\": 0.38608413172622325\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.37309788142620015,\n",
      "    \"macro_r2\": 0.4048825062562318\n",
      "  }\n",
      "}\n",
      "Training and predicting on ../output/asap/rnd_splits/chemprop/run_0/cleaned/split_4.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type               | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | message_passing | BondMessagePassing | 2.2 M  | train\n",
      "1 | agg             | MeanAggregation    | 0      | train\n",
      "2 | bn              | BatchNorm1d        | 2.0 K  | train\n",
      "3 | predictor       | RegressionFFN      | 503 K  | train\n",
      "4 | X_d_transform   | Identity           | 0      | train\n",
      "5 | metrics         | ModuleList         | 0      | train\n",
      "---------------------------------------------------------------\n",
      "2.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.7 M     Total params\n",
      "10.656    Total estimated model params size (MB)\n",
      "25        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/core/saving.py:363: Skipping 'metrics' parameter because it is not possible to safely dump to YAML.\n",
      "/opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics:\n",
      "{\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.3109755268944389,\n",
      "    \"r2\": 0.9034395116859321\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.19381306028427364,\n",
      "    \"r2\": 0.8542786751941044\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.1429759592707131,\n",
      "    \"r2\": 0.7493253676984301\n",
      "  },\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.23492245945313914,\n",
      "    \"r2\": 0.7530365422636985\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.2132007834081363,\n",
      "    \"r2\": 0.782922044336043\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.21917755786214022,\n",
      "    \"macro_r2\": 0.8086004282356416\n",
      "  }\n",
      "}\n",
      "\n",
      "Val metrics:\n",
      "{\n",
      "  \"LogD\": {\n",
      "    \"mean_absolute_error\": 0.521813886816303,\n",
      "    \"r2\": 0.6654304881392514\n",
      "  },\n",
      "  \"MLM\": {\n",
      "    \"mean_absolute_error\": 0.33753299478560805,\n",
      "    \"r2\": 0.43068289468435184\n",
      "  },\n",
      "  \"MDR1-MDCKII\": {\n",
      "    \"mean_absolute_error\": 0.1839600434547671,\n",
      "    \"r2\": 0.6132509883864026\n",
      "  },\n",
      "  \"KSOL\": {\n",
      "    \"mean_absolute_error\": 0.3648529207485866,\n",
      "    \"r2\": 0.23961803119276104\n",
      "  },\n",
      "  \"HLM\": {\n",
      "    \"mean_absolute_error\": 0.2957730010271167,\n",
      "    \"r2\": 0.3247660885196375\n",
      "  },\n",
      "  \"aggregated\": {\n",
      "    \"macro_mean_absolute_error\": 0.34078656936647633,\n",
      "    \"macro_r2\": 0.45474969818448086\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>split_0-epoch</td><td>▁▁▁▂▂▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇██</td></tr><tr><td>split_0-train_loss_epoch</td><td>█▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▃▃▃▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_0-train_loss_step</td><td>▆▄█▄▃▅▃▃▁▃▂▅▃▂▅▂▂▂▂▂▄▂▂▄</td></tr><tr><td>split_0-val/mae</td><td>▆▄▄▃▂▃▄█▃▂▃▆▄▃▂▂▁▁▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_0-val/r2</td><td>▄▆▆▆▇▄▆▁▇███▇███████████████████████████</td></tr><tr><td>split_0-val_loss</td><td>▅▄▃█▅▆▆▃▇▂▄▂▃▂▂▂▂▂▂▄▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_1-epoch</td><td>▁▁▁▁▁▂▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇█</td></tr><tr><td>split_1-train_loss_epoch</td><td>█▇▇▆▇▇▇▇▅▅▄▄▄▄▄▃▃▃▃▃▃▃▂▃▃▂▂▂▂▂▁▂▂▁▁▂▁▁▁▁</td></tr><tr><td>split_1-train_loss_step</td><td>▇▄▄▆▃▃▂▃▅▂▂▁▂▂█▂▁▃▁▂▃▁▁▂</td></tr><tr><td>split_1-val/mae</td><td>▄▇█▄▇▂▄▄▂▂▂▂▁▁▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_1-val/r2</td><td>▁▇▇█▄▇▆█▇███████████████████████████████</td></tr><tr><td>split_1-val_loss</td><td>▃▇█▃█▃█▆▃▄▃▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_2-epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇██</td></tr><tr><td>split_2-train_loss_epoch</td><td>█▇█▆▇▅▅▆▅▄▄▅▄▄▄▄▃▃▂▃▃▃▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_2-train_loss_step</td><td>▆▇▂▄▃▄▄▃█▂▃▃▃▂▂▂▂▁▂▁▁▂▁▁</td></tr><tr><td>split_2-val/mae</td><td>▄▅███▂▆▄▃▂▃▃▄▂▂▂▂▁▂▃▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_2-val/r2</td><td>▂▅▇▇▆▅▆▂▄▁▅▇▇███▇███████████████████████</td></tr><tr><td>split_2-val_loss</td><td>▅▇█▄▁▂▅▃▄▅▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_3-epoch</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▆▇▇█████</td></tr><tr><td>split_3-train_loss_epoch</td><td>█▅▄▃▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_3-train_loss_step</td><td>▇▅▄▃▄█▃▂▃▂▂▄▂▁▁▂▁▅▁▂▂▁▁▃</td></tr><tr><td>split_3-val/mae</td><td>▃▅▃█▅▃▃▂▂▃▂▁▁▁▁▂▁▂▂▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_3-val/r2</td><td>▅▆▄▁▇▆▅▇▇█▇███▇█████████████████████████</td></tr><tr><td>split_3-val_loss</td><td>█▇▅▃▂▃▃▄▂▂▃▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_4-epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▇▇▇████</td></tr><tr><td>split_4-train_loss_epoch</td><td>▇▇█▆▅▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_4-train_loss_step</td><td>█▆▁▄▃▁▂▃▅▃▂▇▂▂▆▁▂▃▁▂▇▁▁▂</td></tr><tr><td>split_4-val/mae</td><td>▄█▄▄▂▃▂▇▃▄▃▃▂▂▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>split_4-val/r2</td><td>▆▆▁▃▇▇▃▇▅▇█████▇█▇██████████████████████</td></tr><tr><td>split_4-val_loss</td><td>█▁▁▂▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▃▆▆▆▁▂▂▂▂▅▅▆▇█▃▃▄▅▆▇▇█▂▂▆▆▇▇█▂▂▃▃▃▅▅▅▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>split_0-epoch</td><td>199</td></tr><tr><td>split_0-train_loss_epoch</td><td>0.13923</td></tr><tr><td>split_0-train_loss_step</td><td>0.37768</td></tr><tr><td>split_0-val/mae</td><td>0.4097</td></tr><tr><td>split_0-val/r2</td><td>0.68362</td></tr><tr><td>split_0-val_loss</td><td>0.29427</td></tr><tr><td>split_1-epoch</td><td>199</td></tr><tr><td>split_1-train_loss_epoch</td><td>0.10067</td></tr><tr><td>split_1-train_loss_step</td><td>0.14334</td></tr><tr><td>split_1-val/mae</td><td>0.36942</td></tr><tr><td>split_1-val/r2</td><td>0.72133</td></tr><tr><td>split_1-val_loss</td><td>0.26455</td></tr><tr><td>split_2-epoch</td><td>199</td></tr><tr><td>split_2-train_loss_epoch</td><td>0.1116</td></tr><tr><td>split_2-train_loss_step</td><td>0.09284</td></tr><tr><td>split_2-val/mae</td><td>0.38949</td></tr><tr><td>split_2-val/r2</td><td>0.69441</td></tr><tr><td>split_2-val_loss</td><td>0.31055</td></tr><tr><td>split_3-epoch</td><td>199</td></tr><tr><td>split_3-train_loss_epoch</td><td>0.12154</td></tr><tr><td>split_3-train_loss_step</td><td>0.30998</td></tr><tr><td>split_3-val/mae</td><td>0.36733</td></tr><tr><td>split_3-val/r2</td><td>0.73692</td></tr><tr><td>split_3-val_loss</td><td>0.24542</td></tr><tr><td>split_4-epoch</td><td>199</td></tr><tr><td>split_4-train_loss_epoch</td><td>0.10158</td></tr><tr><td>split_4-train_loss_step</td><td>0.12616</td></tr><tr><td>split_4-val/mae</td><td>0.33504</td></tr><tr><td>split_4-val/r2</td><td>0.72422</td></tr><tr><td>split_4-val_loss</td><td>0.25375</td></tr><tr><td>trainer/global_step</td><td>1199</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">chemprop_run_1_clean_noisy_stereo_impure</strong> at: <a href='https://wandb.ai/vladvin-org/admet-challenge/runs/kgk9nvl8' target=\"_blank\">https://wandb.ai/vladvin-org/admet-challenge/runs/kgk9nvl8</a><br> View project at: <a href='https://wandb.ai/vladvin-org/admet-challenge' target=\"_blank\">https://wandb.ai/vladvin-org/admet-challenge</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>../wandb/chemprop_run_1_clean_noisy_stereo_impure/wandb/run-20250312_010504-kgk9nvl8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_and_eval(input_paths, save_dirs, RUN_IDX)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "admet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
