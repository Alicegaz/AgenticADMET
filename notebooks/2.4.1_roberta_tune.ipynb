{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iirViHgbOqhE"
      },
      "source": [
        "# Running hyperparameter optimization on RoBERTa model using RayTune"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCUWWfPHOqhG"
      },
      "source": [
        "## Import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qtvN4rm9OqhH",
        "outputId": "9c635e3b-cf36-4294-b179-3781698b5158"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[23:20:49] Initializing Normalizer\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import os\n",
        "import sys\n",
        "\n",
        "import pandas as pd\n",
        "from lightning import pytorch as pl\n",
        "import numpy as np\n",
        "import ray\n",
        "from ray import tune\n",
        "from ray.train import CheckpointConfig, RunConfig, ScalingConfig\n",
        "from ray.train.lightning import (RayDDPStrategy, RayLightningEnvironment,\n",
        "                                 RayTrainReportCallback, prepare_trainer)\n",
        "from ray.train.torch import TorchTrainer\n",
        "from ray.tune.search.hyperopt import HyperOptSearch\n",
        "from ray.tune.search.optuna import OptunaSearch\n",
        "from ray.tune.schedulers import FIFOScheduler\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "os.environ['PYTHONPATH'] = '../agenticadmet'\n",
        "sys.path.insert(0, '../agenticadmet')\n",
        "from datasets import RegressionDataset\n",
        "from models import TransformerRegressionModel\n",
        "from utils import CheckpointParams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "torch.cuda.manual_seed_all(RANDOM_SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "e_rN1prjOqhH"
      },
      "outputs": [],
      "source": [
        "input_path = Path('../data/asap/datasets/rnd_splits/split_0.csv')\n",
        "NUM_WORKERS = 0 # number of workers for dataloader. 0 means using main process for data loading\n",
        "SMILES_COLUMN = 'cxsmiles_std' # name of the column containing SMILES strings\n",
        "TARGET_COLUMNS = ['LogHLM', 'LogMLM', 'LogD', 'LogKSOL', 'LogMDR1-MDCKII'] # list of names of the columns containing targets\n",
        "\n",
        "MODEL_PARAMS = {\n",
        "    'config': {\n",
        "        'vocab_size': 500,\n",
        "        'hidden_size': 384,\n",
        "        'num_hidden_layers': 6,\n",
        "        'num_attention_heads': 8,\n",
        "        'intermediate_size': 1024,\n",
        "        'hidden_act': \"gelu\",\n",
        "        'hidden_dropout_prob': 0.1,\n",
        "        'attention_probs_dropout_prob': 0.1,\n",
        "        'max_position_embeddings': 512,\n",
        "        'initializer_range': 0.02,\n",
        "        'layer_norm_eps': 1e-12,\n",
        "        'pad_token_id': 0,\n",
        "        'position_embedding_type': \"absolute\",\n",
        "        'use_cache': True,\n",
        "        'type_vocab_size': 2\n",
        "    },\n",
        "    'output_dim': len(TARGET_COLUMNS),\n",
        "    'bias_final': False\n",
        "}\n",
        "TOKENIZER_NAME = '<gs_bucket>/artifacts/tokenizers/zinc'\n",
        "CHECKPOINTS = [\n",
        "    CheckpointParams(\n",
        "        path=str(Path('../output/artifacts/mol_mlm_roberta_zinc/last.ckpt').absolute()),\n",
        "        module_from='roberta',\n",
        "        module_to='roberta',\n",
        "        strict=True\n",
        "    )\n",
        "]\n",
        "# CHECKPOINTS = None\n",
        "\n",
        "hpopt_save_dir = Path('../output/asap/rnd_splits/roberta/run_0/split_0/hpopt') # directory to save hyperopt results\n",
        "hpopt_save_dir.mkdir(exist_ok=True, parents=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJHI1hpjOqhI"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "SxTJtNHuOqhI",
        "outputId": "d037adbc-0d7d-4a3a-d846-4d5cc2feea11"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>smiles</th>\n",
              "      <th>HLM</th>\n",
              "      <th>KSOL</th>\n",
              "      <th>LogD</th>\n",
              "      <th>MLM</th>\n",
              "      <th>MDR1-MDCKII</th>\n",
              "      <th>smiles_std</th>\n",
              "      <th>cxsmiles_std</th>\n",
              "      <th>mol_idx</th>\n",
              "      <th>smiles_ext</th>\n",
              "      <th>LogHLM</th>\n",
              "      <th>LogMLM</th>\n",
              "      <th>LogKSOL</th>\n",
              "      <th>LogMDR1-MDCKII</th>\n",
              "      <th>split</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>COC1=CC=CC(Cl)=C1NC(=O)N1CCC[C@H](C(N)=O)C1 |a...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.0</td>\n",
              "      <td>COc1cccc(Cl)c1NC(=O)N1CCC[C@H](C(N)=O)C1</td>\n",
              "      <td>COc1cccc(Cl)c1NC(=O)N1CCC[C@H](C(N)=O)C1 |a:16|</td>\n",
              "      <td>191</td>\n",
              "      <td>|a:16|</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.477121</td>\n",
              "      <td>val</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>O=C(NCC(F)F)[C@H](NC1=CC2=C(C=C1Br)CNC2)C1=CC(...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>333.0</td>\n",
              "      <td>2.9</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.2</td>\n",
              "      <td>O=C(NCC(F)F)[C@H](Nc1cc2c(cc1Br)CNC2)c1cc(Cl)c...</td>\n",
              "      <td>O=C(NCC(F)F)[C@H](Nc1cc2c(cc1Br)CNC2)c1cc(Cl)c...</td>\n",
              "      <td>335</td>\n",
              "      <td>|&amp;1:7|</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.523746</td>\n",
              "      <td>0.079181</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>O=C(NCC(F)F)[C@H](NC1=CC=C2CNCC2=C1)C1=CC(Br)=...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.5</td>\n",
              "      <td>O=C(NCC(F)F)[C@H](Nc1ccc2c(c1)CNC2)c1cc(Br)cc2...</td>\n",
              "      <td>O=C(NCC(F)F)[C@H](Nc1ccc2c(c1)CNC2)c1cc(Br)cc2...</td>\n",
              "      <td>336</td>\n",
              "      <td>|&amp;1:7|</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.176091</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NC(=O)[C@H]1CCCN(C(=O)CC2=CC=CC3=C2C=CO3)C1 |&amp;...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>376.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>8.5</td>\n",
              "      <td>NC(=O)[C@H]1CCCN(C(=O)Cc2cccc3occc23)C1</td>\n",
              "      <td>NC(=O)[C@H]1CCCN(C(=O)Cc2cccc3occc23)C1 |&amp;1:3|</td>\n",
              "      <td>300</td>\n",
              "      <td>|&amp;1:3|</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.576341</td>\n",
              "      <td>0.977724</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>CC1=CC(CC(=O)N2CCC[C@H](C(N)=O)C2)=CC=N1 |&amp;1:11|</td>\n",
              "      <td>NaN</td>\n",
              "      <td>375.0</td>\n",
              "      <td>-0.3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.9</td>\n",
              "      <td>Cc1cc(CC(=O)N2CCC[C@H](C(N)=O)C2)ccn1</td>\n",
              "      <td>Cc1cc(CC(=O)N2CCC[C@H](C(N)=O)C2)ccn1 |&amp;1:11|</td>\n",
              "      <td>249</td>\n",
              "      <td>|&amp;1:11|</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.575188</td>\n",
              "      <td>0.278754</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>399</th>\n",
              "      <td>CC(C)NC[C@H](O)COC1=CC=CC2=CC=CC=C12 |&amp;1:5|</td>\n",
              "      <td>25.5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>63.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>CC(C)NC[C@H](O)COc1cccc2ccccc12</td>\n",
              "      <td>CC(C)NC[C@H](O)COc1cccc2ccccc12 |&amp;1:5|</td>\n",
              "      <td>22</td>\n",
              "      <td>|&amp;1:5|</td>\n",
              "      <td>1.423246</td>\n",
              "      <td>1.806180</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>val</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>400</th>\n",
              "      <td>O=C(O)CC1=CC=CC=C1NC1=C(Cl)C=CC=C1Cl</td>\n",
              "      <td>216.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>386.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>O=C(O)Cc1ccccc1Nc1c(Cl)cccc1Cl</td>\n",
              "      <td>O=C(O)Cc1ccccc1Nc1c(Cl)cccc1Cl</td>\n",
              "      <td>380</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.336460</td>\n",
              "      <td>2.587711</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>val</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>401</th>\n",
              "      <td>NCC1=CC(Cl)=CC(C(=O)NC2=CC=C3CNCC3=C2)=C1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NCc1cc(Cl)cc(C(=O)Nc2ccc3c(c2)CNC3)c1</td>\n",
              "      <td>NCc1cc(Cl)cc(C(=O)Nc2ccc3c(c2)CNC3)c1</td>\n",
              "      <td>303</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>402</th>\n",
              "      <td>COC(=O)NC1=NC2=CC=C(C(=O)C3=CC=CC=C3)C=C2N1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.9</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>COC(=O)Nc1nc2ccc(C(=O)c3ccccc3)cc2[nH]1</td>\n",
              "      <td>COC(=O)Nc1nc2ccc(C(=O)c3ccccc3)cc2[nH]1</td>\n",
              "      <td>166</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>403</th>\n",
              "      <td>CC1=NC=CN1C[C@H]1CCC2=C(C1=O)C1=CC=CC=C1N2C |&amp;...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>127.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Cc1nccn1C[C@H]1CCc2c(c3ccccc3n2C)C1=O</td>\n",
              "      <td>Cc1nccn1C[C@H]1CCc2c(c3ccccc3n2C)C1=O |&amp;1:7|</td>\n",
              "      <td>268</td>\n",
              "      <td>|&amp;1:7|</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.107210</td>\n",
              "      <td>NaN</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>404 rows × 15 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                smiles    HLM   KSOL  LogD  \\\n",
              "0    COC1=CC=CC(Cl)=C1NC(=O)N1CCC[C@H](C(N)=O)C1 |a...    NaN    NaN   0.3   \n",
              "1    O=C(NCC(F)F)[C@H](NC1=CC2=C(C=C1Br)CNC2)C1=CC(...    NaN  333.0   2.9   \n",
              "2    O=C(NCC(F)F)[C@H](NC1=CC=C2CNCC2=C1)C1=CC(Br)=...    NaN    NaN   0.4   \n",
              "3    NC(=O)[C@H]1CCCN(C(=O)CC2=CC=CC3=C2C=CO3)C1 |&...    NaN  376.0   1.0   \n",
              "4     CC1=CC(CC(=O)N2CCC[C@H](C(N)=O)C2)=CC=N1 |&1:11|    NaN  375.0  -0.3   \n",
              "..                                                 ...    ...    ...   ...   \n",
              "399        CC(C)NC[C@H](O)COC1=CC=CC2=CC=CC=C12 |&1:5|   25.5    NaN   NaN   \n",
              "400               O=C(O)CC1=CC=CC=C1NC1=C(Cl)C=CC=C1Cl  216.0    NaN   NaN   \n",
              "401          NCC1=CC(Cl)=CC(C(=O)NC2=CC=C3CNCC3=C2)=C1    NaN    NaN   2.0   \n",
              "402        COC(=O)NC1=NC2=CC=C(C(=O)C3=CC=CC=C3)C=C2N1    NaN    NaN   2.9   \n",
              "403  CC1=NC=CN1C[C@H]1CCC2=C(C1=O)C1=CC=CC=C1N2C |&...    NaN  127.0   NaN   \n",
              "\n",
              "       MLM  MDR1-MDCKII                                         smiles_std  \\\n",
              "0      NaN          2.0           COc1cccc(Cl)c1NC(=O)N1CCC[C@H](C(N)=O)C1   \n",
              "1      NaN          0.2  O=C(NCC(F)F)[C@H](Nc1cc2c(cc1Br)CNC2)c1cc(Cl)c...   \n",
              "2      NaN          0.5  O=C(NCC(F)F)[C@H](Nc1ccc2c(c1)CNC2)c1cc(Br)cc2...   \n",
              "3      NaN          8.5            NC(=O)[C@H]1CCCN(C(=O)Cc2cccc3occc23)C1   \n",
              "4      NaN          0.9              Cc1cc(CC(=O)N2CCC[C@H](C(N)=O)C2)ccn1   \n",
              "..     ...          ...                                                ...   \n",
              "399   63.0          NaN                    CC(C)NC[C@H](O)COc1cccc2ccccc12   \n",
              "400  386.0          NaN                     O=C(O)Cc1ccccc1Nc1c(Cl)cccc1Cl   \n",
              "401    NaN          NaN              NCc1cc(Cl)cc(C(=O)Nc2ccc3c(c2)CNC3)c1   \n",
              "402    NaN          NaN            COC(=O)Nc1nc2ccc(C(=O)c3ccccc3)cc2[nH]1   \n",
              "403    NaN          NaN              Cc1nccn1C[C@H]1CCc2c(c3ccccc3n2C)C1=O   \n",
              "\n",
              "                                          cxsmiles_std  mol_idx smiles_ext  \\\n",
              "0      COc1cccc(Cl)c1NC(=O)N1CCC[C@H](C(N)=O)C1 |a:16|      191     |a:16|   \n",
              "1    O=C(NCC(F)F)[C@H](Nc1cc2c(cc1Br)CNC2)c1cc(Cl)c...      335     |&1:7|   \n",
              "2    O=C(NCC(F)F)[C@H](Nc1ccc2c(c1)CNC2)c1cc(Br)cc2...      336     |&1:7|   \n",
              "3       NC(=O)[C@H]1CCCN(C(=O)Cc2cccc3occc23)C1 |&1:3|      300     |&1:3|   \n",
              "4        Cc1cc(CC(=O)N2CCC[C@H](C(N)=O)C2)ccn1 |&1:11|      249    |&1:11|   \n",
              "..                                                 ...      ...        ...   \n",
              "399             CC(C)NC[C@H](O)COc1cccc2ccccc12 |&1:5|       22     |&1:5|   \n",
              "400                     O=C(O)Cc1ccccc1Nc1c(Cl)cccc1Cl      380        NaN   \n",
              "401              NCc1cc(Cl)cc(C(=O)Nc2ccc3c(c2)CNC3)c1      303        NaN   \n",
              "402            COC(=O)Nc1nc2ccc(C(=O)c3ccccc3)cc2[nH]1      166        NaN   \n",
              "403       Cc1nccn1C[C@H]1CCc2c(c3ccccc3n2C)C1=O |&1:7|      268     |&1:7|   \n",
              "\n",
              "       LogHLM    LogMLM   LogKSOL  LogMDR1-MDCKII  split  \n",
              "0         NaN       NaN       NaN        0.477121    val  \n",
              "1         NaN       NaN  2.523746        0.079181  train  \n",
              "2         NaN       NaN       NaN        0.176091  train  \n",
              "3         NaN       NaN  2.576341        0.977724  train  \n",
              "4         NaN       NaN  2.575188        0.278754  train  \n",
              "..        ...       ...       ...             ...    ...  \n",
              "399  1.423246  1.806180       NaN             NaN    val  \n",
              "400  2.336460  2.587711       NaN             NaN    val  \n",
              "401       NaN       NaN       NaN             NaN  train  \n",
              "402       NaN       NaN       NaN             NaN  train  \n",
              "403       NaN       NaN  2.107210             NaN  train  \n",
              "\n",
              "[404 rows x 15 columns]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_input = pd.read_csv(input_path)\n",
        "df_input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7HfneycOqhI"
      },
      "source": [
        "## Make data points, splits, and datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6UHczQL-OqhJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 out of 323 rows are removed due to missing values\n",
            "Downloading checkpoint from <gs_bucket>/artifacts/tokenizers/zinc...\n",
            "0 out of 81 rows are removed due to missing values\n",
            "Downloading checkpoint from <gs_bucket>/artifacts/tokenizers/zinc...\n"
          ]
        }
      ],
      "source": [
        "train_dset = RegressionDataset(\n",
        "    data_path=input_path,\n",
        "    smiles_col=SMILES_COLUMN,\n",
        "    target_cols=TARGET_COLUMNS,\n",
        "    split='train',\n",
        "    tokenizer_name=TOKENIZER_NAME,\n",
        "    mol_masking_prob=0.3,\n",
        "    mol_masking_val=0.15\n",
        ")\n",
        "val_dset = RegressionDataset(\n",
        "    data_path=input_path,\n",
        "    smiles_col=SMILES_COLUMN,\n",
        "    target_cols=TARGET_COLUMNS,\n",
        "    split='val',\n",
        "    tokenizer_name=TOKENIZER_NAME\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNw0j9HrOqhJ"
      },
      "source": [
        "# Define helper function to train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "aZ0eVnEBOqhJ"
      },
      "outputs": [],
      "source": [
        "def train_model(config, train_dset, val_dset):\n",
        "    # config is a dictionary containing hyperparameters used for the trial\n",
        "    model_params = MODEL_PARAMS.copy()\n",
        "    model_params['hidden_dim'] = int(config['hidden_dim'])\n",
        "    model_params['num_layers'] = int(config['num_layers'])\n",
        "    model_params['dropout'] = float(config['dropout'])\n",
        "    batch_size = int(config['batch_size'])\n",
        "    weight_decay = float(config['weight_decay'])\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dset, batch_size=batch_size, shuffle=True,\n",
        "        num_workers=NUM_WORKERS, collate_fn=train_dset.collate_fn\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_dset, batch_size=batch_size, shuffle=False,\n",
        "        num_workers=NUM_WORKERS, collate_fn=val_dset.collate_fn\n",
        "    )\n",
        "\n",
        "    model = TransformerRegressionModel(\n",
        "        model_name='roberta-base',\n",
        "        model_params=model_params,\n",
        "        weight_decay=weight_decay,\n",
        "        checkpoints=CHECKPOINTS\n",
        "    )\n",
        "\n",
        "    trainer = pl.Trainer(\n",
        "        accelerator=\"auto\",\n",
        "        devices=1,\n",
        "        max_epochs=50, # number of epochs to train for\n",
        "        # below are needed for Ray and Lightning integration\n",
        "        strategy=RayDDPStrategy(),\n",
        "        callbacks=[RayTrainReportCallback()],\n",
        "        plugins=[RayLightningEnvironment()],\n",
        "        enable_progress_bar=False,\n",
        "        enable_checkpointing=False\n",
        "    )\n",
        "\n",
        "    trainer = prepare_trainer(trainer)\n",
        "    trainer.fit(model, train_loader, val_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8TLQM3GOqhJ"
      },
      "source": [
        "## Define parameter search space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "KUZSmgPAOqhJ"
      },
      "outputs": [],
      "source": [
        "search_space = {\n",
        "    \"hidden_dim\": tune.qrandint(lower=128, upper=1024, q=128),\n",
        "    \"num_layers\": tune.qrandint(lower=1, upper=3, q=1),\n",
        "    \"batch_size\": tune.qrandint(lower=16, upper=128, q=16),\n",
        "    \"weight_decay\": tune.loguniform(lower=1e-5, upper=1e-1),\n",
        "    \"dropout\": tune.uniform(lower=0.0, upper=0.2),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "kcCRtbNtOqhJ",
        "outputId": "6ed0e723-bc1a-4b2d-fa78-ecd9f397a081"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div class=\"tuneStatus\">\n",
              "  <div style=\"display: flex;flex-direction: row\">\n",
              "    <div style=\"display: flex;flex-direction: column;\">\n",
              "      <h3>Tune Status</h3>\n",
              "      <table>\n",
              "<tbody>\n",
              "<tr><td>Current time:</td><td>2025-03-12 23:59:27</td></tr>\n",
              "<tr><td>Running for: </td><td>00:38:32.67        </td></tr>\n",
              "<tr><td>Memory:      </td><td>11.2/58.9 GiB      </td></tr>\n",
              "</tbody>\n",
              "</table>\n",
              "    </div>\n",
              "    <div class=\"vDivider\"></div>\n",
              "    <div class=\"systemInfo\">\n",
              "      <h3>System Info</h3>\n",
              "      Using FIFO scheduling algorithm.<br>Logical resource usage: 1.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
              "    </div>\n",
              "    \n",
              "  </div>\n",
              "  <div class=\"hDivider\"></div>\n",
              "  <div class=\"trialStatus\">\n",
              "    <h3>Trial Status</h3>\n",
              "    <table>\n",
              "<thead>\n",
              "<tr><th>Trial name           </th><th>status    </th><th>loc               </th><th style=\"text-align: right;\">    train_loop_config/ba\n",
              "tch_size</th><th style=\"text-align: right;\">            train_loop_config/dr\n",
              "opout</th><th style=\"text-align: right;\">     train_loop_config/hi\n",
              "dden_dim</th><th style=\"text-align: right;\">  train_loop_config/nu\n",
              "m_layers</th><th style=\"text-align: right;\">            train_loop_config/we\n",
              "ight_decay</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  train_loss</th><th style=\"text-align: right;\">  train_loss_step</th><th style=\"text-align: right;\">  val/mae</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>TorchTrainer_56fc16e6</td><td>TERMINATED</td><td>10.128.0.3:1177991</td><td style=\"text-align: right;\"> 96</td><td style=\"text-align: right;\">0.018611   </td><td style=\"text-align: right;\"> 896</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">2.81897e-05</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         66.3584</td><td style=\"text-align: right;\">   0.0906924</td><td style=\"text-align: right;\">        0.063619 </td><td style=\"text-align: right;\"> 0.407999</td></tr>\n",
              "<tr><td>TorchTrainer_b2eb9de9</td><td>TERMINATED</td><td>10.128.0.3:1179654</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">0.173402   </td><td style=\"text-align: right;\"> 768</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">0.00777321 </td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         65.1964</td><td style=\"text-align: right;\">   0.130427 </td><td style=\"text-align: right;\">        0.0826093</td><td style=\"text-align: right;\"> 0.440786</td></tr>\n",
              "<tr><td>TorchTrainer_bd4b22c9</td><td>TERMINATED</td><td>10.128.0.3:1181264</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">0.0257109  </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">1.26546e-05</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         63.5159</td><td style=\"text-align: right;\">   0.128798 </td><td style=\"text-align: right;\">        0.0521069</td><td style=\"text-align: right;\"> 0.408013</td></tr>\n",
              "<tr><td>TorchTrainer_439b693d</td><td>TERMINATED</td><td>10.128.0.3:1182831</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">0.0786423  </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">0.0439758  </td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         63.6506</td><td style=\"text-align: right;\">   0.141451 </td><td style=\"text-align: right;\">        0.0990262</td><td style=\"text-align: right;\"> 0.448804</td></tr>\n",
              "<tr><td>TorchTrainer_e0d70199</td><td>TERMINATED</td><td>10.128.0.3:1184502</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">0.156513   </td><td style=\"text-align: right;\"> 896</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">0.0212319  </td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         64.2665</td><td style=\"text-align: right;\">   0.0991244</td><td style=\"text-align: right;\">        0.0568382</td><td style=\"text-align: right;\"> 0.422184</td></tr>\n",
              "<tr><td>TorchTrainer_5a576c82</td><td>TERMINATED</td><td>10.128.0.3:1186232</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">0.0341362  </td><td style=\"text-align: right;\"> 384</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">0.000809838</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         63.3507</td><td style=\"text-align: right;\">   0.0921696</td><td style=\"text-align: right;\">        0.0925356</td><td style=\"text-align: right;\"> 0.386482</td></tr>\n",
              "<tr><td>TorchTrainer_08c9cf27</td><td>TERMINATED</td><td>10.128.0.3:1187943</td><td style=\"text-align: right;\"> 48</td><td style=\"text-align: right;\">0.114586   </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">0.00352467 </td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         62.2794</td><td style=\"text-align: right;\">   0.11269  </td><td style=\"text-align: right;\">        0.0962803</td><td style=\"text-align: right;\"> 0.404122</td></tr>\n",
              "<tr><td>TorchTrainer_599a7bc7</td><td>TERMINATED</td><td>10.128.0.3:1189481</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">0.0826331  </td><td style=\"text-align: right;\"> 896</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">0.00694254 </td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         66.0455</td><td style=\"text-align: right;\">   0.137496 </td><td style=\"text-align: right;\">        0.155992 </td><td style=\"text-align: right;\"> 0.434657</td></tr>\n",
              "<tr><td>TorchTrainer_5f4597d7</td><td>TERMINATED</td><td>10.128.0.3:1191042</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">0.0430375  </td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">0.000120687</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         73.1326</td><td style=\"text-align: right;\">   0.0779466</td><td style=\"text-align: right;\">        0.0577919</td><td style=\"text-align: right;\"> 0.396328</td></tr>\n",
              "<tr><td>TorchTrainer_481a02c2</td><td>TERMINATED</td><td>10.128.0.3:1192914</td><td style=\"text-align: right;\"> 48</td><td style=\"text-align: right;\">0.0493471  </td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">0.000418933</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         62.508 </td><td style=\"text-align: right;\">   0.0785766</td><td style=\"text-align: right;\">        0.0488015</td><td style=\"text-align: right;\"> 0.421358</td></tr>\n",
              "<tr><td>TorchTrainer_d61f6592</td><td>TERMINATED</td><td>10.128.0.3:1194562</td><td style=\"text-align: right;\"> 96</td><td style=\"text-align: right;\">0.131638   </td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">0.000569736</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         64.1916</td><td style=\"text-align: right;\">   0.122485 </td><td style=\"text-align: right;\">        0.0949383</td><td style=\"text-align: right;\"> 0.422941</td></tr>\n",
              "<tr><td>TorchTrainer_d0f8f668</td><td>TERMINATED</td><td>10.128.0.3:1196284</td><td style=\"text-align: right;\"> 96</td><td style=\"text-align: right;\">0.199139   </td><td style=\"text-align: right;\"> 640</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">7.86124e-05</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         65.1806</td><td style=\"text-align: right;\">   0.128456 </td><td style=\"text-align: right;\">        0.0883827</td><td style=\"text-align: right;\"> 0.428697</td></tr>\n",
              "<tr><td>TorchTrainer_8eaba676</td><td>TERMINATED</td><td>10.128.0.3:1197932</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">0.0672812  </td><td style=\"text-align: right;\"> 384</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">0.00167609 </td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         66.9804</td><td style=\"text-align: right;\">   0.12978  </td><td style=\"text-align: right;\">        0.161829 </td><td style=\"text-align: right;\"> 0.407775</td></tr>\n",
              "<tr><td>TorchTrainer_fe376ce8</td><td>TERMINATED</td><td>10.128.0.3:1199705</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">0.00878129 </td><td style=\"text-align: right;\"> 640</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">0.0925931  </td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         71.0558</td><td style=\"text-align: right;\">   0.122182 </td><td style=\"text-align: right;\">        0.0964442</td><td style=\"text-align: right;\"> 0.421762</td></tr>\n",
              "<tr><td>TorchTrainer_5fa0d29c</td><td>TERMINATED</td><td>10.128.0.3:1201471</td><td style=\"text-align: right;\"> 80</td><td style=\"text-align: right;\">0.106793   </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">0.000169121</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         65.4806</td><td style=\"text-align: right;\">   0.198289 </td><td style=\"text-align: right;\">        0.197974 </td><td style=\"text-align: right;\"> 0.460946</td></tr>\n",
              "<tr><td>TorchTrainer_a1a7455a</td><td>TERMINATED</td><td>10.128.0.3:1203165</td><td style=\"text-align: right;\">112</td><td style=\"text-align: right;\">0.138805   </td><td style=\"text-align: right;\"> 384</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">1.15342e-05</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         66.3255</td><td style=\"text-align: right;\">   0.179719 </td><td style=\"text-align: right;\">        0.201831 </td><td style=\"text-align: right;\"> 0.422031</td></tr>\n",
              "<tr><td>TorchTrainer_6d6626b0</td><td>TERMINATED</td><td>10.128.0.3:1204838</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">0.000715493</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">0.00195928 </td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         64.2467</td><td style=\"text-align: right;\">   0.0998363</td><td style=\"text-align: right;\">        0.0371965</td><td style=\"text-align: right;\"> 0.426872</td></tr>\n",
              "<tr><td>TorchTrainer_995662df</td><td>TERMINATED</td><td>10.128.0.3:1206599</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">0.0419536  </td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">0.00015407 </td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         75.2213</td><td style=\"text-align: right;\">   0.0997465</td><td style=\"text-align: right;\">        0.0264801</td><td style=\"text-align: right;\"> 0.400149</td></tr>\n",
              "<tr><td>TorchTrainer_774d392e</td><td>TERMINATED</td><td>10.128.0.3:1208418</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">0.0477954  </td><td style=\"text-align: right;\"> 384</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">6.10661e-05</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         71.7193</td><td style=\"text-align: right;\">   0.0842546</td><td style=\"text-align: right;\">        0.0433391</td><td style=\"text-align: right;\"> 0.419707</td></tr>\n",
              "<tr><td>TorchTrainer_137d3681</td><td>TERMINATED</td><td>10.128.0.3:1210279</td><td style=\"text-align: right;\"> 48</td><td style=\"text-align: right;\">0.0279941  </td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">0.000430296</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         63.2486</td><td style=\"text-align: right;\">   0.077075 </td><td style=\"text-align: right;\">        0.0633542</td><td style=\"text-align: right;\"> 0.405421</td></tr>\n",
              "<tr><td>TorchTrainer_70578d83</td><td>TERMINATED</td><td>10.128.0.3:1211915</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">0.0644705  </td><td style=\"text-align: right;\"> 768</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">0.000155424</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         66.4851</td><td style=\"text-align: right;\">   0.0904048</td><td style=\"text-align: right;\">        0.176685 </td><td style=\"text-align: right;\"> 0.415268</td></tr>\n",
              "<tr><td>TorchTrainer_5834913a</td><td>TERMINATED</td><td>10.128.0.3:1213545</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">0.0865596  </td><td style=\"text-align: right;\"> 384</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">0.000912688</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         71.214 </td><td style=\"text-align: right;\">   0.103626 </td><td style=\"text-align: right;\">        0.0788866</td><td style=\"text-align: right;\"> 0.423131</td></tr>\n",
              "<tr><td>TorchTrainer_ede8484b</td><td>TERMINATED</td><td>10.128.0.3:1215256</td><td style=\"text-align: right;\"> 48</td><td style=\"text-align: right;\">0.00365229 </td><td style=\"text-align: right;\"> 640</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">3.09465e-05</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         64.4084</td><td style=\"text-align: right;\">   0.0911068</td><td style=\"text-align: right;\">        0.131095 </td><td style=\"text-align: right;\"> 0.406214</td></tr>\n",
              "<tr><td>TorchTrainer_3cee4eec</td><td>TERMINATED</td><td>10.128.0.3:1216885</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">0.0362136  </td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">0.000282945</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         65.9485</td><td style=\"text-align: right;\">   0.0853543</td><td style=\"text-align: right;\">        0.0729531</td><td style=\"text-align: right;\"> 0.412268</td></tr>\n",
              "<tr><td>TorchTrainer_1bb12d79</td><td>TERMINATED</td><td>10.128.0.3:1218490</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">0.0553786  </td><td style=\"text-align: right;\"> 768</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">0.000988358</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         71.5901</td><td style=\"text-align: right;\">   0.0764173</td><td style=\"text-align: right;\">        0.0338355</td><td style=\"text-align: right;\"> 0.411848</td></tr>\n",
              "<tr><td>TorchTrainer_7c654a8e</td><td>TERMINATED</td><td>10.128.0.3:1220227</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">0.0965508  </td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">6.80944e-05</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         65.4465</td><td style=\"text-align: right;\">   0.169902 </td><td style=\"text-align: right;\">        0.127426 </td><td style=\"text-align: right;\"> 0.464765</td></tr>\n",
              "<tr><td>TorchTrainer_e1067dd5</td><td>TERMINATED</td><td>10.128.0.3:1221989</td><td style=\"text-align: right;\"> 80</td><td style=\"text-align: right;\">0.0166925  </td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">0.00361873 </td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         66.944 </td><td style=\"text-align: right;\">   0.146676 </td><td style=\"text-align: right;\">        0.178818 </td><td style=\"text-align: right;\"> 0.430102</td></tr>\n",
              "<tr><td>TorchTrainer_d71d2247</td><td>TERMINATED</td><td>10.128.0.3:1223743</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">0.0693347  </td><td style=\"text-align: right;\"> 384</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">4.20952e-05</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         64.8086</td><td style=\"text-align: right;\">   0.111356 </td><td style=\"text-align: right;\">        0.0672571</td><td style=\"text-align: right;\"> 0.458665</td></tr>\n",
              "<tr><td>TorchTrainer_0d3d2f70</td><td>TERMINATED</td><td>10.128.0.3:1225445</td><td style=\"text-align: right;\"> 48</td><td style=\"text-align: right;\">0.0157346  </td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">2.026e-05  </td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         63.1119</td><td style=\"text-align: right;\">   0.0952846</td><td style=\"text-align: right;\">        0.0854807</td><td style=\"text-align: right;\"> 0.399819</td></tr>\n",
              "<tr><td>TorchTrainer_f780394e</td><td>TERMINATED</td><td>10.128.0.3:1226968</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">0.0344512  </td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">0.000208779</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         72.6174</td><td style=\"text-align: right;\">   0.0927902</td><td style=\"text-align: right;\">        0.0805458</td><td style=\"text-align: right;\"> 0.407854</td></tr>\n",
              "</tbody>\n",
              "</table>\n",
              "  </div>\n",
              "</div>\n",
              "<style>\n",
              ".tuneStatus {\n",
              "  color: var(--jp-ui-font-color1);\n",
              "}\n",
              ".tuneStatus .systemInfo {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              ".tuneStatus td {\n",
              "  white-space: nowrap;\n",
              "}\n",
              ".tuneStatus .trialStatus {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              ".tuneStatus h3 {\n",
              "  font-weight: bold;\n",
              "}\n",
              ".tuneStatus .hDivider {\n",
              "  border-bottom-width: var(--jp-border-width);\n",
              "  border-bottom-color: var(--jp-border-color0);\n",
              "  border-bottom-style: solid;\n",
              "}\n",
              ".tuneStatus .vDivider {\n",
              "  border-left-width: var(--jp-border-width);\n",
              "  border-left-color: var(--jp-border-color0);\n",
              "  border-left-style: solid;\n",
              "  margin: 0.5em 1em 0.5em 1em;\n",
              "}\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:21:02,667 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1806 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(TorchTrainer pid=1177991)\u001b[0m Started distributed worker processes: \n",
            "\u001b[36m(TorchTrainer pid=1177991)\u001b[0m - (node_id=6cf2562c5de19587d3dc355f8ebead9f15bbc1bb9651b7563d063b82, ip=10.128.0.3, pid=1178182) world_rank=0, local_rank=0, node_rank=0\n",
            "\u001b[36m(RayTrainWorker pid=1178182)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(RayTrainWorker pid=1178182)\u001b[0m Getting checkpoint from /home/jupyter/AgenticADMET/notebooks/../output/artifacts/mol_mlm_roberta_zinc/last.ckpt...\n",
            "\u001b[36m(RayTrainWorker pid=1178182)\u001b[0m Loading checkpoint from roberta to roberta...\n",
            "\u001b[36m(RayTrainWorker pid=1178182)\u001b[0m <All keys matched successfully>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(RayTrainWorker pid=1178182)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/ray/train/lightning/_lightning_utils.py:262: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
            "\u001b[36m(RayTrainWorker pid=1178182)\u001b[0m `get_trial_name` is deprecated because the concept of a `Trial` will soon be removed in Ray Train.Ray Train will no longer assume that it's running within a Ray Tune `Trial` in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
            "\u001b[36m(RayTrainWorker pid=1178182)\u001b[0m GPU available: True (cuda), used: True\n",
            "\u001b[36m(RayTrainWorker pid=1178182)\u001b[0m TPU available: False, using: 0 TPU cores\n",
            "\u001b[36m(RayTrainWorker pid=1178182)\u001b[0m HPU available: False, using: 0 HPUs\n",
            "\u001b[36m(RayTrainWorker pid=1178182)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
            "\u001b[36m(RayTrainWorker pid=1178182)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\u001b[36m(RayTrainWorker pid=1178182)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=1178182)\u001b[0m   | Name      | Type         | Params | Mode \n",
            "\u001b[36m(RayTrainWorker pid=1178182)\u001b[0m ---------------------------------------------------\n",
            "\u001b[36m(RayTrainWorker pid=1178182)\u001b[0m 0 | roberta   | RobertaModel | 8.7 M  | train\n",
            "\u001b[36m(RayTrainWorker pid=1178182)\u001b[0m 1 | predictor | MLP          | 1.2 M  | train\n",
            "\u001b[36m(RayTrainWorker pid=1178182)\u001b[0m 2 | criterion | MSE          | 0      | train\n",
            "\u001b[36m(RayTrainWorker pid=1178182)\u001b[0m 3 | metrics   | ModuleList   | 0      | train\n",
            "\u001b[36m(RayTrainWorker pid=1178182)\u001b[0m ---------------------------------------------------\n",
            "\u001b[36m(RayTrainWorker pid=1178182)\u001b[0m 9.8 M     Trainable params\n",
            "\u001b[36m(RayTrainWorker pid=1178182)\u001b[0m 0         Non-trainable params\n",
            "\u001b[36m(RayTrainWorker pid=1178182)\u001b[0m 9.8 M     Total params\n",
            "\u001b[36m(RayTrainWorker pid=1178182)\u001b[0m 39.311    Total estimated model params size (MB)\n",
            "\u001b[36m(RayTrainWorker pid=1178182)\u001b[0m 132       Modules in train mode\n",
            "\u001b[36m(RayTrainWorker pid=1178182)\u001b[0m 0         Modules in eval mode\n",
            "\u001b[36m(RayTrainWorker pid=1178182)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
            "\u001b[36m(RayTrainWorker pid=1178182)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1178182)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
            "\u001b[36m(RayTrainWorker pid=1178182)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "\u001b[36m(RayTrainWorker pid=1178182)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('lr', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1178182)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('train_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1178182)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/56fc16e6/checkpoint_000000)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:21:12,676 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1805 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1178182)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/56fc16e6/checkpoint_000001)\n",
            "2025-03-12 23:21:13,043\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:21:14,218\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1178182)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/56fc16e6/checkpoint_000002)\n",
            "\u001b[36m(RayTrainWorker pid=1178182)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/56fc16e6/checkpoint_000003)\n",
            "2025-03-12 23:21:15,462\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:21:16,651\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1178182)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/56fc16e6/checkpoint_000004)\n",
            "2025-03-12 23:21:17,955\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1178182)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/56fc16e6/checkpoint_000005)\n",
            "\u001b[36m(RayTrainWorker pid=1178182)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/56fc16e6/checkpoint_000006)\n",
            "2025-03-12 23:21:19,119\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1178182)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/56fc16e6/checkpoint_000007)\n",
            "2025-03-12 23:21:20,227\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:21:21,384\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1178182)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/56fc16e6/checkpoint_000008)\n",
            "2025-03-12 23:21:22,572\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1178182)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/56fc16e6/checkpoint_000009)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:21:22,686 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1804 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:21:23,699\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1178182)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/56fc16e6/checkpoint_000010)\n",
            "2025-03-12 23:21:24,940\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1178182)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/56fc16e6/checkpoint_000011)\n",
            "2025-03-12 23:21:26,139\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1178182)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/56fc16e6/checkpoint_000012)\n",
            "2025-03-12 23:21:27,339\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1178182)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/56fc16e6/checkpoint_000013)\n",
            "\u001b[36m(RayTrainWorker pid=1178182)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/56fc16e6/checkpoint_000014)\n",
            "2025-03-12 23:21:28,538\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1178182)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/56fc16e6/checkpoint_000015)\n",
            "2025-03-12 23:21:29,763\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:21:30,934\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1178182)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/56fc16e6/checkpoint_000016)\n",
            "2025-03-12 23:21:32,124\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1178182)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/56fc16e6/checkpoint_000017)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:21:32,695 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1803 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:21:33,341\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1178182)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/56fc16e6/checkpoint_000018)\n",
            "2025-03-12 23:21:34,521\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1178182)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/56fc16e6/checkpoint_000019)\n",
            "\u001b[36m(RayTrainWorker pid=1178182)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/56fc16e6/checkpoint_000020)\n",
            "2025-03-12 23:21:35,710\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:21:36,906\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1178182)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/56fc16e6/checkpoint_000021)\n",
            "2025-03-12 23:21:38,071\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1178182)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/56fc16e6/checkpoint_000022)\n",
            "2025-03-12 23:21:39,180\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1178182)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/56fc16e6/checkpoint_000023)\n",
            "\u001b[36m(RayTrainWorker pid=1178182)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/56fc16e6/checkpoint_000024)\n",
            "2025-03-12 23:21:40,381\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:21:41,495\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1178182)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/56fc16e6/checkpoint_000025)\n",
            "\u001b[36m(RayTrainWorker pid=1178182)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/56fc16e6/checkpoint_000026)\n",
            "2025-03-12 23:21:42,712\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:21:42,703 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1803 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:21:43,940\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1178182)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/56fc16e6/checkpoint_000027)\n",
            "\u001b[36m(RayTrainWorker pid=1178182)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/56fc16e6/checkpoint_000028)\n",
            "2025-03-12 23:21:45,130\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1178182)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/56fc16e6/checkpoint_000029)\n",
            "2025-03-12 23:21:46,341\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1178182)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/56fc16e6/checkpoint_000030)\n",
            "2025-03-12 23:21:47,473\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:21:48,658\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1178182)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/56fc16e6/checkpoint_000031)\n",
            "2025-03-12 23:21:49,863\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1178182)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/56fc16e6/checkpoint_000032)\n",
            "2025-03-12 23:21:51,052\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1178182)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/56fc16e6/checkpoint_000033)\n",
            "2025-03-12 23:21:52,179\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1178182)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/56fc16e6/checkpoint_000034)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:21:52,712 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1802 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:21:53,271\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1178182)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/56fc16e6/checkpoint_000035)\n",
            "2025-03-12 23:21:54,400\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1178182)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/56fc16e6/checkpoint_000036)\n",
            "2025-03-12 23:21:55,612\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1178182)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/56fc16e6/checkpoint_000037)\n",
            "2025-03-12 23:21:56,739\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1178182)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/56fc16e6/checkpoint_000038)\n",
            "\u001b[36m(RayTrainWorker pid=1178182)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/56fc16e6/checkpoint_000039)\n",
            "2025-03-12 23:21:57,897\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:21:59,078\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1178182)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/56fc16e6/checkpoint_000040)\n",
            "\u001b[36m(RayTrainWorker pid=1178182)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/56fc16e6/checkpoint_000041)\n",
            "2025-03-12 23:22:00,226\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:22:01,382\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1178182)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/56fc16e6/checkpoint_000042)\n",
            "2025-03-12 23:22:02,586\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1178182)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/56fc16e6/checkpoint_000043)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:22:02,720 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.18 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1178182)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/56fc16e6/checkpoint_000044)\n",
            "2025-03-12 23:22:03,779\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1178182)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/56fc16e6/checkpoint_000045)\n",
            "2025-03-12 23:22:04,980\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:22:06,181\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1178182)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/56fc16e6/checkpoint_000046)\n",
            "2025-03-12 23:22:07,372\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1178182)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/56fc16e6/checkpoint_000047)\n",
            "2025-03-12 23:22:08,566\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1178182)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/56fc16e6/checkpoint_000048)\n",
            "\u001b[36m(RayTrainWorker pid=1178182)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/56fc16e6/checkpoint_000049)\n",
            "2025-03-12 23:22:09,752\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1178182)\u001b[0m `Trainer.fit` stopped: `max_epochs=50` reached.\n",
            "\u001b[36m(RayTrainWorker pid=1178182)\u001b[0m [rank0]:[W312 23:22:11.265836382 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:22:12,729 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.18 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1179805)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
            "\u001b[36m(TorchTrainer pid=1179654)\u001b[0m Started distributed worker processes: \n",
            "\u001b[36m(TorchTrainer pid=1179654)\u001b[0m - (node_id=6cf2562c5de19587d3dc355f8ebead9f15bbc1bb9651b7563d063b82, ip=10.128.0.3, pid=1179805) world_rank=0, local_rank=0, node_rank=0\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:22:22,737 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1798 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(RayTrainWorker pid=1179805)\u001b[0m Getting checkpoint from /home/jupyter/AgenticADMET/notebooks/../output/artifacts/mol_mlm_roberta_zinc/last.ckpt...\n",
            "\u001b[36m(RayTrainWorker pid=1179805)\u001b[0m Loading checkpoint from roberta to roberta...\n",
            "\u001b[36m(RayTrainWorker pid=1179805)\u001b[0m <All keys matched successfully>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(RayTrainWorker pid=1179805)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/ray/train/lightning/_lightning_utils.py:262: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
            "\u001b[36m(RayTrainWorker pid=1179805)\u001b[0m `get_trial_name` is deprecated because the concept of a `Trial` will soon be removed in Ray Train.Ray Train will no longer assume that it's running within a Ray Tune `Trial` in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
            "\u001b[36m(RayTrainWorker pid=1179805)\u001b[0m GPU available: True (cuda), used: True\n",
            "\u001b[36m(RayTrainWorker pid=1179805)\u001b[0m TPU available: False, using: 0 TPU cores\n",
            "\u001b[36m(RayTrainWorker pid=1179805)\u001b[0m HPU available: False, using: 0 HPUs\n",
            "\u001b[36m(RayTrainWorker pid=1179805)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
            "\u001b[36m(RayTrainWorker pid=1179805)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\u001b[36m(RayTrainWorker pid=1179805)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=1179805)\u001b[0m   | Name      | Type         | Params | Mode \n",
            "\u001b[36m(RayTrainWorker pid=1179805)\u001b[0m ---------------------------------------------------\n",
            "\u001b[36m(RayTrainWorker pid=1179805)\u001b[0m 0 | roberta   | RobertaModel | 8.7 M  | train\n",
            "\u001b[36m(RayTrainWorker pid=1179805)\u001b[0m 1 | predictor | MLP          | 890 K  | train\n",
            "\u001b[36m(RayTrainWorker pid=1179805)\u001b[0m 2 | criterion | MSE          | 0      | train\n",
            "\u001b[36m(RayTrainWorker pid=1179805)\u001b[0m 3 | metrics   | ModuleList   | 0      | train\n",
            "\u001b[36m(RayTrainWorker pid=1179805)\u001b[0m ---------------------------------------------------\n",
            "\u001b[36m(RayTrainWorker pid=1179805)\u001b[0m 9.6 M     Trainable params\n",
            "\u001b[36m(RayTrainWorker pid=1179805)\u001b[0m 0         Non-trainable params\n",
            "\u001b[36m(RayTrainWorker pid=1179805)\u001b[0m 9.6 M     Total params\n",
            "\u001b[36m(RayTrainWorker pid=1179805)\u001b[0m 38.259    Total estimated model params size (MB)\n",
            "\u001b[36m(RayTrainWorker pid=1179805)\u001b[0m 132       Modules in train mode\n",
            "\u001b[36m(RayTrainWorker pid=1179805)\u001b[0m 0         Modules in eval mode\n",
            "\u001b[36m(RayTrainWorker pid=1179805)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
            "\u001b[36m(RayTrainWorker pid=1179805)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1179805)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
            "\u001b[36m(RayTrainWorker pid=1179805)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (11) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "\u001b[36m(RayTrainWorker pid=1179805)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('lr', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1179805)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('train_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1179805)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/b2eb9de9/checkpoint_000000)\n",
            "2025-03-12 23:22:28,805\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1179805)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/b2eb9de9/checkpoint_000001)\n",
            "2025-03-12 23:22:29,976\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1179805)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/b2eb9de9/checkpoint_000002)\n",
            "2025-03-12 23:22:31,157\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1179805)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/b2eb9de9/checkpoint_000003)\n",
            "\u001b[36m(RayTrainWorker pid=1179805)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/b2eb9de9/checkpoint_000004)\n",
            "2025-03-12 23:22:32,319\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:22:32,746 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1797 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1179805)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/b2eb9de9/checkpoint_000005)\n",
            "2025-03-12 23:22:33,444\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:22:34,587\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1179805)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/b2eb9de9/checkpoint_000006)\n",
            "2025-03-12 23:22:35,752\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1179805)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/b2eb9de9/checkpoint_000007)\n",
            "2025-03-12 23:22:36,906\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1179805)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/b2eb9de9/checkpoint_000008)\n",
            "2025-03-12 23:22:38,055\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1179805)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/b2eb9de9/checkpoint_000009)\n",
            "\u001b[36m(RayTrainWorker pid=1179805)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/b2eb9de9/checkpoint_000010)\n",
            "2025-03-12 23:22:39,211\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:22:40,379\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1179805)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/b2eb9de9/checkpoint_000011)\n",
            "\u001b[36m(RayTrainWorker pid=1179805)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/b2eb9de9/checkpoint_000012)\n",
            "2025-03-12 23:22:41,565\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:22:42,754 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1798 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1179805)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/b2eb9de9/checkpoint_000013)\n",
            "2025-03-12 23:22:42,759\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:22:43,932\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1179805)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/b2eb9de9/checkpoint_000014)\n",
            "\u001b[36m(RayTrainWorker pid=1179805)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/b2eb9de9/checkpoint_000015)\n",
            "2025-03-12 23:22:45,126\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:22:46,266\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1179805)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/b2eb9de9/checkpoint_000016)\n",
            "2025-03-12 23:22:47,450\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1179805)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/b2eb9de9/checkpoint_000017)\n",
            "2025-03-12 23:22:48,604\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1179805)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/b2eb9de9/checkpoint_000018)\n",
            "\u001b[36m(RayTrainWorker pid=1179805)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/b2eb9de9/checkpoint_000019)\n",
            "2025-03-12 23:22:49,792\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:22:50,937\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1179805)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/b2eb9de9/checkpoint_000020)\n",
            "2025-03-12 23:22:52,136\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1179805)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/b2eb9de9/checkpoint_000021)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:22:52,763 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1797 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1179805)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/b2eb9de9/checkpoint_000022)\n",
            "2025-03-12 23:22:53,322\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:22:54,483\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1179805)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/b2eb9de9/checkpoint_000023)\n",
            "\u001b[36m(RayTrainWorker pid=1179805)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/b2eb9de9/checkpoint_000024)\n",
            "2025-03-12 23:22:55,667\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:22:56,820\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1179805)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/b2eb9de9/checkpoint_000025)\n",
            "2025-03-12 23:22:58,005\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1179805)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/b2eb9de9/checkpoint_000026)\n",
            "\u001b[36m(RayTrainWorker pid=1179805)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/b2eb9de9/checkpoint_000027)\n",
            "2025-03-12 23:22:59,183\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:23:00,338\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1179805)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/b2eb9de9/checkpoint_000028)\n",
            "\u001b[36m(RayTrainWorker pid=1179805)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/b2eb9de9/checkpoint_000029)\n",
            "2025-03-12 23:23:01,515\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:23:02,705\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1179805)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/b2eb9de9/checkpoint_000030)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:23:02,771 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1795 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:23:03,901\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1179805)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/b2eb9de9/checkpoint_000031)\n",
            "\u001b[36m(RayTrainWorker pid=1179805)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/b2eb9de9/checkpoint_000032)\n",
            "2025-03-12 23:23:05,081\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:23:06,231\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1179805)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/b2eb9de9/checkpoint_000033)\n",
            "2025-03-12 23:23:07,363\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1179805)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/b2eb9de9/checkpoint_000034)\n",
            "2025-03-12 23:23:08,500\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1179805)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/b2eb9de9/checkpoint_000035)\n",
            "2025-03-12 23:23:09,653\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1179805)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/b2eb9de9/checkpoint_000036)\n",
            "2025-03-12 23:23:10,832\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1179805)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/b2eb9de9/checkpoint_000037)\n",
            "2025-03-12 23:23:12,008\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1179805)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/b2eb9de9/checkpoint_000038)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:23:12,781 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1794 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:23:13,156\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1179805)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/b2eb9de9/checkpoint_000039)\n",
            "\u001b[36m(RayTrainWorker pid=1179805)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/b2eb9de9/checkpoint_000040)\n",
            "2025-03-12 23:23:14,332\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:23:15,456\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1179805)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/b2eb9de9/checkpoint_000041)\n",
            "\u001b[36m(RayTrainWorker pid=1179805)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/b2eb9de9/checkpoint_000042)\n",
            "2025-03-12 23:23:16,638\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:23:17,810\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1179805)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/b2eb9de9/checkpoint_000043)\n",
            "2025-03-12 23:23:19,002\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1179805)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/b2eb9de9/checkpoint_000044)\n",
            "2025-03-12 23:23:20,164\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1179805)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/b2eb9de9/checkpoint_000045)\n",
            "2025-03-12 23:23:21,320\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1179805)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/b2eb9de9/checkpoint_000046)\n",
            "2025-03-12 23:23:22,497\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1179805)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/b2eb9de9/checkpoint_000047)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:23:22,789 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1793 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:23:23,683\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1179805)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/b2eb9de9/checkpoint_000048)\n",
            "\u001b[36m(RayTrainWorker pid=1179805)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/b2eb9de9/checkpoint_000049)\n",
            "2025-03-12 23:23:24,860\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1179805)\u001b[0m `Trainer.fit` stopped: `max_epochs=50` reached.\n",
            "\u001b[36m(RayTrainWorker pid=1179805)\u001b[0m [rank0]:[W312 23:23:26.370266771 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:23:32,797 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1792 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(TorchTrainer pid=1181264)\u001b[0m Started distributed worker processes: \n",
            "\u001b[36m(TorchTrainer pid=1181264)\u001b[0m - (node_id=6cf2562c5de19587d3dc355f8ebead9f15bbc1bb9651b7563d063b82, ip=10.128.0.3, pid=1181415) world_rank=0, local_rank=0, node_rank=0\n",
            "\u001b[36m(RayTrainWorker pid=1181415)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(RayTrainWorker pid=1181415)\u001b[0m Getting checkpoint from /home/jupyter/AgenticADMET/notebooks/../output/artifacts/mol_mlm_roberta_zinc/last.ckpt...\n",
            "\u001b[36m(RayTrainWorker pid=1181415)\u001b[0m Loading checkpoint from roberta to roberta...\n",
            "\u001b[36m(RayTrainWorker pid=1181415)\u001b[0m <All keys matched successfully>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(RayTrainWorker pid=1181415)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/ray/train/lightning/_lightning_utils.py:262: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
            "\u001b[36m(RayTrainWorker pid=1181415)\u001b[0m `get_trial_name` is deprecated because the concept of a `Trial` will soon be removed in Ray Train.Ray Train will no longer assume that it's running within a Ray Tune `Trial` in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
            "\u001b[36m(RayTrainWorker pid=1181415)\u001b[0m GPU available: True (cuda), used: True\n",
            "\u001b[36m(RayTrainWorker pid=1181415)\u001b[0m TPU available: False, using: 0 TPU cores\n",
            "\u001b[36m(RayTrainWorker pid=1181415)\u001b[0m HPU available: False, using: 0 HPUs\n",
            "\u001b[36m(RayTrainWorker pid=1181415)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
            "\u001b[36m(RayTrainWorker pid=1181415)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\u001b[36m(RayTrainWorker pid=1181415)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=1181415)\u001b[0m   | Name      | Type         | Params | Mode \n",
            "\u001b[36m(RayTrainWorker pid=1181415)\u001b[0m ---------------------------------------------------\n",
            "\u001b[36m(RayTrainWorker pid=1181415)\u001b[0m 0 | roberta   | RobertaModel | 8.7 M  | train\n",
            "\u001b[36m(RayTrainWorker pid=1181415)\u001b[0m 1 | predictor | MLP          | 49.9 K | train\n",
            "\u001b[36m(RayTrainWorker pid=1181415)\u001b[0m 2 | criterion | MSE          | 0      | train\n",
            "\u001b[36m(RayTrainWorker pid=1181415)\u001b[0m 3 | metrics   | ModuleList   | 0      | train\n",
            "\u001b[36m(RayTrainWorker pid=1181415)\u001b[0m ---------------------------------------------------\n",
            "\u001b[36m(RayTrainWorker pid=1181415)\u001b[0m 8.7 M     Trainable params\n",
            "\u001b[36m(RayTrainWorker pid=1181415)\u001b[0m 0         Non-trainable params\n",
            "\u001b[36m(RayTrainWorker pid=1181415)\u001b[0m 8.7 M     Total params\n",
            "\u001b[36m(RayTrainWorker pid=1181415)\u001b[0m 34.898    Total estimated model params size (MB)\n",
            "\u001b[36m(RayTrainWorker pid=1181415)\u001b[0m 128       Modules in train mode\n",
            "\u001b[36m(RayTrainWorker pid=1181415)\u001b[0m 0         Modules in eval mode\n",
            "\u001b[36m(RayTrainWorker pid=1181415)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
            "\u001b[36m(RayTrainWorker pid=1181415)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1181415)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
            "\u001b[36m(RayTrainWorker pid=1181415)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "\u001b[36m(RayTrainWorker pid=1181415)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('lr', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1181415)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('train_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:23:42,805 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.0815 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1181415)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/bd4b22c9/checkpoint_000000)\n",
            "2025-03-12 23:23:43,944\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1181415)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/bd4b22c9/checkpoint_000001)\n",
            "2025-03-12 23:23:45,068\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1181415)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/bd4b22c9/checkpoint_000002)\n",
            "2025-03-12 23:23:46,199\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1181415)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/bd4b22c9/checkpoint_000003)\n",
            "2025-03-12 23:23:47,298\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1181415)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/bd4b22c9/checkpoint_000004)\n",
            "2025-03-12 23:23:48,400\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1181415)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/bd4b22c9/checkpoint_000005)\n",
            "2025-03-12 23:23:49,530\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1181415)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/bd4b22c9/checkpoint_000006)\n",
            "2025-03-12 23:23:50,665\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1181415)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/bd4b22c9/checkpoint_000007)\n",
            "2025-03-12 23:23:51,757\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1181415)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/bd4b22c9/checkpoint_000008)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:23:52,814 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.0814 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:23:52,869\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1181415)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/bd4b22c9/checkpoint_000009)\n",
            "2025-03-12 23:23:53,976\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1181415)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/bd4b22c9/checkpoint_000010)\n",
            "\u001b[36m(RayTrainWorker pid=1181415)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/bd4b22c9/checkpoint_000011)\n",
            "2025-03-12 23:23:55,180\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1181415)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/bd4b22c9/checkpoint_000012)\n",
            "2025-03-12 23:23:56,293\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:23:57,425\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1181415)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/bd4b22c9/checkpoint_000013)\n",
            "2025-03-12 23:23:58,543\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1181415)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/bd4b22c9/checkpoint_000014)\n",
            "\u001b[36m(RayTrainWorker pid=1181415)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/bd4b22c9/checkpoint_000015)\n",
            "2025-03-12 23:23:59,654\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:24:00,822\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1181415)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/bd4b22c9/checkpoint_000016)\n",
            "2025-03-12 23:24:01,907\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1181415)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/bd4b22c9/checkpoint_000017)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:24:02,822 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1169 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:24:03,028\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1181415)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/bd4b22c9/checkpoint_000018)\n",
            "2025-03-12 23:24:04,146\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1181415)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/bd4b22c9/checkpoint_000019)\n",
            "2025-03-12 23:24:05,274\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1181415)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/bd4b22c9/checkpoint_000020)\n",
            "2025-03-12 23:24:06,354\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1181415)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/bd4b22c9/checkpoint_000021)\n",
            "2025-03-12 23:24:07,493\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1181415)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/bd4b22c9/checkpoint_000022)\n",
            "2025-03-12 23:24:08,610\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1181415)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/bd4b22c9/checkpoint_000023)\n",
            "\u001b[36m(RayTrainWorker pid=1181415)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/bd4b22c9/checkpoint_000024)\n",
            "2025-03-12 23:24:09,794\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:24:10,930\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1181415)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/bd4b22c9/checkpoint_000025)\n",
            "2025-03-12 23:24:12,077\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1181415)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/bd4b22c9/checkpoint_000026)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:24:12,831 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1787 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:24:13,226\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1181415)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/bd4b22c9/checkpoint_000027)\n",
            "2025-03-12 23:24:14,343\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1181415)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/bd4b22c9/checkpoint_000028)\n",
            "2025-03-12 23:24:15,494\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1181415)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/bd4b22c9/checkpoint_000029)\n",
            "2025-03-12 23:24:16,655\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1181415)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/bd4b22c9/checkpoint_000030)\n",
            "2025-03-12 23:24:17,784\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1181415)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/bd4b22c9/checkpoint_000031)\n",
            "2025-03-12 23:24:18,909\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1181415)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/bd4b22c9/checkpoint_000032)\n",
            "2025-03-12 23:24:20,080\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1181415)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/bd4b22c9/checkpoint_000033)\n",
            "2025-03-12 23:24:21,201\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1181415)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/bd4b22c9/checkpoint_000034)\n",
            "2025-03-12 23:24:22,359\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1181415)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/bd4b22c9/checkpoint_000035)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:24:22,840 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1787 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:24:23,488\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1181415)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/bd4b22c9/checkpoint_000036)\n",
            "2025-03-12 23:24:24,663\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1181415)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/bd4b22c9/checkpoint_000037)\n",
            "2025-03-12 23:24:25,807\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1181415)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/bd4b22c9/checkpoint_000038)\n",
            "\u001b[36m(RayTrainWorker pid=1181415)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/bd4b22c9/checkpoint_000039)\n",
            "2025-03-12 23:24:26,953\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:24:28,086\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1181415)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/bd4b22c9/checkpoint_000040)\n",
            "2025-03-12 23:24:29,254\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1181415)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/bd4b22c9/checkpoint_000041)\n",
            "2025-03-12 23:24:30,368\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1181415)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/bd4b22c9/checkpoint_000042)\n",
            "2025-03-12 23:24:31,486\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1181415)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/bd4b22c9/checkpoint_000043)\n",
            "\u001b[36m(RayTrainWorker pid=1181415)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/bd4b22c9/checkpoint_000044)\n",
            "2025-03-12 23:24:32,642\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:24:32,848 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1786 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1181415)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/bd4b22c9/checkpoint_000045)\n",
            "2025-03-12 23:24:33,747\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1181415)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/bd4b22c9/checkpoint_000046)\n",
            "2025-03-12 23:24:34,874\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:24:36,023\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1181415)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/bd4b22c9/checkpoint_000047)\n",
            "\u001b[36m(RayTrainWorker pid=1181415)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/bd4b22c9/checkpoint_000048)\n",
            "2025-03-12 23:24:37,097\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:24:38,274\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1181415)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/bd4b22c9/checkpoint_000049)\n",
            "\u001b[36m(RayTrainWorker pid=1181415)\u001b[0m `Trainer.fit` stopped: `max_epochs=50` reached.\n",
            "\u001b[36m(RayTrainWorker pid=1181415)\u001b[0m [rank0]:[W312 23:24:40.753683395 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:24:42,857 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1785 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(TorchTrainer pid=1182831)\u001b[0m Started distributed worker processes: \n",
            "\u001b[36m(TorchTrainer pid=1182831)\u001b[0m - (node_id=6cf2562c5de19587d3dc355f8ebead9f15bbc1bb9651b7563d063b82, ip=10.128.0.3, pid=1183060) world_rank=0, local_rank=0, node_rank=0\n",
            "\u001b[36m(RayTrainWorker pid=1183060)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:24:52,865 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1784 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(RayTrainWorker pid=1183060)\u001b[0m <All keys matched successfully>\n",
            "\u001b[36m(RayTrainWorker pid=1183060)\u001b[0m Getting checkpoint from /home/jupyter/AgenticADMET/notebooks/../output/artifacts/mol_mlm_roberta_zinc/last.ckpt...\n",
            "\u001b[36m(RayTrainWorker pid=1183060)\u001b[0m Loading checkpoint from roberta to roberta...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(RayTrainWorker pid=1183060)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/ray/train/lightning/_lightning_utils.py:262: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
            "\u001b[36m(RayTrainWorker pid=1183060)\u001b[0m `get_trial_name` is deprecated because the concept of a `Trial` will soon be removed in Ray Train.Ray Train will no longer assume that it's running within a Ray Tune `Trial` in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
            "\u001b[36m(RayTrainWorker pid=1183060)\u001b[0m GPU available: True (cuda), used: True\n",
            "\u001b[36m(RayTrainWorker pid=1183060)\u001b[0m TPU available: False, using: 0 TPU cores\n",
            "\u001b[36m(RayTrainWorker pid=1183060)\u001b[0m HPU available: False, using: 0 HPUs\n",
            "\u001b[36m(RayTrainWorker pid=1183060)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
            "\u001b[36m(RayTrainWorker pid=1183060)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\u001b[36m(RayTrainWorker pid=1183060)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=1183060)\u001b[0m   | Name      | Type         | Params | Mode \n",
            "\u001b[36m(RayTrainWorker pid=1183060)\u001b[0m ---------------------------------------------------\n",
            "\u001b[36m(RayTrainWorker pid=1183060)\u001b[0m 0 | roberta   | RobertaModel | 8.7 M  | train\n",
            "\u001b[36m(RayTrainWorker pid=1183060)\u001b[0m 1 | predictor | MLP          | 99.8 K | train\n",
            "\u001b[36m(RayTrainWorker pid=1183060)\u001b[0m 2 | criterion | MSE          | 0      | train\n",
            "\u001b[36m(RayTrainWorker pid=1183060)\u001b[0m 3 | metrics   | ModuleList   | 0      | train\n",
            "\u001b[36m(RayTrainWorker pid=1183060)\u001b[0m ---------------------------------------------------\n",
            "\u001b[36m(RayTrainWorker pid=1183060)\u001b[0m 8.8 M     Trainable params\n",
            "\u001b[36m(RayTrainWorker pid=1183060)\u001b[0m 0         Non-trainable params\n",
            "\u001b[36m(RayTrainWorker pid=1183060)\u001b[0m 8.8 M     Total params\n",
            "\u001b[36m(RayTrainWorker pid=1183060)\u001b[0m 35.098    Total estimated model params size (MB)\n",
            "\u001b[36m(RayTrainWorker pid=1183060)\u001b[0m 128       Modules in train mode\n",
            "\u001b[36m(RayTrainWorker pid=1183060)\u001b[0m 0         Modules in eval mode\n",
            "\u001b[36m(RayTrainWorker pid=1183060)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
            "\u001b[36m(RayTrainWorker pid=1183060)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1183060)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
            "\u001b[36m(RayTrainWorker pid=1183060)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "\u001b[36m(RayTrainWorker pid=1183060)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('lr', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1183060)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('train_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1183060)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/439b693d/checkpoint_000000)\n",
            "2025-03-12 23:24:57,955\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1183060)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/439b693d/checkpoint_000001)\n",
            "2025-03-12 23:24:59,088\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1183060)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/439b693d/checkpoint_000002)\n",
            "2025-03-12 23:25:00,206\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1183060)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/439b693d/checkpoint_000003)\n",
            "2025-03-12 23:25:01,315\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1183060)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/439b693d/checkpoint_000004)\n",
            "\u001b[36m(RayTrainWorker pid=1183060)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/439b693d/checkpoint_000005)\n",
            "2025-03-12 23:25:02,481\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:25:02,874 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1782 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1183060)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/439b693d/checkpoint_000006)\n",
            "2025-03-12 23:25:03,618\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:25:04,756\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1183060)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/439b693d/checkpoint_000007)\n",
            "2025-03-12 23:25:05,804\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1183060)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/439b693d/checkpoint_000008)\n",
            "\u001b[36m(RayTrainWorker pid=1183060)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/439b693d/checkpoint_000009)\n",
            "2025-03-12 23:25:06,960\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:25:08,097\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1183060)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/439b693d/checkpoint_000010)\n",
            "2025-03-12 23:25:09,194\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1183060)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/439b693d/checkpoint_000011)\n",
            "2025-03-12 23:25:10,333\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1183060)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/439b693d/checkpoint_000012)\n",
            "2025-03-12 23:25:11,488\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1183060)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/439b693d/checkpoint_000013)\n",
            "\u001b[36m(RayTrainWorker pid=1183060)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/439b693d/checkpoint_000014)\n",
            "2025-03-12 23:25:12,643\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:25:12,883 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1781 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1183060)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/439b693d/checkpoint_000015)\n",
            "2025-03-12 23:25:13,797\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:25:14,919\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1183060)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/439b693d/checkpoint_000016)\n",
            "2025-03-12 23:25:16,016\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1183060)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/439b693d/checkpoint_000017)\n",
            "2025-03-12 23:25:17,177\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1183060)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/439b693d/checkpoint_000018)\n",
            "\u001b[36m(RayTrainWorker pid=1183060)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/439b693d/checkpoint_000019)\n",
            "2025-03-12 23:25:18,332\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:25:19,413\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1183060)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/439b693d/checkpoint_000020)\n",
            "\u001b[36m(RayTrainWorker pid=1183060)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/439b693d/checkpoint_000021)\n",
            "2025-03-12 23:25:20,571\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:25:21,714\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1183060)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/439b693d/checkpoint_000022)\n",
            "2025-03-12 23:25:22,888\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1183060)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/439b693d/checkpoint_000023)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:25:22,891 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.178 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:25:23,973\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1183060)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/439b693d/checkpoint_000024)\n",
            "\u001b[36m(RayTrainWorker pid=1183060)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/439b693d/checkpoint_000025)\n",
            "2025-03-12 23:25:25,154\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:25:26,266\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1183060)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/439b693d/checkpoint_000026)\n",
            "2025-03-12 23:25:27,425\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1183060)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/439b693d/checkpoint_000027)\n",
            "2025-03-12 23:25:28,577\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1183060)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/439b693d/checkpoint_000028)\n",
            "\u001b[36m(RayTrainWorker pid=1183060)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/439b693d/checkpoint_000029)\n",
            "2025-03-12 23:25:29,728\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1183060)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/439b693d/checkpoint_000030)\n",
            "2025-03-12 23:25:30,827\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1183060)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/439b693d/checkpoint_000031)\n",
            "2025-03-12 23:25:31,933\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:25:32,900 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.0868 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:25:33,098\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1183060)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/439b693d/checkpoint_000032)\n",
            "2025-03-12 23:25:34,193\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1183060)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/439b693d/checkpoint_000033)\n",
            "2025-03-12 23:25:35,255\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1183060)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/439b693d/checkpoint_000034)\n",
            "\u001b[36m(RayTrainWorker pid=1183060)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/439b693d/checkpoint_000035)\n",
            "2025-03-12 23:25:36,410\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:25:37,556\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1183060)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/439b693d/checkpoint_000036)\n",
            "2025-03-12 23:25:38,684\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1183060)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/439b693d/checkpoint_000037)\n",
            "\u001b[36m(RayTrainWorker pid=1183060)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/439b693d/checkpoint_000038)\n",
            "2025-03-12 23:25:39,862\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:25:41,054\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1183060)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/439b693d/checkpoint_000039)\n",
            "2025-03-12 23:25:42,152\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1183060)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/439b693d/checkpoint_000040)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:25:42,908 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1779 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1183060)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/439b693d/checkpoint_000041)\n",
            "2025-03-12 23:25:43,308\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1183060)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/439b693d/checkpoint_000042)\n",
            "2025-03-12 23:25:44,442\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:25:45,545\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1183060)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/439b693d/checkpoint_000043)\n",
            "2025-03-12 23:25:46,715\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1183060)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/439b693d/checkpoint_000044)\n",
            "2025-03-12 23:25:47,846\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1183060)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/439b693d/checkpoint_000045)\n",
            "2025-03-12 23:25:48,965\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1183060)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/439b693d/checkpoint_000046)\n",
            "2025-03-12 23:25:50,071\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1183060)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/439b693d/checkpoint_000047)\n",
            "2025-03-12 23:25:51,209\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1183060)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/439b693d/checkpoint_000048)\n",
            "2025-03-12 23:25:52,321\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1183060)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/439b693d/checkpoint_000049)\n",
            "\u001b[36m(RayTrainWorker pid=1183060)\u001b[0m `Trainer.fit` stopped: `max_epochs=50` reached.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:25:52,917 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1778 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1183060)\u001b[0m [rank0]:[W312 23:25:54.916366127 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:26:02,925 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1776 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(TorchTrainer pid=1184502)\u001b[0m Started distributed worker processes: \n",
            "\u001b[36m(TorchTrainer pid=1184502)\u001b[0m - (node_id=6cf2562c5de19587d3dc355f8ebead9f15bbc1bb9651b7563d063b82, ip=10.128.0.3, pid=1184670) world_rank=0, local_rank=0, node_rank=0\n",
            "\u001b[36m(RayTrainWorker pid=1184670)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
            "\u001b[36m(RayTrainWorker pid=1184670)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/ray/train/lightning/_lightning_utils.py:262: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
            "\u001b[36m(RayTrainWorker pid=1184670)\u001b[0m `get_trial_name` is deprecated because the concept of a `Trial` will soon be removed in Ray Train.Ray Train will no longer assume that it's running within a Ray Tune `Trial` in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
            "\u001b[36m(RayTrainWorker pid=1184670)\u001b[0m GPU available: True (cuda), used: True\n",
            "\u001b[36m(RayTrainWorker pid=1184670)\u001b[0m TPU available: False, using: 0 TPU cores\n",
            "\u001b[36m(RayTrainWorker pid=1184670)\u001b[0m HPU available: False, using: 0 HPUs\n",
            "\u001b[36m(RayTrainWorker pid=1184670)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(RayTrainWorker pid=1184670)\u001b[0m Getting checkpoint from /home/jupyter/AgenticADMET/notebooks/../output/artifacts/mol_mlm_roberta_zinc/last.ckpt...\n",
            "\u001b[36m(RayTrainWorker pid=1184670)\u001b[0m Loading checkpoint from roberta to roberta...\n",
            "\u001b[36m(RayTrainWorker pid=1184670)\u001b[0m <All keys matched successfully>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(RayTrainWorker pid=1184670)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\u001b[36m(RayTrainWorker pid=1184670)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=1184670)\u001b[0m   | Name      | Type         | Params | Mode \n",
            "\u001b[36m(RayTrainWorker pid=1184670)\u001b[0m ---------------------------------------------------\n",
            "\u001b[36m(RayTrainWorker pid=1184670)\u001b[0m 0 | roberta   | RobertaModel | 8.7 M  | train\n",
            "\u001b[36m(RayTrainWorker pid=1184670)\u001b[0m 1 | predictor | MLP          | 349 K  | train\n",
            "\u001b[36m(RayTrainWorker pid=1184670)\u001b[0m 2 | criterion | MSE          | 0      | train\n",
            "\u001b[36m(RayTrainWorker pid=1184670)\u001b[0m 3 | metrics   | ModuleList   | 0      | train\n",
            "\u001b[36m(RayTrainWorker pid=1184670)\u001b[0m ---------------------------------------------------\n",
            "\u001b[36m(RayTrainWorker pid=1184670)\u001b[0m 9.0 M     Trainable params\n",
            "\u001b[36m(RayTrainWorker pid=1184670)\u001b[0m 0         Non-trainable params\n",
            "\u001b[36m(RayTrainWorker pid=1184670)\u001b[0m 9.0 M     Total params\n",
            "\u001b[36m(RayTrainWorker pid=1184670)\u001b[0m 36.096    Total estimated model params size (MB)\n",
            "\u001b[36m(RayTrainWorker pid=1184670)\u001b[0m 128       Modules in train mode\n",
            "\u001b[36m(RayTrainWorker pid=1184670)\u001b[0m 0         Modules in eval mode\n",
            "\u001b[36m(RayTrainWorker pid=1184670)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
            "\u001b[36m(RayTrainWorker pid=1184670)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1184670)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
            "\u001b[36m(RayTrainWorker pid=1184670)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (11) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "\u001b[36m(RayTrainWorker pid=1184670)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('lr', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1184670)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('train_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1184670)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e0d70199/checkpoint_000000)\n",
            "2025-03-12 23:26:12,087\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1184670)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e0d70199/checkpoint_000001)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:26:12,934 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1588 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1184670)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e0d70199/checkpoint_000002)\n",
            "2025-03-12 23:26:13,184\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:26:14,320\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1184670)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e0d70199/checkpoint_000003)\n",
            "2025-03-12 23:26:15,479\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1184670)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e0d70199/checkpoint_000004)\n",
            "2025-03-12 23:26:16,624\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1184670)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e0d70199/checkpoint_000005)\n",
            "2025-03-12 23:26:17,764\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1184670)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e0d70199/checkpoint_000006)\n",
            "2025-03-12 23:26:18,940\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1184670)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e0d70199/checkpoint_000007)\n",
            "\u001b[36m(RayTrainWorker pid=1184670)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e0d70199/checkpoint_000008)\n",
            "2025-03-12 23:26:20,120\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:26:21,183\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1184670)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e0d70199/checkpoint_000009)\n",
            "2025-03-12 23:26:22,364\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1184670)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e0d70199/checkpoint_000010)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:26:22,942 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1774 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1184670)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e0d70199/checkpoint_000011)\n",
            "2025-03-12 23:26:23,559\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:26:24,728\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1184670)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e0d70199/checkpoint_000012)\n",
            "2025-03-12 23:26:25,852\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1184670)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e0d70199/checkpoint_000013)\n",
            "2025-03-12 23:26:26,988\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1184670)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e0d70199/checkpoint_000014)\n",
            "\u001b[36m(RayTrainWorker pid=1184670)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e0d70199/checkpoint_000015)\n",
            "2025-03-12 23:26:28,151\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1184670)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e0d70199/checkpoint_000016)\n",
            "2025-03-12 23:26:29,367\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:26:30,475\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1184670)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e0d70199/checkpoint_000017)\n",
            "2025-03-12 23:26:31,648\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1184670)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e0d70199/checkpoint_000018)\n",
            "2025-03-12 23:26:32,758\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1184670)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e0d70199/checkpoint_000019)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:26:32,951 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1773 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1184670)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e0d70199/checkpoint_000020)\n",
            "2025-03-12 23:26:33,940\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1184670)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e0d70199/checkpoint_000021)\n",
            "2025-03-12 23:26:35,045\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:26:36,167\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1184670)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e0d70199/checkpoint_000022)\n",
            "2025-03-12 23:26:37,344\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1184670)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e0d70199/checkpoint_000023)\n",
            "\u001b[36m(RayTrainWorker pid=1184670)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e0d70199/checkpoint_000024)\n",
            "2025-03-12 23:26:38,490\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1184670)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e0d70199/checkpoint_000025)\n",
            "2025-03-12 23:26:39,631\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:26:40,771\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1184670)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e0d70199/checkpoint_000026)\n",
            "2025-03-12 23:26:41,899\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1184670)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e0d70199/checkpoint_000027)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:26:42,960 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.0763 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:26:43,047\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1184670)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e0d70199/checkpoint_000028)\n",
            "2025-03-12 23:26:44,147\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1184670)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e0d70199/checkpoint_000029)\n",
            "\u001b[36m(RayTrainWorker pid=1184670)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e0d70199/checkpoint_000030)\n",
            "2025-03-12 23:26:45,338\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:26:46,512\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1184670)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e0d70199/checkpoint_000031)\n",
            "2025-03-12 23:26:47,624\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1184670)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e0d70199/checkpoint_000032)\n",
            "2025-03-12 23:26:48,794\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1184670)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e0d70199/checkpoint_000033)\n",
            "2025-03-12 23:26:49,939\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1184670)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e0d70199/checkpoint_000034)\n",
            "2025-03-12 23:26:51,069\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1184670)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e0d70199/checkpoint_000035)\n",
            "\u001b[36m(RayTrainWorker pid=1184670)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e0d70199/checkpoint_000036)\n",
            "2025-03-12 23:26:52,223\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:26:52,968 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1772 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:26:53,399\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1184670)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e0d70199/checkpoint_000037)\n",
            "2025-03-12 23:26:54,592\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1184670)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e0d70199/checkpoint_000038)\n",
            "2025-03-12 23:26:55,722\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1184670)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e0d70199/checkpoint_000039)\n",
            "2025-03-12 23:26:56,876\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1184670)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e0d70199/checkpoint_000040)\n",
            "2025-03-12 23:26:58,005\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1184670)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e0d70199/checkpoint_000041)\n",
            "2025-03-12 23:26:59,112\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1184670)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e0d70199/checkpoint_000042)\n",
            "\u001b[36m(RayTrainWorker pid=1184670)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e0d70199/checkpoint_000043)\n",
            "2025-03-12 23:27:00,248\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1184670)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e0d70199/checkpoint_000044)\n",
            "2025-03-12 23:27:01,383\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:27:02,525\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1184670)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e0d70199/checkpoint_000045)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:27:02,977 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1771 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1184670)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e0d70199/checkpoint_000046)\n",
            "2025-03-12 23:27:03,637\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:27:04,804\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1184670)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e0d70199/checkpoint_000047)\n",
            "2025-03-12 23:27:05,915\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1184670)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e0d70199/checkpoint_000048)\n",
            "2025-03-12 23:27:07,086\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1184670)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e0d70199/checkpoint_000049)\n",
            "\u001b[36m(RayTrainWorker pid=1184670)\u001b[0m `Trainer.fit` stopped: `max_epochs=50` reached.\n",
            "\u001b[36m(RayTrainWorker pid=1184670)\u001b[0m [rank0]:[W312 23:27:08.552670854 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:27:12,986 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.177 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(TorchTrainer pid=1186232)\u001b[0m Started distributed worker processes: \n",
            "\u001b[36m(TorchTrainer pid=1186232)\u001b[0m - (node_id=6cf2562c5de19587d3dc355f8ebead9f15bbc1bb9651b7563d063b82, ip=10.128.0.3, pid=1186470) world_rank=0, local_rank=0, node_rank=0\n",
            "\u001b[36m(RayTrainWorker pid=1186470)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:27:22,994 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1769 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(RayTrainWorker pid=1186470)\u001b[0m <All keys matched successfully>\n",
            "\u001b[36m(RayTrainWorker pid=1186470)\u001b[0m Getting checkpoint from /home/jupyter/AgenticADMET/notebooks/../output/artifacts/mol_mlm_roberta_zinc/last.ckpt...\n",
            "\u001b[36m(RayTrainWorker pid=1186470)\u001b[0m Loading checkpoint from roberta to roberta...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(RayTrainWorker pid=1186470)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/ray/train/lightning/_lightning_utils.py:262: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
            "\u001b[36m(RayTrainWorker pid=1186470)\u001b[0m `get_trial_name` is deprecated because the concept of a `Trial` will soon be removed in Ray Train.Ray Train will no longer assume that it's running within a Ray Tune `Trial` in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
            "\u001b[36m(RayTrainWorker pid=1186470)\u001b[0m GPU available: True (cuda), used: True\n",
            "\u001b[36m(RayTrainWorker pid=1186470)\u001b[0m TPU available: False, using: 0 TPU cores\n",
            "\u001b[36m(RayTrainWorker pid=1186470)\u001b[0m HPU available: False, using: 0 HPUs\n",
            "\u001b[36m(RayTrainWorker pid=1186470)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
            "\u001b[36m(RayTrainWorker pid=1186470)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\u001b[36m(RayTrainWorker pid=1186470)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=1186470)\u001b[0m   | Name      | Type         | Params | Mode \n",
            "\u001b[36m(RayTrainWorker pid=1186470)\u001b[0m ---------------------------------------------------\n",
            "\u001b[36m(RayTrainWorker pid=1186470)\u001b[0m 0 | roberta   | RobertaModel | 8.7 M  | train\n",
            "\u001b[36m(RayTrainWorker pid=1186470)\u001b[0m 1 | predictor | MLP          | 149 K  | train\n",
            "\u001b[36m(RayTrainWorker pid=1186470)\u001b[0m 2 | criterion | MSE          | 0      | train\n",
            "\u001b[36m(RayTrainWorker pid=1186470)\u001b[0m 3 | metrics   | ModuleList   | 0      | train\n",
            "\u001b[36m(RayTrainWorker pid=1186470)\u001b[0m ---------------------------------------------------\n",
            "\u001b[36m(RayTrainWorker pid=1186470)\u001b[0m 8.8 M     Trainable params\n",
            "\u001b[36m(RayTrainWorker pid=1186470)\u001b[0m 0         Non-trainable params\n",
            "\u001b[36m(RayTrainWorker pid=1186470)\u001b[0m 8.8 M     Total params\n",
            "\u001b[36m(RayTrainWorker pid=1186470)\u001b[0m 35.297    Total estimated model params size (MB)\n",
            "\u001b[36m(RayTrainWorker pid=1186470)\u001b[0m 128       Modules in train mode\n",
            "\u001b[36m(RayTrainWorker pid=1186470)\u001b[0m 0         Modules in eval mode\n",
            "\u001b[36m(RayTrainWorker pid=1186470)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
            "\u001b[36m(RayTrainWorker pid=1186470)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1186470)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
            "\u001b[36m(RayTrainWorker pid=1186470)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (11) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "\u001b[36m(RayTrainWorker pid=1186470)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('lr', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1186470)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('train_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1186470)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5a576c82/checkpoint_000000)\n",
            "\u001b[36m(RayTrainWorker pid=1186470)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5a576c82/checkpoint_000001)\n",
            "2025-03-12 23:27:26,974\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:27:28,061\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1186470)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5a576c82/checkpoint_000002)\n",
            "2025-03-12 23:27:29,137\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1186470)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5a576c82/checkpoint_000003)\n",
            "\u001b[36m(RayTrainWorker pid=1186470)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5a576c82/checkpoint_000004)\n",
            "2025-03-12 23:27:30,276\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:27:31,353\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1186470)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5a576c82/checkpoint_000005)\n",
            "\u001b[36m(RayTrainWorker pid=1186470)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5a576c82/checkpoint_000006)\n",
            "2025-03-12 23:27:32,485\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:27:33,002 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1767 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:27:33,655\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1186470)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5a576c82/checkpoint_000007)\n",
            "2025-03-12 23:27:34,733\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1186470)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5a576c82/checkpoint_000008)\n",
            "2025-03-12 23:27:35,867\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1186470)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5a576c82/checkpoint_000009)\n",
            "\u001b[36m(RayTrainWorker pid=1186470)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5a576c82/checkpoint_000010)\n",
            "2025-03-12 23:27:36,982\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:27:38,119\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1186470)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5a576c82/checkpoint_000011)\n",
            "2025-03-12 23:27:39,256\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1186470)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5a576c82/checkpoint_000012)\n",
            "2025-03-12 23:27:40,400\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1186470)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5a576c82/checkpoint_000013)\n",
            "\u001b[36m(RayTrainWorker pid=1186470)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5a576c82/checkpoint_000014)\n",
            "2025-03-12 23:27:41,552\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1186470)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5a576c82/checkpoint_000015)\n",
            "2025-03-12 23:27:42,665\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:27:43,011 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1767 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1186470)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5a576c82/checkpoint_000016)\n",
            "2025-03-12 23:27:43,787\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:27:44,937\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1186470)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5a576c82/checkpoint_000017)\n",
            "2025-03-12 23:27:46,076\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1186470)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5a576c82/checkpoint_000018)\n",
            "2025-03-12 23:27:47,200\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1186470)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5a576c82/checkpoint_000019)\n",
            "2025-03-12 23:27:48,321\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1186470)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5a576c82/checkpoint_000020)\n",
            "2025-03-12 23:27:49,407\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1186470)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5a576c82/checkpoint_000021)\n",
            "2025-03-12 23:27:50,546\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1186470)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5a576c82/checkpoint_000022)\n",
            "2025-03-12 23:27:51,644\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1186470)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5a576c82/checkpoint_000023)\n",
            "\u001b[36m(RayTrainWorker pid=1186470)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5a576c82/checkpoint_000024)\n",
            "2025-03-12 23:27:52,733\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:27:53,019 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1766 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:27:53,898\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1186470)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5a576c82/checkpoint_000025)\n",
            "2025-03-12 23:27:55,054\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1186470)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5a576c82/checkpoint_000026)\n",
            "2025-03-12 23:27:56,174\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1186470)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5a576c82/checkpoint_000027)\n",
            "2025-03-12 23:27:57,290\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1186470)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5a576c82/checkpoint_000028)\n",
            "\u001b[36m(RayTrainWorker pid=1186470)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5a576c82/checkpoint_000029)\n",
            "2025-03-12 23:27:58,423\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1186470)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5a576c82/checkpoint_000030)\n",
            "2025-03-12 23:27:59,547\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:28:00,686\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1186470)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5a576c82/checkpoint_000031)\n",
            "2025-03-12 23:28:01,821\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1186470)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5a576c82/checkpoint_000032)\n",
            "2025-03-12 23:28:02,962\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1186470)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5a576c82/checkpoint_000033)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:28:03,028 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1765 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1186470)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5a576c82/checkpoint_000034)\n",
            "2025-03-12 23:28:04,038\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:28:05,178\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1186470)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5a576c82/checkpoint_000035)\n",
            "2025-03-12 23:28:06,301\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1186470)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5a576c82/checkpoint_000036)\n",
            "2025-03-12 23:28:07,455\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1186470)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5a576c82/checkpoint_000037)\n",
            "\u001b[36m(RayTrainWorker pid=1186470)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5a576c82/checkpoint_000038)\n",
            "2025-03-12 23:28:08,603\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1186470)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5a576c82/checkpoint_000039)\n",
            "2025-03-12 23:28:09,720\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1186470)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5a576c82/checkpoint_000040)\n",
            "2025-03-12 23:28:10,848\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:28:11,987\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1186470)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5a576c82/checkpoint_000041)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:28:13,037 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.0776 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1186470)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5a576c82/checkpoint_000042)\n",
            "2025-03-12 23:28:13,074\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:28:14,179\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1186470)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5a576c82/checkpoint_000043)\n",
            "\u001b[36m(RayTrainWorker pid=1186470)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5a576c82/checkpoint_000044)\n",
            "2025-03-12 23:28:15,328\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:28:16,488\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1186470)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5a576c82/checkpoint_000045)\n",
            "2025-03-12 23:28:17,639\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1186470)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5a576c82/checkpoint_000046)\n",
            "2025-03-12 23:28:18,766\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1186470)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5a576c82/checkpoint_000047)\n",
            "2025-03-12 23:28:19,926\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1186470)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5a576c82/checkpoint_000048)\n",
            "2025-03-12 23:28:21,104\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1186470)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5a576c82/checkpoint_000049)\n",
            "\u001b[36m(RayTrainWorker pid=1186470)\u001b[0m `Trainer.fit` stopped: `max_epochs=50` reached.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:28:23,045 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1763 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1186470)\u001b[0m [rank0]:[W312 23:28:23.712867241 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:28:33,054 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1762 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1188093)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
            "\u001b[36m(TorchTrainer pid=1187943)\u001b[0m Started distributed worker processes: \n",
            "\u001b[36m(TorchTrainer pid=1187943)\u001b[0m - (node_id=6cf2562c5de19587d3dc355f8ebead9f15bbc1bb9651b7563d063b82, ip=10.128.0.3, pid=1188093) world_rank=0, local_rank=0, node_rank=0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(RayTrainWorker pid=1188093)\u001b[0m Getting checkpoint from /home/jupyter/AgenticADMET/notebooks/../output/artifacts/mol_mlm_roberta_zinc/last.ckpt...\n",
            "\u001b[36m(RayTrainWorker pid=1188093)\u001b[0m Loading checkpoint from roberta to roberta...\n",
            "\u001b[36m(RayTrainWorker pid=1188093)\u001b[0m <All keys matched successfully>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(RayTrainWorker pid=1188093)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/ray/train/lightning/_lightning_utils.py:262: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
            "\u001b[36m(RayTrainWorker pid=1188093)\u001b[0m `get_trial_name` is deprecated because the concept of a `Trial` will soon be removed in Ray Train.Ray Train will no longer assume that it's running within a Ray Tune `Trial` in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
            "\u001b[36m(RayTrainWorker pid=1188093)\u001b[0m GPU available: True (cuda), used: True\n",
            "\u001b[36m(RayTrainWorker pid=1188093)\u001b[0m TPU available: False, using: 0 TPU cores\n",
            "\u001b[36m(RayTrainWorker pid=1188093)\u001b[0m HPU available: False, using: 0 HPUs\n",
            "\u001b[36m(RayTrainWorker pid=1188093)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
            "\u001b[36m(RayTrainWorker pid=1188093)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\u001b[36m(RayTrainWorker pid=1188093)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=1188093)\u001b[0m   | Name      | Type         | Params | Mode \n",
            "\u001b[36m(RayTrainWorker pid=1188093)\u001b[0m ---------------------------------------------------\n",
            "\u001b[36m(RayTrainWorker pid=1188093)\u001b[0m 0 | roberta   | RobertaModel | 8.7 M  | train\n",
            "\u001b[36m(RayTrainWorker pid=1188093)\u001b[0m 1 | predictor | MLP          | 165 K  | train\n",
            "\u001b[36m(RayTrainWorker pid=1188093)\u001b[0m 2 | criterion | MSE          | 0      | train\n",
            "\u001b[36m(RayTrainWorker pid=1188093)\u001b[0m 3 | metrics   | ModuleList   | 0      | train\n",
            "\u001b[36m(RayTrainWorker pid=1188093)\u001b[0m ---------------------------------------------------\n",
            "\u001b[36m(RayTrainWorker pid=1188093)\u001b[0m 8.8 M     Trainable params\n",
            "\u001b[36m(RayTrainWorker pid=1188093)\u001b[0m 0         Non-trainable params\n",
            "\u001b[36m(RayTrainWorker pid=1188093)\u001b[0m 8.8 M     Total params\n",
            "\u001b[36m(RayTrainWorker pid=1188093)\u001b[0m 35.361    Total estimated model params size (MB)\n",
            "\u001b[36m(RayTrainWorker pid=1188093)\u001b[0m 132       Modules in train mode\n",
            "\u001b[36m(RayTrainWorker pid=1188093)\u001b[0m 0         Modules in eval mode\n",
            "\u001b[36m(RayTrainWorker pid=1188093)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
            "\u001b[36m(RayTrainWorker pid=1188093)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1188093)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
            "\u001b[36m(RayTrainWorker pid=1188093)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "\u001b[36m(RayTrainWorker pid=1188093)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('lr', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1188093)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('train_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1188093)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/08c9cf27/checkpoint_000000)\n",
            "2025-03-12 23:28:40,904\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1188093)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/08c9cf27/checkpoint_000001)\n",
            "\u001b[36m(RayTrainWorker pid=1188093)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/08c9cf27/checkpoint_000002)\n",
            "2025-03-12 23:28:41,993\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:28:43,068\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:28:43,063 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.0772 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1188093)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/08c9cf27/checkpoint_000003)\n",
            "\u001b[36m(RayTrainWorker pid=1188093)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/08c9cf27/checkpoint_000004)\n",
            "2025-03-12 23:28:44,218\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:28:45,284\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1188093)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/08c9cf27/checkpoint_000005)\n",
            "2025-03-12 23:28:46,395\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1188093)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/08c9cf27/checkpoint_000006)\n",
            "2025-03-12 23:28:47,453\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1188093)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/08c9cf27/checkpoint_000007)\n",
            "2025-03-12 23:28:48,546\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1188093)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/08c9cf27/checkpoint_000008)\n",
            "\u001b[36m(RayTrainWorker pid=1188093)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/08c9cf27/checkpoint_000009)\n",
            "2025-03-12 23:28:49,619\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:28:50,655\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1188093)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/08c9cf27/checkpoint_000010)\n",
            "2025-03-12 23:28:51,800\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1188093)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/08c9cf27/checkpoint_000011)\n",
            "2025-03-12 23:28:52,833\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1188093)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/08c9cf27/checkpoint_000012)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:28:53,071 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.176 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:28:53,939\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1188093)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/08c9cf27/checkpoint_000013)\n",
            "\u001b[36m(RayTrainWorker pid=1188093)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/08c9cf27/checkpoint_000014)\n",
            "2025-03-12 23:28:55,022\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:28:56,112\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1188093)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/08c9cf27/checkpoint_000015)\n",
            "2025-03-12 23:28:57,233\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1188093)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/08c9cf27/checkpoint_000016)\n",
            "2025-03-12 23:28:58,347\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1188093)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/08c9cf27/checkpoint_000017)\n",
            "2025-03-12 23:28:59,440\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1188093)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/08c9cf27/checkpoint_000018)\n",
            "2025-03-12 23:29:00,523\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1188093)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/08c9cf27/checkpoint_000019)\n",
            "2025-03-12 23:29:01,666\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1188093)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/08c9cf27/checkpoint_000020)\n",
            "2025-03-12 23:29:02,840\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1188093)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/08c9cf27/checkpoint_000021)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:29:03,080 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1759 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:29:03,933\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1188093)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/08c9cf27/checkpoint_000022)\n",
            "2025-03-12 23:29:05,020\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1188093)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/08c9cf27/checkpoint_000023)\n",
            "\u001b[36m(RayTrainWorker pid=1188093)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/08c9cf27/checkpoint_000024)\n",
            "2025-03-12 23:29:06,108\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1188093)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/08c9cf27/checkpoint_000025)\n",
            "2025-03-12 23:29:07,243\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1188093)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/08c9cf27/checkpoint_000026)\n",
            "2025-03-12 23:29:08,372\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1188093)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/08c9cf27/checkpoint_000027)\n",
            "2025-03-12 23:29:09,474\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1188093)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/08c9cf27/checkpoint_000028)\n",
            "2025-03-12 23:29:10,594\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:29:11,678\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1188093)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/08c9cf27/checkpoint_000029)\n",
            "\u001b[36m(RayTrainWorker pid=1188093)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/08c9cf27/checkpoint_000030)\n",
            "2025-03-12 23:29:12,733\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:29:13,088 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1758 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1188093)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/08c9cf27/checkpoint_000031)\n",
            "2025-03-12 23:29:13,853\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1188093)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/08c9cf27/checkpoint_000032)\n",
            "2025-03-12 23:29:14,988\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:29:16,012\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1188093)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/08c9cf27/checkpoint_000033)\n",
            "2025-03-12 23:29:17,138\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1188093)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/08c9cf27/checkpoint_000034)\n",
            "2025-03-12 23:29:18,295\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1188093)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/08c9cf27/checkpoint_000035)\n",
            "\u001b[36m(RayTrainWorker pid=1188093)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/08c9cf27/checkpoint_000036)\n",
            "2025-03-12 23:29:19,361\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1188093)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/08c9cf27/checkpoint_000037)\n",
            "2025-03-12 23:29:20,488\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1188093)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/08c9cf27/checkpoint_000038)\n",
            "2025-03-12 23:29:21,610\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1188093)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/08c9cf27/checkpoint_000039)\n",
            "2025-03-12 23:29:22,737\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:29:23,097 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1758 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1188093)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/08c9cf27/checkpoint_000040)\n",
            "2025-03-12 23:29:23,844\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1188093)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/08c9cf27/checkpoint_000041)\n",
            "2025-03-12 23:29:24,979\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:29:26,074\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1188093)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/08c9cf27/checkpoint_000042)\n",
            "2025-03-12 23:29:27,194\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1188093)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/08c9cf27/checkpoint_000043)\n",
            "2025-03-12 23:29:28,311\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1188093)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/08c9cf27/checkpoint_000044)\n",
            "2025-03-12 23:29:29,418\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1188093)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/08c9cf27/checkpoint_000045)\n",
            "2025-03-12 23:29:30,535\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1188093)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/08c9cf27/checkpoint_000046)\n",
            "2025-03-12 23:29:31,634\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1188093)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/08c9cf27/checkpoint_000047)\n",
            "2025-03-12 23:29:32,745\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1188093)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/08c9cf27/checkpoint_000048)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:29:33,106 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1757 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:29:33,894\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1188093)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/08c9cf27/checkpoint_000049)\n",
            "\u001b[36m(RayTrainWorker pid=1188093)\u001b[0m `Trainer.fit` stopped: `max_epochs=50` reached.\n",
            "\u001b[36m(RayTrainWorker pid=1188093)\u001b[0m [rank0]:[W312 23:29:35.486424149 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:29:43,114 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1756 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(TorchTrainer pid=1189481)\u001b[0m Started distributed worker processes: \n",
            "\u001b[36m(TorchTrainer pid=1189481)\u001b[0m - (node_id=6cf2562c5de19587d3dc355f8ebead9f15bbc1bb9651b7563d063b82, ip=10.128.0.3, pid=1189646) world_rank=0, local_rank=0, node_rank=0\n",
            "\u001b[36m(RayTrainWorker pid=1189646)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(RayTrainWorker pid=1189646)\u001b[0m Getting checkpoint from /home/jupyter/AgenticADMET/notebooks/../output/artifacts/mol_mlm_roberta_zinc/last.ckpt...\n",
            "\u001b[36m(RayTrainWorker pid=1189646)\u001b[0m Loading checkpoint from roberta to roberta...\n",
            "\u001b[36m(RayTrainWorker pid=1189646)\u001b[0m <All keys matched successfully>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(RayTrainWorker pid=1189646)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/ray/train/lightning/_lightning_utils.py:262: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
            "\u001b[36m(RayTrainWorker pid=1189646)\u001b[0m `get_trial_name` is deprecated because the concept of a `Trial` will soon be removed in Ray Train.Ray Train will no longer assume that it's running within a Ray Tune `Trial` in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
            "\u001b[36m(RayTrainWorker pid=1189646)\u001b[0m GPU available: True (cuda), used: True\n",
            "\u001b[36m(RayTrainWorker pid=1189646)\u001b[0m TPU available: False, using: 0 TPU cores\n",
            "\u001b[36m(RayTrainWorker pid=1189646)\u001b[0m HPU available: False, using: 0 HPUs\n",
            "\u001b[36m(RayTrainWorker pid=1189646)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
            "\u001b[36m(RayTrainWorker pid=1189646)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\u001b[36m(RayTrainWorker pid=1189646)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=1189646)\u001b[0m   | Name      | Type         | Params | Mode \n",
            "\u001b[36m(RayTrainWorker pid=1189646)\u001b[0m ---------------------------------------------------\n",
            "\u001b[36m(RayTrainWorker pid=1189646)\u001b[0m 0 | roberta   | RobertaModel | 8.7 M  | train\n",
            "\u001b[36m(RayTrainWorker pid=1189646)\u001b[0m 1 | predictor | MLP          | 349 K  | train\n",
            "\u001b[36m(RayTrainWorker pid=1189646)\u001b[0m 2 | criterion | MSE          | 0      | train\n",
            "\u001b[36m(RayTrainWorker pid=1189646)\u001b[0m 3 | metrics   | ModuleList   | 0      | train\n",
            "\u001b[36m(RayTrainWorker pid=1189646)\u001b[0m ---------------------------------------------------\n",
            "\u001b[36m(RayTrainWorker pid=1189646)\u001b[0m 9.0 M     Trainable params\n",
            "\u001b[36m(RayTrainWorker pid=1189646)\u001b[0m 0         Non-trainable params\n",
            "\u001b[36m(RayTrainWorker pid=1189646)\u001b[0m 9.0 M     Total params\n",
            "\u001b[36m(RayTrainWorker pid=1189646)\u001b[0m 36.096    Total estimated model params size (MB)\n",
            "\u001b[36m(RayTrainWorker pid=1189646)\u001b[0m 128       Modules in train mode\n",
            "\u001b[36m(RayTrainWorker pid=1189646)\u001b[0m 0         Modules in eval mode\n",
            "\u001b[36m(RayTrainWorker pid=1189646)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
            "\u001b[36m(RayTrainWorker pid=1189646)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1189646)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
            "\u001b[36m(RayTrainWorker pid=1189646)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "\u001b[36m(RayTrainWorker pid=1189646)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('lr', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1189646)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('train_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1189646)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/599a7bc7/checkpoint_000000)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:29:53,123 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1755 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:29:53,472\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1189646)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/599a7bc7/checkpoint_000001)\n",
            "2025-03-12 23:29:54,588\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1189646)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/599a7bc7/checkpoint_000002)\n",
            "2025-03-12 23:29:55,762\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1189646)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/599a7bc7/checkpoint_000003)\n",
            "2025-03-12 23:29:56,913\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1189646)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/599a7bc7/checkpoint_000004)\n",
            "2025-03-12 23:29:58,127\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1189646)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/599a7bc7/checkpoint_000005)\n",
            "2025-03-12 23:29:59,262\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1189646)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/599a7bc7/checkpoint_000006)\n",
            "2025-03-12 23:30:00,357\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1189646)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/599a7bc7/checkpoint_000007)\n",
            "\u001b[36m(RayTrainWorker pid=1189646)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/599a7bc7/checkpoint_000008)\n",
            "2025-03-12 23:30:01,533\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:30:02,728\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1189646)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/599a7bc7/checkpoint_000009)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:30:03,131 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1753 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:30:03,931\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1189646)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/599a7bc7/checkpoint_000010)\n",
            "2025-03-12 23:30:05,071\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1189646)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/599a7bc7/checkpoint_000011)\n",
            "2025-03-12 23:30:06,261\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1189646)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/599a7bc7/checkpoint_000012)\n",
            "2025-03-12 23:30:07,434\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1189646)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/599a7bc7/checkpoint_000013)\n",
            "2025-03-12 23:30:08,638\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1189646)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/599a7bc7/checkpoint_000014)\n",
            "2025-03-12 23:30:09,858\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1189646)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/599a7bc7/checkpoint_000015)\n",
            "2025-03-12 23:30:11,051\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1189646)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/599a7bc7/checkpoint_000016)\n",
            "2025-03-12 23:30:12,230\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1189646)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/599a7bc7/checkpoint_000017)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:30:13,140 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1753 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:30:13,406\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1189646)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/599a7bc7/checkpoint_000018)\n",
            "2025-03-12 23:30:14,551\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1189646)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/599a7bc7/checkpoint_000019)\n",
            "\u001b[36m(RayTrainWorker pid=1189646)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/599a7bc7/checkpoint_000020)\n",
            "2025-03-12 23:30:15,727\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:30:16,921\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1189646)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/599a7bc7/checkpoint_000021)\n",
            "\u001b[36m(RayTrainWorker pid=1189646)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/599a7bc7/checkpoint_000022)\n",
            "2025-03-12 23:30:18,078\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1189646)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/599a7bc7/checkpoint_000023)\n",
            "2025-03-12 23:30:19,170\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:30:20,390\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1189646)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/599a7bc7/checkpoint_000024)\n",
            "\u001b[36m(RayTrainWorker pid=1189646)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/599a7bc7/checkpoint_000025)\n",
            "2025-03-12 23:30:21,523\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:30:22,693\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1189646)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/599a7bc7/checkpoint_000026)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:30:23,148 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1747 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:30:23,905\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1189646)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/599a7bc7/checkpoint_000027)\n",
            "2025-03-12 23:30:25,127\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1189646)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/599a7bc7/checkpoint_000028)\n",
            "\u001b[36m(RayTrainWorker pid=1189646)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/599a7bc7/checkpoint_000029)\n",
            "2025-03-12 23:30:26,335\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:30:27,478\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1189646)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/599a7bc7/checkpoint_000030)\n",
            "2025-03-12 23:30:28,681\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1189646)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/599a7bc7/checkpoint_000031)\n",
            "2025-03-12 23:30:29,866\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1189646)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/599a7bc7/checkpoint_000032)\n",
            "2025-03-12 23:30:31,063\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1189646)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/599a7bc7/checkpoint_000033)\n",
            "2025-03-12 23:30:32,202\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1189646)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/599a7bc7/checkpoint_000034)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:30:33,156 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1526 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:30:33,405\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1189646)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/599a7bc7/checkpoint_000035)\n",
            "2025-03-12 23:30:34,551\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1189646)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/599a7bc7/checkpoint_000036)\n",
            "\u001b[36m(RayTrainWorker pid=1189646)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/599a7bc7/checkpoint_000037)\n",
            "2025-03-12 23:30:35,796\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1189646)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/599a7bc7/checkpoint_000038)\n",
            "2025-03-12 23:30:36,920\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:30:38,121\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1189646)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/599a7bc7/checkpoint_000039)\n",
            "2025-03-12 23:30:39,316\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1189646)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/599a7bc7/checkpoint_000040)\n",
            "\u001b[36m(RayTrainWorker pid=1189646)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/599a7bc7/checkpoint_000041)\n",
            "2025-03-12 23:30:40,481\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1189646)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/599a7bc7/checkpoint_000042)\n",
            "2025-03-12 23:30:41,592\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1189646)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/599a7bc7/checkpoint_000043)\n",
            "2025-03-12 23:30:42,820\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:30:43,164 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1751 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:30:43,921\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1189646)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/599a7bc7/checkpoint_000044)\n",
            "2025-03-12 23:30:45,097\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1189646)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/599a7bc7/checkpoint_000045)\n",
            "\u001b[36m(RayTrainWorker pid=1189646)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/599a7bc7/checkpoint_000046)\n",
            "2025-03-12 23:30:46,276\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:30:47,496\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1189646)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/599a7bc7/checkpoint_000047)\n",
            "2025-03-12 23:30:48,689\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1189646)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/599a7bc7/checkpoint_000048)\n",
            "\u001b[36m(RayTrainWorker pid=1189646)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/599a7bc7/checkpoint_000049)\n",
            "2025-03-12 23:30:49,865\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1189646)\u001b[0m `Trainer.fit` stopped: `max_epochs=50` reached.\n",
            "\u001b[36m(RayTrainWorker pid=1189646)\u001b[0m [rank0]:[W312 23:30:51.491454377 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:30:53,173 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.175 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(TorchTrainer pid=1191042)\u001b[0m Started distributed worker processes: \n",
            "\u001b[36m(TorchTrainer pid=1191042)\u001b[0m - (node_id=6cf2562c5de19587d3dc355f8ebead9f15bbc1bb9651b7563d063b82, ip=10.128.0.3, pid=1191270) world_rank=0, local_rank=0, node_rank=0\n",
            "\u001b[36m(RayTrainWorker pid=1191270)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:31:03,181 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1748 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(RayTrainWorker pid=1191270)\u001b[0m Getting checkpoint from /home/jupyter/AgenticADMET/notebooks/../output/artifacts/mol_mlm_roberta_zinc/last.ckpt...\n",
            "\u001b[36m(RayTrainWorker pid=1191270)\u001b[0m Loading checkpoint from roberta to roberta...\n",
            "\u001b[36m(RayTrainWorker pid=1191270)\u001b[0m <All keys matched successfully>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(RayTrainWorker pid=1191270)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/ray/train/lightning/_lightning_utils.py:262: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
            "\u001b[36m(RayTrainWorker pid=1191270)\u001b[0m `get_trial_name` is deprecated because the concept of a `Trial` will soon be removed in Ray Train.Ray Train will no longer assume that it's running within a Ray Tune `Trial` in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
            "\u001b[36m(RayTrainWorker pid=1191270)\u001b[0m GPU available: True (cuda), used: True\n",
            "\u001b[36m(RayTrainWorker pid=1191270)\u001b[0m TPU available: False, using: 0 TPU cores\n",
            "\u001b[36m(RayTrainWorker pid=1191270)\u001b[0m HPU available: False, using: 0 HPUs\n",
            "\u001b[36m(RayTrainWorker pid=1191270)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
            "\u001b[36m(RayTrainWorker pid=1191270)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\u001b[36m(RayTrainWorker pid=1191270)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=1191270)\u001b[0m   | Name      | Type         | Params | Mode \n",
            "\u001b[36m(RayTrainWorker pid=1191270)\u001b[0m ---------------------------------------------------\n",
            "\u001b[36m(RayTrainWorker pid=1191270)\u001b[0m 0 | roberta   | RobertaModel | 8.7 M  | train\n",
            "\u001b[36m(RayTrainWorker pid=1191270)\u001b[0m 1 | predictor | MLP          | 1.4 M  | train\n",
            "\u001b[36m(RayTrainWorker pid=1191270)\u001b[0m 2 | criterion | MSE          | 0      | train\n",
            "\u001b[36m(RayTrainWorker pid=1191270)\u001b[0m 3 | metrics   | ModuleList   | 0      | train\n",
            "\u001b[36m(RayTrainWorker pid=1191270)\u001b[0m ---------------------------------------------------\n",
            "\u001b[36m(RayTrainWorker pid=1191270)\u001b[0m 10.1 M    Trainable params\n",
            "\u001b[36m(RayTrainWorker pid=1191270)\u001b[0m 0         Non-trainable params\n",
            "\u001b[36m(RayTrainWorker pid=1191270)\u001b[0m 10.1 M    Total params\n",
            "\u001b[36m(RayTrainWorker pid=1191270)\u001b[0m 40.494    Total estimated model params size (MB)\n",
            "\u001b[36m(RayTrainWorker pid=1191270)\u001b[0m 132       Modules in train mode\n",
            "\u001b[36m(RayTrainWorker pid=1191270)\u001b[0m 0         Modules in eval mode\n",
            "\u001b[36m(RayTrainWorker pid=1191270)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
            "\u001b[36m(RayTrainWorker pid=1191270)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1191270)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
            "\u001b[36m(RayTrainWorker pid=1191270)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (21) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "\u001b[36m(RayTrainWorker pid=1191270)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('lr', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1191270)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('train_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1191270)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5f4597d7/checkpoint_000000)\n",
            "2025-03-12 23:31:09,486\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1191270)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5f4597d7/checkpoint_000001)\n",
            "2025-03-12 23:31:10,786\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1191270)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5f4597d7/checkpoint_000002)\n",
            "2025-03-12 23:31:12,084\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1191270)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5f4597d7/checkpoint_000003)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:31:13,190 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.0614 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1191270)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5f4597d7/checkpoint_000004)\n",
            "2025-03-12 23:31:13,404\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1191270)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5f4597d7/checkpoint_000005)\n",
            "2025-03-12 23:31:14,732\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:31:16,009\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1191270)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5f4597d7/checkpoint_000006)\n",
            "2025-03-12 23:31:17,339\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1191270)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5f4597d7/checkpoint_000007)\n",
            "\u001b[36m(RayTrainWorker pid=1191270)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5f4597d7/checkpoint_000008)\n",
            "2025-03-12 23:31:18,731\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:31:20,019\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1191270)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5f4597d7/checkpoint_000009)\n",
            "\u001b[36m(RayTrainWorker pid=1191270)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5f4597d7/checkpoint_000010)\n",
            "2025-03-12 23:31:21,369\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1191270)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5f4597d7/checkpoint_000011)\n",
            "2025-03-12 23:31:22,702\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:31:23,199 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1746 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1191270)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5f4597d7/checkpoint_000012)\n",
            "2025-03-12 23:31:24,028\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:31:25,378\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1191270)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5f4597d7/checkpoint_000013)\n",
            "2025-03-12 23:31:26,731\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1191270)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5f4597d7/checkpoint_000014)\n",
            "2025-03-12 23:31:28,030\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1191270)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5f4597d7/checkpoint_000015)\n",
            "2025-03-12 23:31:29,366\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1191270)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5f4597d7/checkpoint_000016)\n",
            "2025-03-12 23:31:30,723\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1191270)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5f4597d7/checkpoint_000017)\n",
            "2025-03-12 23:31:32,047\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1191270)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5f4597d7/checkpoint_000018)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:31:33,207 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.0613 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:31:33,365\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1191270)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5f4597d7/checkpoint_000019)\n",
            "2025-03-12 23:31:34,707\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1191270)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5f4597d7/checkpoint_000020)\n",
            "2025-03-12 23:31:36,039\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1191270)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5f4597d7/checkpoint_000021)\n",
            "2025-03-12 23:31:37,269\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1191270)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5f4597d7/checkpoint_000022)\n",
            "2025-03-12 23:31:38,649\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1191270)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5f4597d7/checkpoint_000023)\n",
            "2025-03-12 23:31:40,012\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1191270)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5f4597d7/checkpoint_000024)\n",
            "2025-03-12 23:31:41,327\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1191270)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5f4597d7/checkpoint_000025)\n",
            "2025-03-12 23:31:42,678\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1191270)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5f4597d7/checkpoint_000026)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:31:43,217 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1745 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:31:43,979\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1191270)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5f4597d7/checkpoint_000027)\n",
            "\u001b[36m(RayTrainWorker pid=1191270)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5f4597d7/checkpoint_000028)\n",
            "2025-03-12 23:31:45,350\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1191270)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5f4597d7/checkpoint_000029)\n",
            "2025-03-12 23:31:46,662\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:31:47,985\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1191270)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5f4597d7/checkpoint_000030)\n",
            "2025-03-12 23:31:49,309\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1191270)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5f4597d7/checkpoint_000031)\n",
            "\u001b[36m(RayTrainWorker pid=1191270)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5f4597d7/checkpoint_000032)\n",
            "2025-03-12 23:31:50,644\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1191270)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5f4597d7/checkpoint_000033)\n",
            "2025-03-12 23:31:51,992\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:31:53,271\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:31:53,224 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.0611 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1191270)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5f4597d7/checkpoint_000034)\n",
            "2025-03-12 23:31:54,625\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1191270)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5f4597d7/checkpoint_000035)\n",
            "2025-03-12 23:31:55,906\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1191270)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5f4597d7/checkpoint_000036)\n",
            "2025-03-12 23:31:57,211\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1191270)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5f4597d7/checkpoint_000037)\n",
            "\u001b[36m(RayTrainWorker pid=1191270)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5f4597d7/checkpoint_000038)\n",
            "2025-03-12 23:31:58,517\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1191270)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5f4597d7/checkpoint_000039)\n",
            "2025-03-12 23:31:59,836\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:32:01,136\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1191270)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5f4597d7/checkpoint_000040)\n",
            "2025-03-12 23:32:02,439\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1191270)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5f4597d7/checkpoint_000041)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:32:03,233 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1742 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:32:03,783\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1191270)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5f4597d7/checkpoint_000042)\n",
            "\u001b[36m(RayTrainWorker pid=1191270)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5f4597d7/checkpoint_000043)2025-03-12 23:32:05,139\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\n",
            "2025-03-12 23:32:06,442\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1191270)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5f4597d7/checkpoint_000044)\n",
            "2025-03-12 23:32:07,776\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1191270)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5f4597d7/checkpoint_000045)\n",
            "2025-03-12 23:32:09,116\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1191270)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5f4597d7/checkpoint_000046)\n",
            "2025-03-12 23:32:10,420\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1191270)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5f4597d7/checkpoint_000047)\n",
            "2025-03-12 23:32:11,709\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1191270)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5f4597d7/checkpoint_000048)\n",
            "\u001b[36m(RayTrainWorker pid=1191270)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5f4597d7/checkpoint_000049)\n",
            "\u001b[36m(RayTrainWorker pid=1191270)\u001b[0m `Trainer.fit` stopped: `max_epochs=50` reached.\n",
            "2025-03-12 23:32:12,999\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:32:13,242 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1742 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1191270)\u001b[0m [rank0]:[W312 23:32:14.598544043 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:32:23,251 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1741 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(TorchTrainer pid=1192914)\u001b[0m Started distributed worker processes: \n",
            "\u001b[36m(TorchTrainer pid=1192914)\u001b[0m - (node_id=6cf2562c5de19587d3dc355f8ebead9f15bbc1bb9651b7563d063b82, ip=10.128.0.3, pid=1193133) world_rank=0, local_rank=0, node_rank=0\n",
            "\u001b[36m(RayTrainWorker pid=1193133)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
            "\u001b[36m(RayTrainWorker pid=1193133)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/ray/train/lightning/_lightning_utils.py:262: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
            "\u001b[36m(RayTrainWorker pid=1193133)\u001b[0m `get_trial_name` is deprecated because the concept of a `Trial` will soon be removed in Ray Train.Ray Train will no longer assume that it's running within a Ray Tune `Trial` in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
            "\u001b[36m(RayTrainWorker pid=1193133)\u001b[0m GPU available: True (cuda), used: True\n",
            "\u001b[36m(RayTrainWorker pid=1193133)\u001b[0m TPU available: False, using: 0 TPU cores\n",
            "\u001b[36m(RayTrainWorker pid=1193133)\u001b[0m HPU available: False, using: 0 HPUs\n",
            "\u001b[36m(RayTrainWorker pid=1193133)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(RayTrainWorker pid=1193133)\u001b[0m Getting checkpoint from /home/jupyter/AgenticADMET/notebooks/../output/artifacts/mol_mlm_roberta_zinc/last.ckpt...\n",
            "\u001b[36m(RayTrainWorker pid=1193133)\u001b[0m Loading checkpoint from roberta to roberta...\n",
            "\u001b[36m(RayTrainWorker pid=1193133)\u001b[0m <All keys matched successfully>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(RayTrainWorker pid=1193133)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\u001b[36m(RayTrainWorker pid=1193133)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=1193133)\u001b[0m   | Name      | Type         | Params | Mode \n",
            "\u001b[36m(RayTrainWorker pid=1193133)\u001b[0m ---------------------------------------------------\n",
            "\u001b[36m(RayTrainWorker pid=1193133)\u001b[0m 0 | roberta   | RobertaModel | 8.7 M  | train\n",
            "\u001b[36m(RayTrainWorker pid=1193133)\u001b[0m 1 | predictor | MLP          | 199 K  | train\n",
            "\u001b[36m(RayTrainWorker pid=1193133)\u001b[0m 2 | criterion | MSE          | 0      | train\n",
            "\u001b[36m(RayTrainWorker pid=1193133)\u001b[0m 3 | metrics   | ModuleList   | 0      | train\n",
            "\u001b[36m(RayTrainWorker pid=1193133)\u001b[0m ---------------------------------------------------\n",
            "\u001b[36m(RayTrainWorker pid=1193133)\u001b[0m 8.9 M     Trainable params\n",
            "\u001b[36m(RayTrainWorker pid=1193133)\u001b[0m 0         Non-trainable params\n",
            "\u001b[36m(RayTrainWorker pid=1193133)\u001b[0m 8.9 M     Total params\n",
            "\u001b[36m(RayTrainWorker pid=1193133)\u001b[0m 35.497    Total estimated model params size (MB)\n",
            "\u001b[36m(RayTrainWorker pid=1193133)\u001b[0m 128       Modules in train mode\n",
            "\u001b[36m(RayTrainWorker pid=1193133)\u001b[0m 0         Modules in eval mode\n",
            "\u001b[36m(RayTrainWorker pid=1193133)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
            "\u001b[36m(RayTrainWorker pid=1193133)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1193133)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
            "\u001b[36m(RayTrainWorker pid=1193133)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "\u001b[36m(RayTrainWorker pid=1193133)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('lr', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1193133)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('train_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1193133)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/481a02c2/checkpoint_000000)\n",
            "2025-03-12 23:32:33,292\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:32:33,259 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.0747 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1193133)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/481a02c2/checkpoint_000001)\n",
            "2025-03-12 23:32:34,379\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1193133)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/481a02c2/checkpoint_000002)\n",
            "2025-03-12 23:32:35,510\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1193133)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/481a02c2/checkpoint_000003)\n",
            "\u001b[36m(RayTrainWorker pid=1193133)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/481a02c2/checkpoint_000004)\n",
            "2025-03-12 23:32:36,592\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:32:37,676\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1193133)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/481a02c2/checkpoint_000005)\n",
            "2025-03-12 23:32:38,794\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1193133)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/481a02c2/checkpoint_000006)\n",
            "2025-03-12 23:32:39,828\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1193133)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/481a02c2/checkpoint_000007)\n",
            "\u001b[36m(RayTrainWorker pid=1193133)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/481a02c2/checkpoint_000008)\n",
            "2025-03-12 23:32:40,967\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:32:42,006\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1193133)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/481a02c2/checkpoint_000009)\n",
            "\u001b[36m(RayTrainWorker pid=1193133)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/481a02c2/checkpoint_000010)\n",
            "2025-03-12 23:32:43,132\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:32:43,268 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.174 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1193133)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/481a02c2/checkpoint_000011)\n",
            "2025-03-12 23:32:44,258\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1193133)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/481a02c2/checkpoint_000012)\n",
            "2025-03-12 23:32:45,367\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:32:46,498\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1193133)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/481a02c2/checkpoint_000013)\n",
            "\u001b[36m(RayTrainWorker pid=1193133)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/481a02c2/checkpoint_000014)\n",
            "2025-03-12 23:32:47,625\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:32:48,708\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1193133)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/481a02c2/checkpoint_000015)\n",
            "2025-03-12 23:32:49,814\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1193133)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/481a02c2/checkpoint_000016)\n",
            "2025-03-12 23:32:50,901\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1193133)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/481a02c2/checkpoint_000017)\n",
            "2025-03-12 23:32:52,020\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1193133)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/481a02c2/checkpoint_000018)\n",
            "\u001b[36m(RayTrainWorker pid=1193133)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/481a02c2/checkpoint_000019)\n",
            "2025-03-12 23:32:53,118\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:32:53,276 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1739 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:32:54,205\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1193133)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/481a02c2/checkpoint_000020)\n",
            "2025-03-12 23:32:55,299\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1193133)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/481a02c2/checkpoint_000021)\n",
            "2025-03-12 23:32:56,415\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1193133)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/481a02c2/checkpoint_000022)\n",
            "\u001b[36m(RayTrainWorker pid=1193133)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/481a02c2/checkpoint_000023)\n",
            "2025-03-12 23:32:57,524\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:32:58,606\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1193133)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/481a02c2/checkpoint_000024)\n",
            "2025-03-12 23:32:59,605\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1193133)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/481a02c2/checkpoint_000025)\n",
            "2025-03-12 23:33:00,724\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1193133)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/481a02c2/checkpoint_000026)\n",
            "2025-03-12 23:33:01,838\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1193133)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/481a02c2/checkpoint_000027)\n",
            "2025-03-12 23:33:02,939\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1193133)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/481a02c2/checkpoint_000028)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:33:03,284 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1737 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1193133)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/481a02c2/checkpoint_000029)\n",
            "2025-03-12 23:33:04,035\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:33:05,107\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1193133)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/481a02c2/checkpoint_000030)\n",
            "2025-03-12 23:33:06,215\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1193133)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/481a02c2/checkpoint_000031)\n",
            "2025-03-12 23:33:07,333\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1193133)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/481a02c2/checkpoint_000032)\n",
            "2025-03-12 23:33:08,465\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1193133)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/481a02c2/checkpoint_000033)\n",
            "\u001b[36m(RayTrainWorker pid=1193133)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/481a02c2/checkpoint_000034)\n",
            "2025-03-12 23:33:09,572\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1193133)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/481a02c2/checkpoint_000035)\n",
            "2025-03-12 23:33:10,694\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1193133)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/481a02c2/checkpoint_000036)\n",
            "2025-03-12 23:33:11,815\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1193133)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/481a02c2/checkpoint_000037)\n",
            "2025-03-12 23:33:12,942\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:33:13,292 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1737 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1193133)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/481a02c2/checkpoint_000038)\n",
            "2025-03-12 23:33:14,059\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1193133)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/481a02c2/checkpoint_000039)\n",
            "2025-03-12 23:33:15,184\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:33:16,238\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1193133)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/481a02c2/checkpoint_000040)\n",
            "2025-03-12 23:33:17,364\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1193133)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/481a02c2/checkpoint_000041)\n",
            "\u001b[36m(RayTrainWorker pid=1193133)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/481a02c2/checkpoint_000042)\n",
            "2025-03-12 23:33:18,453\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1193133)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/481a02c2/checkpoint_000043)\n",
            "2025-03-12 23:33:19,580\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1193133)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/481a02c2/checkpoint_000044)\n",
            "2025-03-12 23:33:20,699\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:33:21,878\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1193133)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/481a02c2/checkpoint_000045)\n",
            "2025-03-12 23:33:22,979\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1193133)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/481a02c2/checkpoint_000046)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:33:23,301 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1736 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1193133)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/481a02c2/checkpoint_000047)\n",
            "2025-03-12 23:33:24,082\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:33:25,240\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1193133)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/481a02c2/checkpoint_000048)\n",
            "\u001b[36m(RayTrainWorker pid=1193133)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/481a02c2/checkpoint_000049)\n",
            "2025-03-12 23:33:26,332\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1193133)\u001b[0m `Trainer.fit` stopped: `max_epochs=50` reached.\n",
            "\u001b[36m(RayTrainWorker pid=1193133)\u001b[0m [rank0]:[W312 23:33:28.886879379 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:33:33,310 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1735 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1194751)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
            "\u001b[36m(TorchTrainer pid=1194562)\u001b[0m Started distributed worker processes: \n",
            "\u001b[36m(TorchTrainer pid=1194562)\u001b[0m - (node_id=6cf2562c5de19587d3dc355f8ebead9f15bbc1bb9651b7563d063b82, ip=10.128.0.3, pid=1194751) world_rank=0, local_rank=0, node_rank=0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(RayTrainWorker pid=1194751)\u001b[0m Getting checkpoint from /home/jupyter/AgenticADMET/notebooks/../output/artifacts/mol_mlm_roberta_zinc/last.ckpt...\n",
            "\u001b[36m(RayTrainWorker pid=1194751)\u001b[0m Loading checkpoint from roberta to roberta...\n",
            "\u001b[36m(RayTrainWorker pid=1194751)\u001b[0m <All keys matched successfully>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(RayTrainWorker pid=1194751)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/ray/train/lightning/_lightning_utils.py:262: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
            "\u001b[36m(RayTrainWorker pid=1194751)\u001b[0m `get_trial_name` is deprecated because the concept of a `Trial` will soon be removed in Ray Train.Ray Train will no longer assume that it's running within a Ray Tune `Trial` in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
            "\u001b[36m(RayTrainWorker pid=1194751)\u001b[0m GPU available: True (cuda), used: True\n",
            "\u001b[36m(RayTrainWorker pid=1194751)\u001b[0m TPU available: False, using: 0 TPU cores\n",
            "\u001b[36m(RayTrainWorker pid=1194751)\u001b[0m HPU available: False, using: 0 HPUs\n",
            "\u001b[36m(RayTrainWorker pid=1194751)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:33:43,318 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1734 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1194751)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\u001b[36m(RayTrainWorker pid=1194751)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=1194751)\u001b[0m   | Name      | Type         | Params | Mode \n",
            "\u001b[36m(RayTrainWorker pid=1194751)\u001b[0m ---------------------------------------------------\n",
            "\u001b[36m(RayTrainWorker pid=1194751)\u001b[0m 0 | roberta   | RobertaModel | 8.7 M  | train\n",
            "\u001b[36m(RayTrainWorker pid=1194751)\u001b[0m 1 | predictor | MLP          | 199 K  | train\n",
            "\u001b[36m(RayTrainWorker pid=1194751)\u001b[0m 2 | criterion | MSE          | 0      | train\n",
            "\u001b[36m(RayTrainWorker pid=1194751)\u001b[0m 3 | metrics   | ModuleList   | 0      | train\n",
            "\u001b[36m(RayTrainWorker pid=1194751)\u001b[0m ---------------------------------------------------\n",
            "\u001b[36m(RayTrainWorker pid=1194751)\u001b[0m 8.9 M     Trainable params\n",
            "\u001b[36m(RayTrainWorker pid=1194751)\u001b[0m 0         Non-trainable params\n",
            "\u001b[36m(RayTrainWorker pid=1194751)\u001b[0m 8.9 M     Total params\n",
            "\u001b[36m(RayTrainWorker pid=1194751)\u001b[0m 35.497    Total estimated model params size (MB)\n",
            "\u001b[36m(RayTrainWorker pid=1194751)\u001b[0m 128       Modules in train mode\n",
            "\u001b[36m(RayTrainWorker pid=1194751)\u001b[0m 0         Modules in eval mode\n",
            "\u001b[36m(RayTrainWorker pid=1194751)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
            "\u001b[36m(RayTrainWorker pid=1194751)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1194751)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
            "\u001b[36m(RayTrainWorker pid=1194751)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "\u001b[36m(RayTrainWorker pid=1194751)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('lr', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1194751)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('train_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1194751)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d61f6592/checkpoint_000000)\n",
            "2025-03-12 23:33:46,479\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1194751)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d61f6592/checkpoint_000001)\n",
            "2025-03-12 23:33:47,643\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1194751)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d61f6592/checkpoint_000002)\n",
            "\u001b[36m(RayTrainWorker pid=1194751)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d61f6592/checkpoint_000003)\n",
            "2025-03-12 23:33:48,804\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1194751)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d61f6592/checkpoint_000004)\n",
            "2025-03-12 23:33:49,916\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1194751)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d61f6592/checkpoint_000005)\n",
            "2025-03-12 23:33:51,039\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:33:52,123\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1194751)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d61f6592/checkpoint_000006)\n",
            "\u001b[36m(RayTrainWorker pid=1194751)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d61f6592/checkpoint_000007)\n",
            "2025-03-12 23:33:53,176\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:33:53,327 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1733 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:33:54,333\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1194751)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d61f6592/checkpoint_000008)\n",
            "2025-03-12 23:33:55,459\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1194751)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d61f6592/checkpoint_000009)\n",
            "\u001b[36m(RayTrainWorker pid=1194751)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d61f6592/checkpoint_000010)\n",
            "2025-03-12 23:33:56,651\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:33:57,755\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1194751)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d61f6592/checkpoint_000011)\n",
            "2025-03-12 23:33:58,942\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1194751)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d61f6592/checkpoint_000012)\n",
            "2025-03-12 23:34:00,103\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1194751)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d61f6592/checkpoint_000013)\n",
            "2025-03-12 23:34:01,231\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1194751)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d61f6592/checkpoint_000014)\n",
            "2025-03-12 23:34:02,438\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1194751)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d61f6592/checkpoint_000015)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:34:03,336 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1638 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1194751)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d61f6592/checkpoint_000016)\n",
            "2025-03-12 23:34:03,602\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:34:04,781\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1194751)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d61f6592/checkpoint_000017)\n",
            "2025-03-12 23:34:05,932\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1194751)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d61f6592/checkpoint_000018)\n",
            "\u001b[36m(RayTrainWorker pid=1194751)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d61f6592/checkpoint_000019)\n",
            "2025-03-12 23:34:07,082\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:34:08,238\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1194751)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d61f6592/checkpoint_000020)\n",
            "2025-03-12 23:34:09,378\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1194751)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d61f6592/checkpoint_000021)\n",
            "\u001b[36m(RayTrainWorker pid=1194751)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d61f6592/checkpoint_000022)\n",
            "2025-03-12 23:34:10,553\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:34:11,628\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1194751)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d61f6592/checkpoint_000023)\n",
            "2025-03-12 23:34:12,757\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1194751)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d61f6592/checkpoint_000024)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:34:13,344 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1729 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:34:13,891\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1194751)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d61f6592/checkpoint_000025)\n",
            "2025-03-12 23:34:15,065\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1194751)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d61f6592/checkpoint_000026)\n",
            "\u001b[36m(RayTrainWorker pid=1194751)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d61f6592/checkpoint_000027)\n",
            "2025-03-12 23:34:16,262\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:34:17,443\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1194751)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d61f6592/checkpoint_000028)\n",
            "\u001b[36m(RayTrainWorker pid=1194751)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d61f6592/checkpoint_000029)\n",
            "2025-03-12 23:34:18,620\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:34:19,704\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1194751)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d61f6592/checkpoint_000030)\n",
            "2025-03-12 23:34:20,893\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1194751)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d61f6592/checkpoint_000031)\n",
            "\u001b[36m(RayTrainWorker pid=1194751)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d61f6592/checkpoint_000032)\n",
            "2025-03-12 23:34:22,009\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:34:23,080\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1194751)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d61f6592/checkpoint_000033)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:34:23,353 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.173 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:34:24,082\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1194751)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d61f6592/checkpoint_000034)\n",
            "2025-03-12 23:34:25,146\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1194751)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d61f6592/checkpoint_000035)\n",
            "2025-03-12 23:34:26,218\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1194751)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d61f6592/checkpoint_000036)\n",
            "\u001b[36m(RayTrainWorker pid=1194751)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d61f6592/checkpoint_000037)\n",
            "2025-03-12 23:34:27,428\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:34:28,509\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1194751)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d61f6592/checkpoint_000038)\n",
            "\u001b[36m(RayTrainWorker pid=1194751)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d61f6592/checkpoint_000039)\n",
            "2025-03-12 23:34:29,664\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1194751)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d61f6592/checkpoint_000040)\n",
            "2025-03-12 23:34:30,800\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1194751)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d61f6592/checkpoint_000041)\n",
            "2025-03-12 23:34:31,927\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:34:33,015\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1194751)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d61f6592/checkpoint_000042)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:34:33,361 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1729 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:34:34,133\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1194751)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d61f6592/checkpoint_000043)\n",
            "\u001b[36m(RayTrainWorker pid=1194751)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d61f6592/checkpoint_000044)\n",
            "2025-03-12 23:34:35,283\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1194751)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d61f6592/checkpoint_000045)\n",
            "2025-03-12 23:34:36,407\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:34:37,569\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1194751)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d61f6592/checkpoint_000046)\n",
            "2025-03-12 23:34:38,724\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1194751)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d61f6592/checkpoint_000047)\n",
            "\u001b[36m(RayTrainWorker pid=1194751)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d61f6592/checkpoint_000048)\n",
            "2025-03-12 23:34:39,885\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:34:41,037\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1194751)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d61f6592/checkpoint_000049)\n",
            "\u001b[36m(RayTrainWorker pid=1194751)\u001b[0m `Trainer.fit` stopped: `max_epochs=50` reached.\n",
            "\u001b[36m(RayTrainWorker pid=1194751)\u001b[0m [rank0]:[W312 23:34:43.639177491 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:34:43,369 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1728 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:34:53,379 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1727 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(TorchTrainer pid=1196284)\u001b[0m Started distributed worker processes: \n",
            "\u001b[36m(TorchTrainer pid=1196284)\u001b[0m - (node_id=6cf2562c5de19587d3dc355f8ebead9f15bbc1bb9651b7563d063b82, ip=10.128.0.3, pid=1196443) world_rank=0, local_rank=0, node_rank=0\n",
            "\u001b[36m(RayTrainWorker pid=1196443)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(RayTrainWorker pid=1196443)\u001b[0m Getting checkpoint from /home/jupyter/AgenticADMET/notebooks/../output/artifacts/mol_mlm_roberta_zinc/last.ckpt...\n",
            "\u001b[36m(RayTrainWorker pid=1196443)\u001b[0m Loading checkpoint from roberta to roberta...\n",
            "\u001b[36m(RayTrainWorker pid=1196443)\u001b[0m <All keys matched successfully>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(RayTrainWorker pid=1196443)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/ray/train/lightning/_lightning_utils.py:262: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
            "\u001b[36m(RayTrainWorker pid=1196443)\u001b[0m `get_trial_name` is deprecated because the concept of a `Trial` will soon be removed in Ray Train.Ray Train will no longer assume that it's running within a Ray Tune `Trial` in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
            "\u001b[36m(RayTrainWorker pid=1196443)\u001b[0m GPU available: True (cuda), used: True\n",
            "\u001b[36m(RayTrainWorker pid=1196443)\u001b[0m TPU available: False, using: 0 TPU cores\n",
            "\u001b[36m(RayTrainWorker pid=1196443)\u001b[0m HPU available: False, using: 0 HPUs\n",
            "\u001b[36m(RayTrainWorker pid=1196443)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
            "\u001b[36m(RayTrainWorker pid=1196443)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\u001b[36m(RayTrainWorker pid=1196443)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=1196443)\u001b[0m   | Name      | Type         | Params | Mode \n",
            "\u001b[36m(RayTrainWorker pid=1196443)\u001b[0m ---------------------------------------------------\n",
            "\u001b[36m(RayTrainWorker pid=1196443)\u001b[0m 0 | roberta   | RobertaModel | 8.7 M  | train\n",
            "\u001b[36m(RayTrainWorker pid=1196443)\u001b[0m 1 | predictor | MLP          | 249 K  | train\n",
            "\u001b[36m(RayTrainWorker pid=1196443)\u001b[0m 2 | criterion | MSE          | 0      | train\n",
            "\u001b[36m(RayTrainWorker pid=1196443)\u001b[0m 3 | metrics   | ModuleList   | 0      | train\n",
            "\u001b[36m(RayTrainWorker pid=1196443)\u001b[0m ---------------------------------------------------\n",
            "\u001b[36m(RayTrainWorker pid=1196443)\u001b[0m 8.9 M     Trainable params\n",
            "\u001b[36m(RayTrainWorker pid=1196443)\u001b[0m 0         Non-trainable params\n",
            "\u001b[36m(RayTrainWorker pid=1196443)\u001b[0m 8.9 M     Total params\n",
            "\u001b[36m(RayTrainWorker pid=1196443)\u001b[0m 35.697    Total estimated model params size (MB)\n",
            "\u001b[36m(RayTrainWorker pid=1196443)\u001b[0m 128       Modules in train mode\n",
            "\u001b[36m(RayTrainWorker pid=1196443)\u001b[0m 0         Modules in eval mode\n",
            "\u001b[36m(RayTrainWorker pid=1196443)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
            "\u001b[36m(RayTrainWorker pid=1196443)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1196443)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
            "\u001b[36m(RayTrainWorker pid=1196443)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "\u001b[36m(RayTrainWorker pid=1196443)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('lr', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1196443)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('train_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1196443)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d0f8f668/checkpoint_000000)\n",
            "2025-03-12 23:35:01,517\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1196443)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d0f8f668/checkpoint_000001)\n",
            "2025-03-12 23:35:02,664\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1196443)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d0f8f668/checkpoint_000002)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:35:03,387 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1725 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:35:03,871\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1196443)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d0f8f668/checkpoint_000003)\n",
            "2025-03-12 23:35:05,044\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1196443)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d0f8f668/checkpoint_000004)\n",
            "2025-03-12 23:35:06,223\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1196443)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d0f8f668/checkpoint_000005)\n",
            "2025-03-12 23:35:07,336\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1196443)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d0f8f668/checkpoint_000006)\n",
            "2025-03-12 23:35:08,449\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1196443)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d0f8f668/checkpoint_000007)\n",
            "\u001b[36m(RayTrainWorker pid=1196443)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d0f8f668/checkpoint_000008)\n",
            "2025-03-12 23:35:09,634\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:35:10,821\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1196443)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d0f8f668/checkpoint_000009)\n",
            "2025-03-12 23:35:12,036\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1196443)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d0f8f668/checkpoint_000010)\n",
            "2025-03-12 23:35:13,191\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1196443)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d0f8f668/checkpoint_000011)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:35:13,396 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1724 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:35:14,321\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1196443)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d0f8f668/checkpoint_000012)\n",
            "2025-03-12 23:35:15,483\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1196443)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d0f8f668/checkpoint_000013)\n",
            "2025-03-12 23:35:16,600\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1196443)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d0f8f668/checkpoint_000014)\n",
            "2025-03-12 23:35:17,785\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1196443)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d0f8f668/checkpoint_000015)\n",
            "\u001b[36m(RayTrainWorker pid=1196443)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d0f8f668/checkpoint_000016)\n",
            "2025-03-12 23:35:18,950\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:35:20,107\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1196443)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d0f8f668/checkpoint_000017)\n",
            "2025-03-12 23:35:21,278\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1196443)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d0f8f668/checkpoint_000018)\n",
            "\u001b[36m(RayTrainWorker pid=1196443)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d0f8f668/checkpoint_000019)\n",
            "2025-03-12 23:35:22,432\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:35:23,405 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.0994 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:35:23,607\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1196443)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d0f8f668/checkpoint_000020)\n",
            "\u001b[36m(RayTrainWorker pid=1196443)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d0f8f668/checkpoint_000021)\n",
            "2025-03-12 23:35:24,770\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:35:25,923\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1196443)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d0f8f668/checkpoint_000022)\n",
            "2025-03-12 23:35:27,005\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1196443)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d0f8f668/checkpoint_000023)\n",
            "2025-03-12 23:35:28,203\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1196443)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d0f8f668/checkpoint_000024)\n",
            "2025-03-12 23:35:29,348\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1196443)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d0f8f668/checkpoint_000025)\n",
            "\u001b[36m(RayTrainWorker pid=1196443)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d0f8f668/checkpoint_000026)\n",
            "2025-03-12 23:35:30,510\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:35:31,661\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1196443)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d0f8f668/checkpoint_000027)\n",
            "2025-03-12 23:35:32,834\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1196443)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d0f8f668/checkpoint_000028)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:35:33,413 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1723 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:35:34,014\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1196443)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d0f8f668/checkpoint_000029)\n",
            "\u001b[36m(RayTrainWorker pid=1196443)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d0f8f668/checkpoint_000030)\n",
            "2025-03-12 23:35:35,117\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:35:36,296\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1196443)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d0f8f668/checkpoint_000031)\n",
            "\u001b[36m(RayTrainWorker pid=1196443)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d0f8f668/checkpoint_000032)\n",
            "2025-03-12 23:35:37,457\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:35:38,624\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1196443)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d0f8f668/checkpoint_000033)\n",
            "2025-03-12 23:35:39,794\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1196443)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d0f8f668/checkpoint_000034)\n",
            "2025-03-12 23:35:40,992\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1196443)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d0f8f668/checkpoint_000035)\n",
            "2025-03-12 23:35:42,102\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1196443)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d0f8f668/checkpoint_000036)\n",
            "\u001b[36m(RayTrainWorker pid=1196443)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d0f8f668/checkpoint_000037)\n",
            "2025-03-12 23:35:43,317\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:35:43,422 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1723 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1196443)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d0f8f668/checkpoint_000038)2025-03-12 23:35:44,413\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\n",
            "2025-03-12 23:35:45,597\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1196443)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d0f8f668/checkpoint_000039)\n",
            "\u001b[36m(RayTrainWorker pid=1196443)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d0f8f668/checkpoint_000040)\n",
            "2025-03-12 23:35:46,772\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:35:47,974\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1196443)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d0f8f668/checkpoint_000041)\n",
            "\u001b[36m(RayTrainWorker pid=1196443)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d0f8f668/checkpoint_000042)\n",
            "2025-03-12 23:35:49,021\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1196443)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d0f8f668/checkpoint_000043)\n",
            "2025-03-12 23:35:50,162\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:35:51,312\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1196443)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d0f8f668/checkpoint_000044)\n",
            "2025-03-12 23:35:52,470\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1196443)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d0f8f668/checkpoint_000045)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:35:53,431 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1009 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:35:53,653\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1196443)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d0f8f668/checkpoint_000046)\n",
            "2025-03-12 23:35:54,793\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1196443)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d0f8f668/checkpoint_000047)\n",
            "2025-03-12 23:35:55,935\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1196443)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d0f8f668/checkpoint_000048)\n",
            "2025-03-12 23:35:57,089\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1196443)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d0f8f668/checkpoint_000049)\n",
            "\u001b[36m(RayTrainWorker pid=1196443)\u001b[0m `Trainer.fit` stopped: `max_epochs=50` reached.\n",
            "\u001b[36m(RayTrainWorker pid=1196443)\u001b[0m [rank0]:[W312 23:35:59.644763896 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:36:03,439 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.172 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(TorchTrainer pid=1197932)\u001b[0m Started distributed worker processes: \n",
            "\u001b[36m(TorchTrainer pid=1197932)\u001b[0m - (node_id=6cf2562c5de19587d3dc355f8ebead9f15bbc1bb9651b7563d063b82, ip=10.128.0.3, pid=1198249) world_rank=0, local_rank=0, node_rank=0\n",
            "\u001b[36m(RayTrainWorker pid=1198249)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:36:13,448 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1719 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1198249)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/ray/train/lightning/_lightning_utils.py:262: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
            "\u001b[36m(RayTrainWorker pid=1198249)\u001b[0m `get_trial_name` is deprecated because the concept of a `Trial` will soon be removed in Ray Train.Ray Train will no longer assume that it's running within a Ray Tune `Trial` in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
            "\u001b[36m(RayTrainWorker pid=1198249)\u001b[0m GPU available: True (cuda), used: True\n",
            "\u001b[36m(RayTrainWorker pid=1198249)\u001b[0m TPU available: False, using: 0 TPU cores\n",
            "\u001b[36m(RayTrainWorker pid=1198249)\u001b[0m HPU available: False, using: 0 HPUs\n",
            "\u001b[36m(RayTrainWorker pid=1198249)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(RayTrainWorker pid=1198249)\u001b[0m Getting checkpoint from /home/jupyter/AgenticADMET/notebooks/../output/artifacts/mol_mlm_roberta_zinc/last.ckpt...\n",
            "\u001b[36m(RayTrainWorker pid=1198249)\u001b[0m Loading checkpoint from roberta to roberta...\n",
            "\u001b[36m(RayTrainWorker pid=1198249)\u001b[0m <All keys matched successfully>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(RayTrainWorker pid=1198249)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\u001b[36m(RayTrainWorker pid=1198249)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=1198249)\u001b[0m   | Name      | Type         | Params | Mode \n",
            "\u001b[36m(RayTrainWorker pid=1198249)\u001b[0m ---------------------------------------------------\n",
            "\u001b[36m(RayTrainWorker pid=1198249)\u001b[0m 0 | roberta   | RobertaModel | 8.7 M  | train\n",
            "\u001b[36m(RayTrainWorker pid=1198249)\u001b[0m 1 | predictor | MLP          | 297 K  | train\n",
            "\u001b[36m(RayTrainWorker pid=1198249)\u001b[0m 2 | criterion | MSE          | 0      | train\n",
            "\u001b[36m(RayTrainWorker pid=1198249)\u001b[0m 3 | metrics   | ModuleList   | 0      | train\n",
            "\u001b[36m(RayTrainWorker pid=1198249)\u001b[0m ---------------------------------------------------\n",
            "\u001b[36m(RayTrainWorker pid=1198249)\u001b[0m 9.0 M     Trainable params\n",
            "\u001b[36m(RayTrainWorker pid=1198249)\u001b[0m 0         Non-trainable params\n",
            "\u001b[36m(RayTrainWorker pid=1198249)\u001b[0m 9.0 M     Total params\n",
            "\u001b[36m(RayTrainWorker pid=1198249)\u001b[0m 35.889    Total estimated model params size (MB)\n",
            "\u001b[36m(RayTrainWorker pid=1198249)\u001b[0m 132       Modules in train mode\n",
            "\u001b[36m(RayTrainWorker pid=1198249)\u001b[0m 0         Modules in eval mode\n",
            "\u001b[36m(RayTrainWorker pid=1198249)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
            "\u001b[36m(RayTrainWorker pid=1198249)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1198249)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
            "\u001b[36m(RayTrainWorker pid=1198249)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "\u001b[36m(RayTrainWorker pid=1198249)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('lr', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1198249)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('train_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1198249)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/8eaba676/checkpoint_000000)\n",
            "2025-03-12 23:36:17,809\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1198249)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/8eaba676/checkpoint_000001)\n",
            "2025-03-12 23:36:18,932\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1198249)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/8eaba676/checkpoint_000002)\n",
            "2025-03-12 23:36:20,098\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1198249)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/8eaba676/checkpoint_000003)\n",
            "\u001b[36m(RayTrainWorker pid=1198249)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/8eaba676/checkpoint_000004)\n",
            "2025-03-12 23:36:21,276\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1198249)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/8eaba676/checkpoint_000005)\n",
            "2025-03-12 23:36:22,500\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:36:23,456 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.0714 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:36:23,608\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1198249)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/8eaba676/checkpoint_000006)\n",
            "\u001b[36m(RayTrainWorker pid=1198249)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/8eaba676/checkpoint_000007)\n",
            "2025-03-12 23:36:24,769\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:36:25,914\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1198249)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/8eaba676/checkpoint_000008)\n",
            "\u001b[36m(RayTrainWorker pid=1198249)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/8eaba676/checkpoint_000009)\n",
            "2025-03-12 23:36:27,117\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:36:28,332\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1198249)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/8eaba676/checkpoint_000010)\n",
            "2025-03-12 23:36:29,514\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1198249)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/8eaba676/checkpoint_000011)\n",
            "2025-03-12 23:36:30,725\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1198249)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/8eaba676/checkpoint_000012)\n",
            "\u001b[36m(RayTrainWorker pid=1198249)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/8eaba676/checkpoint_000013)\n",
            "2025-03-12 23:36:31,946\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:36:33,088\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1198249)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/8eaba676/checkpoint_000014)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:36:33,466 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1718 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:36:34,310\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1198249)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/8eaba676/checkpoint_000015)\n",
            "\u001b[36m(RayTrainWorker pid=1198249)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/8eaba676/checkpoint_000016)\n",
            "2025-03-12 23:36:35,513\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:36:36,710\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1198249)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/8eaba676/checkpoint_000017)\n",
            "2025-03-12 23:36:37,925\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1198249)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/8eaba676/checkpoint_000018)\n",
            "\u001b[36m(RayTrainWorker pid=1198249)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/8eaba676/checkpoint_000019)\n",
            "2025-03-12 23:36:39,114\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:36:40,293\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1198249)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/8eaba676/checkpoint_000020)\n",
            "2025-03-12 23:36:41,532\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1198249)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/8eaba676/checkpoint_000021)\n",
            "2025-03-12 23:36:42,758\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1198249)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/8eaba676/checkpoint_000022)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:36:43,475 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1717 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:36:43,889\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1198249)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/8eaba676/checkpoint_000023)\n",
            "2025-03-12 23:36:45,101\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1198249)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/8eaba676/checkpoint_000024)\n",
            "\u001b[36m(RayTrainWorker pid=1198249)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/8eaba676/checkpoint_000025)\n",
            "2025-03-12 23:36:46,253\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:36:47,445\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1198249)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/8eaba676/checkpoint_000026)\n",
            "2025-03-12 23:36:48,699\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1198249)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/8eaba676/checkpoint_000027)\n",
            "2025-03-12 23:36:49,934\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1198249)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/8eaba676/checkpoint_000028)\n",
            "2025-03-12 23:36:51,111\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1198249)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/8eaba676/checkpoint_000029)\n",
            "2025-03-12 23:36:52,278\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1198249)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/8eaba676/checkpoint_000030)\n",
            "2025-03-12 23:36:53,490\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:36:53,484 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1716 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1198249)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/8eaba676/checkpoint_000031)\n",
            "\u001b[36m(RayTrainWorker pid=1198249)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/8eaba676/checkpoint_000032)\n",
            "2025-03-12 23:36:54,664\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1198249)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/8eaba676/checkpoint_000033)\n",
            "2025-03-12 23:36:55,895\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:36:57,048\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1198249)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/8eaba676/checkpoint_000034)\n",
            "\u001b[36m(RayTrainWorker pid=1198249)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/8eaba676/checkpoint_000035)\n",
            "2025-03-12 23:36:58,247\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1198249)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/8eaba676/checkpoint_000036)\n",
            "2025-03-12 23:36:59,408\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:37:00,640\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1198249)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/8eaba676/checkpoint_000037)\n",
            "2025-03-12 23:37:01,807\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1198249)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/8eaba676/checkpoint_000038)\n",
            "2025-03-12 23:37:03,053\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1198249)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/8eaba676/checkpoint_000039)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:37:03,492 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1714 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:37:04,259\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1198249)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/8eaba676/checkpoint_000040)\n",
            "\u001b[36m(RayTrainWorker pid=1198249)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/8eaba676/checkpoint_000041)\n",
            "2025-03-12 23:37:05,425\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:37:06,602\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1198249)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/8eaba676/checkpoint_000042)\n",
            "2025-03-12 23:37:07,774\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1198249)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/8eaba676/checkpoint_000043)\n",
            "2025-03-12 23:37:08,962\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1198249)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/8eaba676/checkpoint_000044)\n",
            "2025-03-12 23:37:10,126\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1198249)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/8eaba676/checkpoint_000045)\n",
            "2025-03-12 23:37:11,354\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1198249)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/8eaba676/checkpoint_000046)\n",
            "\u001b[36m(RayTrainWorker pid=1198249)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/8eaba676/checkpoint_000047)\n",
            "2025-03-12 23:37:12,592\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:37:13,501 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1714 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:37:13,800\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1198249)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/8eaba676/checkpoint_000048)\n",
            "2025-03-12 23:37:14,981\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1198249)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/8eaba676/checkpoint_000049)\n",
            "\u001b[36m(RayTrainWorker pid=1198249)\u001b[0m `Trainer.fit` stopped: `max_epochs=50` reached.\n",
            "\u001b[36m(RayTrainWorker pid=1198249)\u001b[0m [rank0]:[W312 23:37:17.767354909 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:37:23,510 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1713 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(TorchTrainer pid=1199705)\u001b[0m Started distributed worker processes: \n",
            "\u001b[36m(TorchTrainer pid=1199705)\u001b[0m - (node_id=6cf2562c5de19587d3dc355f8ebead9f15bbc1bb9651b7563d063b82, ip=10.128.0.3, pid=1199986) world_rank=0, local_rank=0, node_rank=0\n",
            "\u001b[36m(RayTrainWorker pid=1199986)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(RayTrainWorker pid=1199986)\u001b[0m Getting checkpoint from /home/jupyter/AgenticADMET/notebooks/../output/artifacts/mol_mlm_roberta_zinc/last.ckpt...\n",
            "\u001b[36m(RayTrainWorker pid=1199986)\u001b[0m Loading checkpoint from roberta to roberta...\n",
            "\u001b[36m(RayTrainWorker pid=1199986)\u001b[0m <All keys matched successfully>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(RayTrainWorker pid=1199986)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/ray/train/lightning/_lightning_utils.py:262: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
            "\u001b[36m(RayTrainWorker pid=1199986)\u001b[0m `get_trial_name` is deprecated because the concept of a `Trial` will soon be removed in Ray Train.Ray Train will no longer assume that it's running within a Ray Tune `Trial` in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
            "\u001b[36m(RayTrainWorker pid=1199986)\u001b[0m GPU available: True (cuda), used: True\n",
            "\u001b[36m(RayTrainWorker pid=1199986)\u001b[0m TPU available: False, using: 0 TPU cores\n",
            "\u001b[36m(RayTrainWorker pid=1199986)\u001b[0m HPU available: False, using: 0 HPUs\n",
            "\u001b[36m(RayTrainWorker pid=1199986)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
            "\u001b[36m(RayTrainWorker pid=1199986)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\u001b[36m(RayTrainWorker pid=1199986)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=1199986)\u001b[0m   | Name      | Type         | Params | Mode \n",
            "\u001b[36m(RayTrainWorker pid=1199986)\u001b[0m ---------------------------------------------------\n",
            "\u001b[36m(RayTrainWorker pid=1199986)\u001b[0m 0 | roberta   | RobertaModel | 8.7 M  | train\n",
            "\u001b[36m(RayTrainWorker pid=1199986)\u001b[0m 1 | predictor | MLP          | 249 K  | train\n",
            "\u001b[36m(RayTrainWorker pid=1199986)\u001b[0m 2 | criterion | MSE          | 0      | train\n",
            "\u001b[36m(RayTrainWorker pid=1199986)\u001b[0m 3 | metrics   | ModuleList   | 0      | train\n",
            "\u001b[36m(RayTrainWorker pid=1199986)\u001b[0m ---------------------------------------------------\n",
            "\u001b[36m(RayTrainWorker pid=1199986)\u001b[0m 8.9 M     Trainable params\n",
            "\u001b[36m(RayTrainWorker pid=1199986)\u001b[0m 0         Non-trainable params\n",
            "\u001b[36m(RayTrainWorker pid=1199986)\u001b[0m 8.9 M     Total params\n",
            "\u001b[36m(RayTrainWorker pid=1199986)\u001b[0m 35.697    Total estimated model params size (MB)\n",
            "\u001b[36m(RayTrainWorker pid=1199986)\u001b[0m 128       Modules in train mode\n",
            "\u001b[36m(RayTrainWorker pid=1199986)\u001b[0m 0         Modules in eval mode\n",
            "\u001b[36m(RayTrainWorker pid=1199986)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
            "\u001b[36m(RayTrainWorker pid=1199986)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1199986)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
            "\u001b[36m(RayTrainWorker pid=1199986)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (21) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:37:33,519 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1712 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1199986)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('lr', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1199986)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('train_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1199986)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/fe376ce8/checkpoint_000000)\n",
            "2025-03-12 23:37:36,096\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1199986)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/fe376ce8/checkpoint_000001)\n",
            "2025-03-12 23:37:37,345\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1199986)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/fe376ce8/checkpoint_000002)\n",
            "2025-03-12 23:37:38,635\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1199986)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/fe376ce8/checkpoint_000003)\n",
            "\u001b[36m(RayTrainWorker pid=1199986)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/fe376ce8/checkpoint_000004)\n",
            "2025-03-12 23:37:39,928\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:37:41,198\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1199986)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/fe376ce8/checkpoint_000005)\n",
            "2025-03-12 23:37:42,444\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1199986)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/fe376ce8/checkpoint_000006)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:37:43,527 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1038 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:37:43,744\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1199986)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/fe376ce8/checkpoint_000007)\n",
            "2025-03-12 23:37:45,000\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1199986)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/fe376ce8/checkpoint_000008)\n",
            "\u001b[36m(RayTrainWorker pid=1199986)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/fe376ce8/checkpoint_000009)\n",
            "2025-03-12 23:37:46,272\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:37:47,511\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1199986)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/fe376ce8/checkpoint_000010)\n",
            "2025-03-12 23:37:48,770\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1199986)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/fe376ce8/checkpoint_000011)\n",
            "2025-03-12 23:37:50,012\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1199986)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/fe376ce8/checkpoint_000012)\n",
            "\u001b[36m(RayTrainWorker pid=1199986)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/fe376ce8/checkpoint_000013)\n",
            "2025-03-12 23:37:51,290\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1199986)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/fe376ce8/checkpoint_000014)\n",
            "2025-03-12 23:37:52,546\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:37:53,536 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1196 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1199986)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/fe376ce8/checkpoint_000015)\n",
            "2025-03-12 23:37:53,763\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:37:55,022\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1199986)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/fe376ce8/checkpoint_000016)\n",
            "\u001b[36m(RayTrainWorker pid=1199986)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/fe376ce8/checkpoint_000017)\n",
            "2025-03-12 23:37:56,340\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:37:57,588\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1199986)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/fe376ce8/checkpoint_000018)\n",
            "2025-03-12 23:37:58,949\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1199986)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/fe376ce8/checkpoint_000019)\n",
            "2025-03-12 23:38:00,163\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1199986)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/fe376ce8/checkpoint_000020)\n",
            "\u001b[36m(RayTrainWorker pid=1199986)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/fe376ce8/checkpoint_000021)\n",
            "2025-03-12 23:38:01,450\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:38:02,728\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1199986)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/fe376ce8/checkpoint_000022)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:38:03,545 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1709 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1199986)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/fe376ce8/checkpoint_000023)\n",
            "2025-03-12 23:38:04,024\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:38:05,305\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1199986)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/fe376ce8/checkpoint_000024)\n",
            "2025-03-12 23:38:06,549\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1199986)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/fe376ce8/checkpoint_000025)\n",
            "2025-03-12 23:38:07,803\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1199986)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/fe376ce8/checkpoint_000026)\n",
            "\u001b[36m(RayTrainWorker pid=1199986)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/fe376ce8/checkpoint_000027)\n",
            "2025-03-12 23:38:09,055\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:38:10,318\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1199986)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/fe376ce8/checkpoint_000028)\n",
            "2025-03-12 23:38:11,555\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1199986)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/fe376ce8/checkpoint_000029)\n",
            "\u001b[36m(RayTrainWorker pid=1199986)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/fe376ce8/checkpoint_000030)\n",
            "2025-03-12 23:38:12,854\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:38:13,553 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1708 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:38:14,123\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1199986)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/fe376ce8/checkpoint_000031)\n",
            "\u001b[36m(RayTrainWorker pid=1199986)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/fe376ce8/checkpoint_000032)\n",
            "2025-03-12 23:38:15,399\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:38:16,661\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1199986)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/fe376ce8/checkpoint_000033)\n",
            "2025-03-12 23:38:17,918\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1199986)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/fe376ce8/checkpoint_000034)\n",
            "\u001b[36m(RayTrainWorker pid=1199986)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/fe376ce8/checkpoint_000035)\n",
            "2025-03-12 23:38:19,194\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:38:20,487\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1199986)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/fe376ce8/checkpoint_000036)\n",
            "2025-03-12 23:38:21,813\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1199986)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/fe376ce8/checkpoint_000037)\n",
            "2025-03-12 23:38:23,117\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1199986)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/fe376ce8/checkpoint_000038)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:38:23,562 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1708 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:38:24,406\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1199986)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/fe376ce8/checkpoint_000039)\n",
            "\u001b[36m(RayTrainWorker pid=1199986)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/fe376ce8/checkpoint_000040)\n",
            "2025-03-12 23:38:25,667\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:38:26,922\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1199986)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/fe376ce8/checkpoint_000041)\n",
            "2025-03-12 23:38:28,171\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1199986)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/fe376ce8/checkpoint_000042)\n",
            "\u001b[36m(RayTrainWorker pid=1199986)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/fe376ce8/checkpoint_000043)\n",
            "2025-03-12 23:38:29,449\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:38:30,743\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1199986)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/fe376ce8/checkpoint_000044)\n",
            "\u001b[36m(RayTrainWorker pid=1199986)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/fe376ce8/checkpoint_000045)\n",
            "2025-03-12 23:38:32,023\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:38:33,293\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1199986)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/fe376ce8/checkpoint_000046)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:38:33,570 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1707 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1199986)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/fe376ce8/checkpoint_000047)\n",
            "2025-03-12 23:38:34,590\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:38:35,849\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1199986)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/fe376ce8/checkpoint_000048)\n",
            "2025-03-12 23:38:37,097\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1199986)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/fe376ce8/checkpoint_000049)\n",
            "\u001b[36m(RayTrainWorker pid=1199986)\u001b[0m `Trainer.fit` stopped: `max_epochs=50` reached.\n",
            "\u001b[36m(RayTrainWorker pid=1199986)\u001b[0m [rank0]:[W312 23:38:39.753132221 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:38:43,579 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1707 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(TorchTrainer pid=1201471)\u001b[0m Started distributed worker processes: \n",
            "\u001b[36m(TorchTrainer pid=1201471)\u001b[0m - (node_id=6cf2562c5de19587d3dc355f8ebead9f15bbc1bb9651b7563d063b82, ip=10.128.0.3, pid=1201705) world_rank=0, local_rank=0, node_rank=0\n",
            "\u001b[36m(RayTrainWorker pid=1201705)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:38:53,587 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1705 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(RayTrainWorker pid=1201705)\u001b[0m Getting checkpoint from /home/jupyter/AgenticADMET/notebooks/../output/artifacts/mol_mlm_roberta_zinc/last.ckpt...\n",
            "\u001b[36m(RayTrainWorker pid=1201705)\u001b[0m Loading checkpoint from roberta to roberta...\n",
            "\u001b[36m(RayTrainWorker pid=1201705)\u001b[0m <All keys matched successfully>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(RayTrainWorker pid=1201705)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/ray/train/lightning/_lightning_utils.py:262: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
            "\u001b[36m(RayTrainWorker pid=1201705)\u001b[0m `get_trial_name` is deprecated because the concept of a `Trial` will soon be removed in Ray Train.Ray Train will no longer assume that it's running within a Ray Tune `Trial` in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
            "\u001b[36m(RayTrainWorker pid=1201705)\u001b[0m GPU available: True (cuda), used: True\n",
            "\u001b[36m(RayTrainWorker pid=1201705)\u001b[0m TPU available: False, using: 0 TPU cores\n",
            "\u001b[36m(RayTrainWorker pid=1201705)\u001b[0m HPU available: False, using: 0 HPUs\n",
            "\u001b[36m(RayTrainWorker pid=1201705)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
            "\u001b[36m(RayTrainWorker pid=1201705)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\u001b[36m(RayTrainWorker pid=1201705)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=1201705)\u001b[0m   | Name      | Type         | Params | Mode \n",
            "\u001b[36m(RayTrainWorker pid=1201705)\u001b[0m ---------------------------------------------------\n",
            "\u001b[36m(RayTrainWorker pid=1201705)\u001b[0m 0 | roberta   | RobertaModel | 8.7 M  | train\n",
            "\u001b[36m(RayTrainWorker pid=1201705)\u001b[0m 1 | predictor | MLP          | 49.9 K | train\n",
            "\u001b[36m(RayTrainWorker pid=1201705)\u001b[0m 2 | criterion | MSE          | 0      | train\n",
            "\u001b[36m(RayTrainWorker pid=1201705)\u001b[0m 3 | metrics   | ModuleList   | 0      | train\n",
            "\u001b[36m(RayTrainWorker pid=1201705)\u001b[0m ---------------------------------------------------\n",
            "\u001b[36m(RayTrainWorker pid=1201705)\u001b[0m 8.7 M     Trainable params\n",
            "\u001b[36m(RayTrainWorker pid=1201705)\u001b[0m 0         Non-trainable params\n",
            "\u001b[36m(RayTrainWorker pid=1201705)\u001b[0m 8.7 M     Total params\n",
            "\u001b[36m(RayTrainWorker pid=1201705)\u001b[0m 34.898    Total estimated model params size (MB)\n",
            "\u001b[36m(RayTrainWorker pid=1201705)\u001b[0m 128       Modules in train mode\n",
            "\u001b[36m(RayTrainWorker pid=1201705)\u001b[0m 0         Modules in eval mode\n",
            "\u001b[36m(RayTrainWorker pid=1201705)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
            "\u001b[36m(RayTrainWorker pid=1201705)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1201705)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
            "\u001b[36m(RayTrainWorker pid=1201705)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "\u001b[36m(RayTrainWorker pid=1201705)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('lr', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1201705)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('train_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1201705)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5fa0d29c/checkpoint_000000)\n",
            "2025-03-12 23:38:57,654\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1201705)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5fa0d29c/checkpoint_000001)\n",
            "2025-03-12 23:38:58,789\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1201705)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5fa0d29c/checkpoint_000002)\n",
            "2025-03-12 23:38:59,936\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1201705)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5fa0d29c/checkpoint_000003)\n",
            "2025-03-12 23:39:01,079\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1201705)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5fa0d29c/checkpoint_000004)\n",
            "\u001b[36m(RayTrainWorker pid=1201705)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5fa0d29c/checkpoint_000005)\n",
            "2025-03-12 23:39:02,245\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1201705)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5fa0d29c/checkpoint_000006)\n",
            "2025-03-12 23:39:03,409\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:39:03,596 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1703 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1201705)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5fa0d29c/checkpoint_000007)\n",
            "2025-03-12 23:39:04,607\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:39:05,781\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1201705)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5fa0d29c/checkpoint_000008)\n",
            "2025-03-12 23:39:06,927\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1201705)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5fa0d29c/checkpoint_000009)\n",
            "\u001b[36m(RayTrainWorker pid=1201705)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5fa0d29c/checkpoint_000010)\n",
            "2025-03-12 23:39:08,112\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1201705)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5fa0d29c/checkpoint_000011)\n",
            "2025-03-12 23:39:09,247\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:39:10,421\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1201705)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5fa0d29c/checkpoint_000012)\n",
            "2025-03-12 23:39:11,532\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1201705)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5fa0d29c/checkpoint_000013)\n",
            "2025-03-12 23:39:12,657\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1201705)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5fa0d29c/checkpoint_000014)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:39:13,604 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.0944 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1201705)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5fa0d29c/checkpoint_000015)\n",
            "2025-03-12 23:39:13,838\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:39:14,970\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1201705)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5fa0d29c/checkpoint_000016)\n",
            "2025-03-12 23:39:16,153\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1201705)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5fa0d29c/checkpoint_000017)\n",
            "2025-03-12 23:39:17,347\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1201705)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5fa0d29c/checkpoint_000018)\n",
            "2025-03-12 23:39:18,537\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1201705)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5fa0d29c/checkpoint_000019)\n",
            "\u001b[36m(RayTrainWorker pid=1201705)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5fa0d29c/checkpoint_000020)\n",
            "2025-03-12 23:39:19,701\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:39:20,863\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1201705)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5fa0d29c/checkpoint_000021)\n",
            "2025-03-12 23:39:21,976\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1201705)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5fa0d29c/checkpoint_000022)\n",
            "2025-03-12 23:39:23,119\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1201705)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5fa0d29c/checkpoint_000023)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:39:23,613 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1702 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1201705)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5fa0d29c/checkpoint_000024)\n",
            "2025-03-12 23:39:24,313\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1201705)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5fa0d29c/checkpoint_000025)\n",
            "2025-03-12 23:39:25,457\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:39:26,644\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1201705)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5fa0d29c/checkpoint_000026)\n",
            "2025-03-12 23:39:27,795\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1201705)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5fa0d29c/checkpoint_000027)\n",
            "2025-03-12 23:39:28,993\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1201705)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5fa0d29c/checkpoint_000028)\n",
            "2025-03-12 23:39:30,146\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1201705)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5fa0d29c/checkpoint_000029)\n",
            "2025-03-12 23:39:31,330\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1201705)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5fa0d29c/checkpoint_000030)\n",
            "2025-03-12 23:39:32,466\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1201705)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5fa0d29c/checkpoint_000031)\n",
            "2025-03-12 23:39:33,592\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:39:33,622 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1701 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1201705)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5fa0d29c/checkpoint_000032)\n",
            "2025-03-12 23:39:34,707\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1201705)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5fa0d29c/checkpoint_000033)\n",
            "2025-03-12 23:39:35,783\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1201705)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5fa0d29c/checkpoint_000034)\n",
            "\u001b[36m(RayTrainWorker pid=1201705)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5fa0d29c/checkpoint_000035)\n",
            "2025-03-12 23:39:36,924\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1201705)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5fa0d29c/checkpoint_000036)\n",
            "2025-03-12 23:39:38,047\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1201705)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5fa0d29c/checkpoint_000037)\n",
            "2025-03-12 23:39:39,178\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:39:40,361\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1201705)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5fa0d29c/checkpoint_000038)\n",
            "2025-03-12 23:39:41,580\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1201705)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5fa0d29c/checkpoint_000039)\n",
            "2025-03-12 23:39:42,753\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1201705)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5fa0d29c/checkpoint_000040)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:39:43,630 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1701 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:39:43,929\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1201705)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5fa0d29c/checkpoint_000041)\n",
            "\u001b[36m(RayTrainWorker pid=1201705)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5fa0d29c/checkpoint_000042)\n",
            "2025-03-12 23:39:45,144\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:39:46,324\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1201705)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5fa0d29c/checkpoint_000043)\n",
            "\u001b[36m(RayTrainWorker pid=1201705)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5fa0d29c/checkpoint_000044)\n",
            "2025-03-12 23:39:47,508\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:39:48,666\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1201705)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5fa0d29c/checkpoint_000045)\n",
            "2025-03-12 23:39:49,838\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1201705)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5fa0d29c/checkpoint_000046)\n",
            "2025-03-12 23:39:51,003\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1201705)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5fa0d29c/checkpoint_000047)\n",
            "2025-03-12 23:39:52,171\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1201705)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5fa0d29c/checkpoint_000048)\n",
            "\u001b[36m(RayTrainWorker pid=1201705)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5fa0d29c/checkpoint_000049)\n",
            "\u001b[36m(RayTrainWorker pid=1201705)\u001b[0m `Trainer.fit` stopped: `max_epochs=50` reached.\n",
            "2025-03-12 23:39:53,336\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:39:53,639 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.17 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1201705)\u001b[0m [rank0]:[W312 23:39:55.065402254 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:40:03,647 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1698 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(TorchTrainer pid=1203165)\u001b[0m Started distributed worker processes: \n",
            "\u001b[36m(TorchTrainer pid=1203165)\u001b[0m - (node_id=6cf2562c5de19587d3dc355f8ebead9f15bbc1bb9651b7563d063b82, ip=10.128.0.3, pid=1203341) world_rank=0, local_rank=0, node_rank=0\n",
            "\u001b[36m(RayTrainWorker pid=1203341)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(RayTrainWorker pid=1203341)\u001b[0m Getting checkpoint from /home/jupyter/AgenticADMET/notebooks/../output/artifacts/mol_mlm_roberta_zinc/last.ckpt...\n",
            "\u001b[36m(RayTrainWorker pid=1203341)\u001b[0m Loading checkpoint from roberta to roberta...\n",
            "\u001b[36m(RayTrainWorker pid=1203341)\u001b[0m <All keys matched successfully>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(RayTrainWorker pid=1203341)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/ray/train/lightning/_lightning_utils.py:262: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
            "\u001b[36m(RayTrainWorker pid=1203341)\u001b[0m `get_trial_name` is deprecated because the concept of a `Trial` will soon be removed in Ray Train.Ray Train will no longer assume that it's running within a Ray Tune `Trial` in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
            "\u001b[36m(RayTrainWorker pid=1203341)\u001b[0m GPU available: True (cuda), used: True\n",
            "\u001b[36m(RayTrainWorker pid=1203341)\u001b[0m TPU available: False, using: 0 TPU cores\n",
            "\u001b[36m(RayTrainWorker pid=1203341)\u001b[0m HPU available: False, using: 0 HPUs\n",
            "\u001b[36m(RayTrainWorker pid=1203341)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
            "\u001b[36m(RayTrainWorker pid=1203341)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\u001b[36m(RayTrainWorker pid=1203341)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=1203341)\u001b[0m   | Name      | Type         | Params | Mode \n",
            "\u001b[36m(RayTrainWorker pid=1203341)\u001b[0m ---------------------------------------------------\n",
            "\u001b[36m(RayTrainWorker pid=1203341)\u001b[0m 0 | roberta   | RobertaModel | 8.7 M  | train\n",
            "\u001b[36m(RayTrainWorker pid=1203341)\u001b[0m 1 | predictor | MLP          | 297 K  | train\n",
            "\u001b[36m(RayTrainWorker pid=1203341)\u001b[0m 2 | criterion | MSE          | 0      | train\n",
            "\u001b[36m(RayTrainWorker pid=1203341)\u001b[0m 3 | metrics   | ModuleList   | 0      | train\n",
            "\u001b[36m(RayTrainWorker pid=1203341)\u001b[0m ---------------------------------------------------\n",
            "\u001b[36m(RayTrainWorker pid=1203341)\u001b[0m 9.0 M     Trainable params\n",
            "\u001b[36m(RayTrainWorker pid=1203341)\u001b[0m 0         Non-trainable params\n",
            "\u001b[36m(RayTrainWorker pid=1203341)\u001b[0m 9.0 M     Total params\n",
            "\u001b[36m(RayTrainWorker pid=1203341)\u001b[0m 35.889    Total estimated model params size (MB)\n",
            "\u001b[36m(RayTrainWorker pid=1203341)\u001b[0m 132       Modules in train mode\n",
            "\u001b[36m(RayTrainWorker pid=1203341)\u001b[0m 0         Modules in eval mode\n",
            "\u001b[36m(RayTrainWorker pid=1203341)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
            "\u001b[36m(RayTrainWorker pid=1203341)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1203341)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
            "\u001b[36m(RayTrainWorker pid=1203341)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "\u001b[36m(RayTrainWorker pid=1203341)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('lr', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1203341)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('train_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1203341)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/a1a7455a/checkpoint_000000)\n",
            "2025-03-12 23:40:13,729\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:40:13,656 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.0693 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1203341)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/a1a7455a/checkpoint_000001)\n",
            "2025-03-12 23:40:14,873\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1203341)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/a1a7455a/checkpoint_000002)\n",
            "2025-03-12 23:40:16,008\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1203341)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/a1a7455a/checkpoint_000003)\n",
            "\u001b[36m(RayTrainWorker pid=1203341)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/a1a7455a/checkpoint_000004)\n",
            "2025-03-12 23:40:17,250\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:40:18,362\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1203341)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/a1a7455a/checkpoint_000005)\n",
            "2025-03-12 23:40:19,577\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1203341)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/a1a7455a/checkpoint_000006)\n",
            "2025-03-12 23:40:20,690\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1203341)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/a1a7455a/checkpoint_000007)\n",
            "2025-03-12 23:40:21,841\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1203341)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/a1a7455a/checkpoint_000008)\n",
            "\u001b[36m(RayTrainWorker pid=1203341)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/a1a7455a/checkpoint_000009)\n",
            "2025-03-12 23:40:23,013\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:40:23,665 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1696 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1203341)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/a1a7455a/checkpoint_000010)\n",
            "2025-03-12 23:40:24,229\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:40:25,396\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1203341)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/a1a7455a/checkpoint_000011)\n",
            "\u001b[36m(RayTrainWorker pid=1203341)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/a1a7455a/checkpoint_000012)\n",
            "2025-03-12 23:40:26,592\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:40:27,786\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1203341)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/a1a7455a/checkpoint_000013)\n",
            "\u001b[36m(RayTrainWorker pid=1203341)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/a1a7455a/checkpoint_000014)\n",
            "2025-03-12 23:40:28,973\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:40:30,172\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1203341)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/a1a7455a/checkpoint_000015)\n",
            "2025-03-12 23:40:31,344\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1203341)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/a1a7455a/checkpoint_000016)\n",
            "2025-03-12 23:40:32,524\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1203341)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/a1a7455a/checkpoint_000017)\n",
            "\u001b[36m(RayTrainWorker pid=1203341)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/a1a7455a/checkpoint_000018)\n",
            "2025-03-12 23:40:33,713\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:40:33,673 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.0692 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:40:34,834\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1203341)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/a1a7455a/checkpoint_000019)\n",
            "2025-03-12 23:40:36,023\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1203341)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/a1a7455a/checkpoint_000020)\n",
            "2025-03-12 23:40:37,196\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1203341)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/a1a7455a/checkpoint_000021)\n",
            "2025-03-12 23:40:38,378\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1203341)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/a1a7455a/checkpoint_000022)\n",
            "2025-03-12 23:40:39,587\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1203341)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/a1a7455a/checkpoint_000023)\n",
            "2025-03-12 23:40:40,676\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1203341)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/a1a7455a/checkpoint_000024)\n",
            "2025-03-12 23:40:41,856\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1203341)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/a1a7455a/checkpoint_000025)\n",
            "2025-03-12 23:40:43,102\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1203341)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/a1a7455a/checkpoint_000026)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:40:43,682 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1695 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1203341)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/a1a7455a/checkpoint_000027)\n",
            "2025-03-12 23:40:44,263\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:40:45,433\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1203341)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/a1a7455a/checkpoint_000028)\n",
            "2025-03-12 23:40:46,662\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1203341)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/a1a7455a/checkpoint_000029)\n",
            "2025-03-12 23:40:47,812\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1203341)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/a1a7455a/checkpoint_000030)\n",
            "2025-03-12 23:40:49,041\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1203341)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/a1a7455a/checkpoint_000031)\n",
            "2025-03-12 23:40:50,245\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1203341)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/a1a7455a/checkpoint_000032)\n",
            "\u001b[36m(RayTrainWorker pid=1203341)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/a1a7455a/checkpoint_000033)\n",
            "2025-03-12 23:40:51,429\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:40:52,605\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1203341)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/a1a7455a/checkpoint_000034)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:40:53,690 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.069 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1203341)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/a1a7455a/checkpoint_000035)\n",
            "2025-03-12 23:40:53,790\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1203341)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/a1a7455a/checkpoint_000036)\n",
            "2025-03-12 23:40:54,938\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:40:56,132\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1203341)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/a1a7455a/checkpoint_000037)\n",
            "\u001b[36m(RayTrainWorker pid=1203341)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/a1a7455a/checkpoint_000038)\n",
            "2025-03-12 23:40:57,295\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:40:58,475\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1203341)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/a1a7455a/checkpoint_000039)\n",
            "2025-03-12 23:40:59,594\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1203341)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/a1a7455a/checkpoint_000040)\n",
            "\u001b[36m(RayTrainWorker pid=1203341)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/a1a7455a/checkpoint_000041)\n",
            "2025-03-12 23:41:00,817\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:41:01,931\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1203341)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/a1a7455a/checkpoint_000042)\n",
            "2025-03-12 23:41:03,082\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1203341)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/a1a7455a/checkpoint_000043)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:41:03,699 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1692 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:41:04,305\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1203341)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/a1a7455a/checkpoint_000044)\n",
            "2025-03-12 23:41:05,484\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1203341)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/a1a7455a/checkpoint_000045)\n",
            "2025-03-12 23:41:06,703\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1203341)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/a1a7455a/checkpoint_000046)\n",
            "2025-03-12 23:41:07,912\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1203341)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/a1a7455a/checkpoint_000047)\n",
            "\u001b[36m(RayTrainWorker pid=1203341)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/a1a7455a/checkpoint_000048)\n",
            "2025-03-12 23:41:09,113\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:41:10,290\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1203341)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/a1a7455a/checkpoint_000049)\n",
            "\u001b[36m(RayTrainWorker pid=1203341)\u001b[0m `Trainer.fit` stopped: `max_epochs=50` reached.\n",
            "\u001b[36m(RayTrainWorker pid=1203341)\u001b[0m [rank0]:[W312 23:41:12.182018378 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:41:13,708 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1692 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(TorchTrainer pid=1204838)\u001b[0m Started distributed worker processes: \n",
            "\u001b[36m(TorchTrainer pid=1204838)\u001b[0m - (node_id=6cf2562c5de19587d3dc355f8ebead9f15bbc1bb9651b7563d063b82, ip=10.128.0.3, pid=1205068) world_rank=0, local_rank=0, node_rank=0\n",
            "\u001b[36m(RayTrainWorker pid=1205068)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:41:23,716 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.169 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1205068)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/ray/train/lightning/_lightning_utils.py:262: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
            "\u001b[36m(RayTrainWorker pid=1205068)\u001b[0m `get_trial_name` is deprecated because the concept of a `Trial` will soon be removed in Ray Train.Ray Train will no longer assume that it's running within a Ray Tune `Trial` in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
            "\u001b[36m(RayTrainWorker pid=1205068)\u001b[0m GPU available: True (cuda), used: True\n",
            "\u001b[36m(RayTrainWorker pid=1205068)\u001b[0m TPU available: False, using: 0 TPU cores\n",
            "\u001b[36m(RayTrainWorker pid=1205068)\u001b[0m HPU available: False, using: 0 HPUs\n",
            "\u001b[36m(RayTrainWorker pid=1205068)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(RayTrainWorker pid=1205068)\u001b[0m Getting checkpoint from /home/jupyter/AgenticADMET/notebooks/../output/artifacts/mol_mlm_roberta_zinc/last.ckpt...\n",
            "\u001b[36m(RayTrainWorker pid=1205068)\u001b[0m Loading checkpoint from roberta to roberta...\n",
            "\u001b[36m(RayTrainWorker pid=1205068)\u001b[0m <All keys matched successfully>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(RayTrainWorker pid=1205068)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\u001b[36m(RayTrainWorker pid=1205068)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=1205068)\u001b[0m   | Name      | Type         | Params | Mode \n",
            "\u001b[36m(RayTrainWorker pid=1205068)\u001b[0m ---------------------------------------------------\n",
            "\u001b[36m(RayTrainWorker pid=1205068)\u001b[0m 0 | roberta   | RobertaModel | 8.7 M  | train\n",
            "\u001b[36m(RayTrainWorker pid=1205068)\u001b[0m 1 | predictor | MLP          | 99.8 K | train\n",
            "\u001b[36m(RayTrainWorker pid=1205068)\u001b[0m 2 | criterion | MSE          | 0      | train\n",
            "\u001b[36m(RayTrainWorker pid=1205068)\u001b[0m 3 | metrics   | ModuleList   | 0      | train\n",
            "\u001b[36m(RayTrainWorker pid=1205068)\u001b[0m ---------------------------------------------------\n",
            "\u001b[36m(RayTrainWorker pid=1205068)\u001b[0m 8.8 M     Trainable params\n",
            "\u001b[36m(RayTrainWorker pid=1205068)\u001b[0m 0         Non-trainable params\n",
            "\u001b[36m(RayTrainWorker pid=1205068)\u001b[0m 8.8 M     Total params\n",
            "\u001b[36m(RayTrainWorker pid=1205068)\u001b[0m 35.098    Total estimated model params size (MB)\n",
            "\u001b[36m(RayTrainWorker pid=1205068)\u001b[0m 128       Modules in train mode\n",
            "\u001b[36m(RayTrainWorker pid=1205068)\u001b[0m 0         Modules in eval mode\n",
            "\u001b[36m(RayTrainWorker pid=1205068)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
            "\u001b[36m(RayTrainWorker pid=1205068)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1205068)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
            "\u001b[36m(RayTrainWorker pid=1205068)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (11) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "\u001b[36m(RayTrainWorker pid=1205068)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('lr', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1205068)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('train_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1205068)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/6d6626b0/checkpoint_000000)\n",
            "\u001b[36m(RayTrainWorker pid=1205068)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/6d6626b0/checkpoint_000001)\n",
            "2025-03-12 23:41:30,458\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:41:31,600\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1205068)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/6d6626b0/checkpoint_000002)\n",
            "2025-03-12 23:41:32,733\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1205068)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/6d6626b0/checkpoint_000003)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:41:33,725 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.0707 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:41:33,888\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1205068)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/6d6626b0/checkpoint_000004)\n",
            "2025-03-12 23:41:35,003\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1205068)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/6d6626b0/checkpoint_000005)\n",
            "\u001b[36m(RayTrainWorker pid=1205068)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/6d6626b0/checkpoint_000006)\n",
            "2025-03-12 23:41:36,201\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:41:37,293\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1205068)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/6d6626b0/checkpoint_000007)\n",
            "\u001b[36m(RayTrainWorker pid=1205068)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/6d6626b0/checkpoint_000008)\n",
            "2025-03-12 23:41:38,473\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1205068)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/6d6626b0/checkpoint_000009)\n",
            "2025-03-12 23:41:39,613\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:41:40,704\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1205068)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/6d6626b0/checkpoint_000010)\n",
            "2025-03-12 23:41:41,814\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1205068)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/6d6626b0/checkpoint_000011)\n",
            "2025-03-12 23:41:42,918\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1205068)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/6d6626b0/checkpoint_000012)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:41:43,734 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1688 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1205068)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/6d6626b0/checkpoint_000013)\n",
            "2025-03-12 23:41:44,026\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1205068)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/6d6626b0/checkpoint_000014)\n",
            "2025-03-12 23:41:45,151\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1205068)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/6d6626b0/checkpoint_000015)\n",
            "2025-03-12 23:41:46,269\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1205068)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/6d6626b0/checkpoint_000016)\n",
            "2025-03-12 23:41:47,405\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:41:48,502\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1205068)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/6d6626b0/checkpoint_000017)\n",
            "2025-03-12 23:41:49,629\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1205068)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/6d6626b0/checkpoint_000018)\n",
            "\u001b[36m(RayTrainWorker pid=1205068)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/6d6626b0/checkpoint_000019)\n",
            "2025-03-12 23:41:50,807\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:41:51,958\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1205068)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/6d6626b0/checkpoint_000020)\n",
            "2025-03-12 23:41:53,094\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1205068)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/6d6626b0/checkpoint_000021)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:41:53,742 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1687 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:41:54,240\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1205068)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/6d6626b0/checkpoint_000022)\n",
            "2025-03-12 23:41:55,382\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1205068)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/6d6626b0/checkpoint_000023)\n",
            "2025-03-12 23:41:56,509\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1205068)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/6d6626b0/checkpoint_000024)\n",
            "2025-03-12 23:41:57,660\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1205068)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/6d6626b0/checkpoint_000025)\n",
            "\u001b[36m(RayTrainWorker pid=1205068)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/6d6626b0/checkpoint_000026)\n",
            "2025-03-12 23:41:58,843\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:41:59,996\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1205068)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/6d6626b0/checkpoint_000027)\n",
            "\u001b[36m(RayTrainWorker pid=1205068)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/6d6626b0/checkpoint_000028)\n",
            "2025-03-12 23:42:01,195\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:42:02,340\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1205068)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/6d6626b0/checkpoint_000029)\n",
            "2025-03-12 23:42:03,510\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1205068)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/6d6626b0/checkpoint_000030)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:42:03,752 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1686 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1205068)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/6d6626b0/checkpoint_000031)\n",
            "2025-03-12 23:42:04,690\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1205068)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/6d6626b0/checkpoint_000032)\n",
            "2025-03-12 23:42:05,842\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:42:06,995\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1205068)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/6d6626b0/checkpoint_000033)\n",
            "\u001b[36m(RayTrainWorker pid=1205068)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/6d6626b0/checkpoint_000034)\n",
            "2025-03-12 23:42:08,098\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:42:09,242\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1205068)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/6d6626b0/checkpoint_000035)\n",
            "2025-03-12 23:42:10,407\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1205068)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/6d6626b0/checkpoint_000036)\n",
            "\u001b[36m(RayTrainWorker pid=1205068)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/6d6626b0/checkpoint_000037)\n",
            "2025-03-12 23:42:11,560\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1205068)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/6d6626b0/checkpoint_000038)\n",
            "2025-03-12 23:42:12,713\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:42:13,760 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.0703 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1205068)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/6d6626b0/checkpoint_000039)\n",
            "2025-03-12 23:42:13,842\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:42:15,003\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1205068)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/6d6626b0/checkpoint_000040)\n",
            "2025-03-12 23:42:16,143\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1205068)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/6d6626b0/checkpoint_000041)\n",
            "2025-03-12 23:42:17,255\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1205068)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/6d6626b0/checkpoint_000042)\n",
            "2025-03-12 23:42:18,342\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1205068)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/6d6626b0/checkpoint_000043)\n",
            "\u001b[36m(RayTrainWorker pid=1205068)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/6d6626b0/checkpoint_000044)\n",
            "2025-03-12 23:42:19,492\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:42:20,653\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1205068)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/6d6626b0/checkpoint_000045)\n",
            "\u001b[36m(RayTrainWorker pid=1205068)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/6d6626b0/checkpoint_000046)\n",
            "2025-03-12 23:42:21,754\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1205068)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/6d6626b0/checkpoint_000047)\n",
            "2025-03-12 23:42:22,888\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:42:23,769 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1149 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1205068)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/6d6626b0/checkpoint_000048)\n",
            "2025-03-12 23:42:24,009\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1205068)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/6d6626b0/checkpoint_000049)\n",
            "2025-03-12 23:42:25,140\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1205068)\u001b[0m `Trainer.fit` stopped: `max_epochs=50` reached.\n",
            "\u001b[36m(RayTrainWorker pid=1205068)\u001b[0m [rank0]:[W312 23:42:27.736384533 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:42:33,777 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1684 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(TorchTrainer pid=1206599)\u001b[0m Started distributed worker processes: \n",
            "\u001b[36m(TorchTrainer pid=1206599)\u001b[0m - (node_id=6cf2562c5de19587d3dc355f8ebead9f15bbc1bb9651b7563d063b82, ip=10.128.0.3, pid=1206756) world_rank=0, local_rank=0, node_rank=0\n",
            "\u001b[36m(RayTrainWorker pid=1206756)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(RayTrainWorker pid=1206756)\u001b[0m Getting checkpoint from /home/jupyter/AgenticADMET/notebooks/../output/artifacts/mol_mlm_roberta_zinc/last.ckpt...\n",
            "\u001b[36m(RayTrainWorker pid=1206756)\u001b[0m Loading checkpoint from roberta to roberta...\n",
            "\u001b[36m(RayTrainWorker pid=1206756)\u001b[0m <All keys matched successfully>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(RayTrainWorker pid=1206756)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/ray/train/lightning/_lightning_utils.py:262: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
            "\u001b[36m(RayTrainWorker pid=1206756)\u001b[0m `get_trial_name` is deprecated because the concept of a `Trial` will soon be removed in Ray Train.Ray Train will no longer assume that it's running within a Ray Tune `Trial` in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
            "\u001b[36m(RayTrainWorker pid=1206756)\u001b[0m GPU available: True (cuda), used: True\n",
            "\u001b[36m(RayTrainWorker pid=1206756)\u001b[0m TPU available: False, using: 0 TPU cores\n",
            "\u001b[36m(RayTrainWorker pid=1206756)\u001b[0m HPU available: False, using: 0 HPUs\n",
            "\u001b[36m(RayTrainWorker pid=1206756)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
            "\u001b[36m(RayTrainWorker pid=1206756)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\u001b[36m(RayTrainWorker pid=1206756)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=1206756)\u001b[0m   | Name      | Type         | Params | Mode \n",
            "\u001b[36m(RayTrainWorker pid=1206756)\u001b[0m ---------------------------------------------------\n",
            "\u001b[36m(RayTrainWorker pid=1206756)\u001b[0m 0 | roberta   | RobertaModel | 8.7 M  | train\n",
            "\u001b[36m(RayTrainWorker pid=1206756)\u001b[0m 1 | predictor | MLP          | 1.4 M  | train\n",
            "\u001b[36m(RayTrainWorker pid=1206756)\u001b[0m 2 | criterion | MSE          | 0      | train\n",
            "\u001b[36m(RayTrainWorker pid=1206756)\u001b[0m 3 | metrics   | ModuleList   | 0      | train\n",
            "\u001b[36m(RayTrainWorker pid=1206756)\u001b[0m ---------------------------------------------------\n",
            "\u001b[36m(RayTrainWorker pid=1206756)\u001b[0m 10.1 M    Trainable params\n",
            "\u001b[36m(RayTrainWorker pid=1206756)\u001b[0m 0         Non-trainable params\n",
            "\u001b[36m(RayTrainWorker pid=1206756)\u001b[0m 10.1 M    Total params\n",
            "\u001b[36m(RayTrainWorker pid=1206756)\u001b[0m 40.494    Total estimated model params size (MB)\n",
            "\u001b[36m(RayTrainWorker pid=1206756)\u001b[0m 132       Modules in train mode\n",
            "\u001b[36m(RayTrainWorker pid=1206756)\u001b[0m 0         Modules in eval mode\n",
            "\u001b[36m(RayTrainWorker pid=1206756)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
            "\u001b[36m(RayTrainWorker pid=1206756)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1206756)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
            "\u001b[36m(RayTrainWorker pid=1206756)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (21) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:42:43,785 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1682 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1206756)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('lr', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1206756)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('train_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1206756)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/995662df/checkpoint_000000)\n",
            "2025-03-12 23:42:45,961\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1206756)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/995662df/checkpoint_000001)\n",
            "2025-03-12 23:42:47,307\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1206756)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/995662df/checkpoint_000002)\n",
            "\u001b[36m(RayTrainWorker pid=1206756)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/995662df/checkpoint_000003)\n",
            "2025-03-12 23:42:48,659\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:42:49,987\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1206756)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/995662df/checkpoint_000004)\n",
            "2025-03-12 23:42:51,307\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1206756)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/995662df/checkpoint_000005)\n",
            "2025-03-12 23:42:52,651\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1206756)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/995662df/checkpoint_000006)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:42:53,794 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.079 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1206756)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/995662df/checkpoint_000007)\n",
            "2025-03-12 23:42:54,064\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:42:55,369\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1206756)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/995662df/checkpoint_000008)\n",
            "2025-03-12 23:42:56,768\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1206756)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/995662df/checkpoint_000009)\n",
            "2025-03-12 23:42:58,061\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1206756)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/995662df/checkpoint_000010)\n",
            "2025-03-12 23:42:59,487\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1206756)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/995662df/checkpoint_000011)\n",
            "2025-03-12 23:43:00,857\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1206756)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/995662df/checkpoint_000012)\n",
            "2025-03-12 23:43:02,225\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1206756)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/995662df/checkpoint_000013)\n",
            "2025-03-12 23:43:03,611\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1206756)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/995662df/checkpoint_000014)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:43:03,803 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1679 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:43:04,960\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1206756)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/995662df/checkpoint_000015)\n",
            "2025-03-12 23:43:06,308\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1206756)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/995662df/checkpoint_000016)\n",
            "2025-03-12 23:43:07,791\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1206756)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/995662df/checkpoint_000017)\n",
            "2025-03-12 23:43:09,308\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1206756)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/995662df/checkpoint_000018)\n",
            "\u001b[36m(RayTrainWorker pid=1206756)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/995662df/checkpoint_000019)\n",
            "2025-03-12 23:43:10,708\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1206756)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/995662df/checkpoint_000020)\n",
            "2025-03-12 23:43:12,147\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1206756)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/995662df/checkpoint_000021)\n",
            "2025-03-12 23:43:13,485\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:43:13,811 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1678 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:43:14,864\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1206756)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/995662df/checkpoint_000022)\n",
            "\u001b[36m(RayTrainWorker pid=1206756)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/995662df/checkpoint_000023)\n",
            "2025-03-12 23:43:16,142\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1206756)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/995662df/checkpoint_000024)\n",
            "2025-03-12 23:43:17,487\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1206756)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/995662df/checkpoint_000025)\n",
            "2025-03-12 23:43:18,836\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:43:20,180\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1206756)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/995662df/checkpoint_000026)\n",
            "2025-03-12 23:43:21,519\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1206756)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/995662df/checkpoint_000027)\n",
            "\u001b[36m(RayTrainWorker pid=1206756)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/995662df/checkpoint_000028)\n",
            "2025-03-12 23:43:22,852\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:43:23,820 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1677 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1206756)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/995662df/checkpoint_000029)\n",
            "2025-03-12 23:43:24,190\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:43:25,594\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1206756)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/995662df/checkpoint_000030)\n",
            "\u001b[36m(RayTrainWorker pid=1206756)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/995662df/checkpoint_000031)\n",
            "2025-03-12 23:43:26,949\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1206756)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/995662df/checkpoint_000032)\n",
            "2025-03-12 23:43:28,295\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:43:29,671\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1206756)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/995662df/checkpoint_000033)\n",
            "2025-03-12 23:43:30,992\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1206756)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/995662df/checkpoint_000034)\n",
            "2025-03-12 23:43:32,347\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1206756)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/995662df/checkpoint_000035)\n",
            "2025-03-12 23:43:33,675\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1206756)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/995662df/checkpoint_000036)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:43:33,829 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1677 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1206756)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/995662df/checkpoint_000037)\n",
            "2025-03-12 23:43:34,986\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:43:36,388\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1206756)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/995662df/checkpoint_000038)\n",
            "\u001b[36m(RayTrainWorker pid=1206756)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/995662df/checkpoint_000039)\n",
            "2025-03-12 23:43:37,754\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:43:39,146\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1206756)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/995662df/checkpoint_000040)\n",
            "2025-03-12 23:43:40,502\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1206756)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/995662df/checkpoint_000041)\n",
            "\u001b[36m(RayTrainWorker pid=1206756)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/995662df/checkpoint_000042)\n",
            "2025-03-12 23:43:41,851\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1206756)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/995662df/checkpoint_000043)\n",
            "2025-03-12 23:43:43,202\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:43:43,838 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1676 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1206756)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/995662df/checkpoint_000044)\n",
            "2025-03-12 23:43:44,542\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1206756)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/995662df/checkpoint_000045)\n",
            "2025-03-12 23:43:45,900\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:43:47,272\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1206756)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/995662df/checkpoint_000046)\n",
            "2025-03-12 23:43:48,621\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1206756)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/995662df/checkpoint_000047)\n",
            "2025-03-12 23:43:50,024\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1206756)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/995662df/checkpoint_000048)\n",
            "\u001b[36m(RayTrainWorker pid=1206756)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/995662df/checkpoint_000049)\n",
            "2025-03-12 23:43:51,346\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1206756)\u001b[0m `Trainer.fit` stopped: `max_epochs=50` reached.\n",
            "\u001b[36m(RayTrainWorker pid=1206756)\u001b[0m [rank0]:[W312 23:43:53.045234661 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:43:53,846 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1675 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:44:03,855 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1673 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1208580)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
            "\u001b[36m(TorchTrainer pid=1208418)\u001b[0m Started distributed worker processes: \n",
            "\u001b[36m(TorchTrainer pid=1208418)\u001b[0m - (node_id=6cf2562c5de19587d3dc355f8ebead9f15bbc1bb9651b7563d063b82, ip=10.128.0.3, pid=1208580) world_rank=0, local_rank=0, node_rank=0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(RayTrainWorker pid=1208580)\u001b[0m Getting checkpoint from /home/jupyter/AgenticADMET/notebooks/../output/artifacts/mol_mlm_roberta_zinc/last.ckpt...\n",
            "\u001b[36m(RayTrainWorker pid=1208580)\u001b[0m Loading checkpoint from roberta to roberta...\n",
            "\u001b[36m(RayTrainWorker pid=1208580)\u001b[0m <All keys matched successfully>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(RayTrainWorker pid=1208580)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/ray/train/lightning/_lightning_utils.py:262: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
            "\u001b[36m(RayTrainWorker pid=1208580)\u001b[0m `get_trial_name` is deprecated because the concept of a `Trial` will soon be removed in Ray Train.Ray Train will no longer assume that it's running within a Ray Tune `Trial` in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
            "\u001b[36m(RayTrainWorker pid=1208580)\u001b[0m GPU available: True (cuda), used: True\n",
            "\u001b[36m(RayTrainWorker pid=1208580)\u001b[0m TPU available: False, using: 0 TPU cores\n",
            "\u001b[36m(RayTrainWorker pid=1208580)\u001b[0m HPU available: False, using: 0 HPUs\n",
            "\u001b[36m(RayTrainWorker pid=1208580)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
            "\u001b[36m(RayTrainWorker pid=1208580)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\u001b[36m(RayTrainWorker pid=1208580)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=1208580)\u001b[0m   | Name      | Type         | Params | Mode \n",
            "\u001b[36m(RayTrainWorker pid=1208580)\u001b[0m ---------------------------------------------------\n",
            "\u001b[36m(RayTrainWorker pid=1208580)\u001b[0m 0 | roberta   | RobertaModel | 8.7 M  | train\n",
            "\u001b[36m(RayTrainWorker pid=1208580)\u001b[0m 1 | predictor | MLP          | 297 K  | train\n",
            "\u001b[36m(RayTrainWorker pid=1208580)\u001b[0m 2 | criterion | MSE          | 0      | train\n",
            "\u001b[36m(RayTrainWorker pid=1208580)\u001b[0m 3 | metrics   | ModuleList   | 0      | train\n",
            "\u001b[36m(RayTrainWorker pid=1208580)\u001b[0m ---------------------------------------------------\n",
            "\u001b[36m(RayTrainWorker pid=1208580)\u001b[0m 9.0 M     Trainable params\n",
            "\u001b[36m(RayTrainWorker pid=1208580)\u001b[0m 0         Non-trainable params\n",
            "\u001b[36m(RayTrainWorker pid=1208580)\u001b[0m 9.0 M     Total params\n",
            "\u001b[36m(RayTrainWorker pid=1208580)\u001b[0m 35.889    Total estimated model params size (MB)\n",
            "\u001b[36m(RayTrainWorker pid=1208580)\u001b[0m 132       Modules in train mode\n",
            "\u001b[36m(RayTrainWorker pid=1208580)\u001b[0m 0         Modules in eval mode\n",
            "\u001b[36m(RayTrainWorker pid=1208580)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
            "\u001b[36m(RayTrainWorker pid=1208580)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1208580)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
            "\u001b[36m(RayTrainWorker pid=1208580)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (21) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "\u001b[36m(RayTrainWorker pid=1208580)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('lr', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1208580)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('train_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1208580)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/774d392e/checkpoint_000000)\n",
            "\u001b[36m(RayTrainWorker pid=1208580)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/774d392e/checkpoint_000001)\n",
            "2025-03-12 23:44:12,016\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1208580)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/774d392e/checkpoint_000002)\n",
            "2025-03-12 23:44:13,443\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:44:13,864 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1672 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:44:14,771\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1208580)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/774d392e/checkpoint_000003)\n",
            "2025-03-12 23:44:16,089\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1208580)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/774d392e/checkpoint_000004)\n",
            "\u001b[36m(RayTrainWorker pid=1208580)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/774d392e/checkpoint_000005)\n",
            "2025-03-12 23:44:17,368\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:44:18,645\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1208580)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/774d392e/checkpoint_000006)\n",
            "\u001b[36m(RayTrainWorker pid=1208580)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/774d392e/checkpoint_000007)\n",
            "2025-03-12 23:44:19,968\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:44:21,230\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1208580)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/774d392e/checkpoint_000008)\n",
            "\u001b[36m(RayTrainWorker pid=1208580)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/774d392e/checkpoint_000009)\n",
            "2025-03-12 23:44:22,549\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:44:23,762\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1208580)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/774d392e/checkpoint_000010)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:44:23,872 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1625 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:44:25,047\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1208580)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/774d392e/checkpoint_000011)\n",
            "\u001b[36m(RayTrainWorker pid=1208580)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/774d392e/checkpoint_000012)\n",
            "2025-03-12 23:44:26,336\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1208580)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/774d392e/checkpoint_000013)\n",
            "2025-03-12 23:44:27,580\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:44:28,903\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1208580)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/774d392e/checkpoint_000014)\n",
            "\u001b[36m(RayTrainWorker pid=1208580)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/774d392e/checkpoint_000015)\n",
            "2025-03-12 23:44:30,161\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:44:31,480\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1208580)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/774d392e/checkpoint_000016)\n",
            "2025-03-12 23:44:32,699\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1208580)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/774d392e/checkpoint_000017)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:44:33,881 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.0666 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:44:34,031\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1208580)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/774d392e/checkpoint_000018)\n",
            "2025-03-12 23:44:35,250\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1208580)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/774d392e/checkpoint_000019)\n",
            "\u001b[36m(RayTrainWorker pid=1208580)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/774d392e/checkpoint_000020)\n",
            "2025-03-12 23:44:36,546\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1208580)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/774d392e/checkpoint_000021)\n",
            "2025-03-12 23:44:37,814\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:44:39,101\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1208580)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/774d392e/checkpoint_000022)\n",
            "2025-03-12 23:44:40,416\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1208580)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/774d392e/checkpoint_000023)\n",
            "\u001b[36m(RayTrainWorker pid=1208580)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/774d392e/checkpoint_000024)\n",
            "2025-03-12 23:44:41,708\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1208580)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/774d392e/checkpoint_000025)\n",
            "2025-03-12 23:44:42,982\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:44:43,890 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.167 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:44:44,233\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1208580)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/774d392e/checkpoint_000026)\n",
            "2025-03-12 23:44:45,491\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1208580)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/774d392e/checkpoint_000027)\n",
            "2025-03-12 23:44:46,750\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1208580)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/774d392e/checkpoint_000028)\n",
            "\u001b[36m(RayTrainWorker pid=1208580)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/774d392e/checkpoint_000029)\n",
            "2025-03-12 23:44:47,988\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:44:49,216\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1208580)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/774d392e/checkpoint_000030)\n",
            "2025-03-12 23:44:50,526\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1208580)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/774d392e/checkpoint_000031)\n",
            "2025-03-12 23:44:51,791\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1208580)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/774d392e/checkpoint_000032)\n",
            "2025-03-12 23:44:53,069\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1208580)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/774d392e/checkpoint_000033)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:44:53,899 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1669 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:44:54,315\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1208580)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/774d392e/checkpoint_000034)\n",
            "\u001b[36m(RayTrainWorker pid=1208580)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/774d392e/checkpoint_000035)\n",
            "2025-03-12 23:44:55,617\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1208580)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/774d392e/checkpoint_000036)\n",
            "2025-03-12 23:44:56,862\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:44:58,146\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1208580)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/774d392e/checkpoint_000037)\n",
            "2025-03-12 23:44:59,410\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1208580)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/774d392e/checkpoint_000038)\n",
            "2025-03-12 23:45:00,683\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1208580)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/774d392e/checkpoint_000039)\n",
            "2025-03-12 23:45:01,967\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1208580)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/774d392e/checkpoint_000040)\n",
            "2025-03-12 23:45:03,309\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1208580)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/774d392e/checkpoint_000041)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:45:03,907 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1667 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1208580)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/774d392e/checkpoint_000042)\n",
            "2025-03-12 23:45:04,665\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1208580)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/774d392e/checkpoint_000043)\n",
            "2025-03-12 23:45:05,926\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:45:07,223\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1208580)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/774d392e/checkpoint_000044)\n",
            "\u001b[36m(RayTrainWorker pid=1208580)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/774d392e/checkpoint_000045)\n",
            "2025-03-12 23:45:08,480\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:45:09,742\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1208580)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/774d392e/checkpoint_000046)\n",
            "2025-03-12 23:45:10,989\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1208580)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/774d392e/checkpoint_000047)\n",
            "2025-03-12 23:45:12,314\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1208580)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/774d392e/checkpoint_000048)\n",
            "\u001b[36m(RayTrainWorker pid=1208580)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/774d392e/checkpoint_000049)\n",
            "\u001b[36m(RayTrainWorker pid=1208580)\u001b[0m `Trainer.fit` stopped: `max_epochs=50` reached.\n",
            "2025-03-12 23:45:13,625\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:45:13,916 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.162 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1208580)\u001b[0m [rank0]:[W312 23:45:15.210222437 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:45:23,925 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1649 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(TorchTrainer pid=1210279)\u001b[0m Started distributed worker processes: \n",
            "\u001b[36m(TorchTrainer pid=1210279)\u001b[0m - (node_id=6cf2562c5de19587d3dc355f8ebead9f15bbc1bb9651b7563d063b82, ip=10.128.0.3, pid=1210437) world_rank=0, local_rank=0, node_rank=0\n",
            "\u001b[36m(RayTrainWorker pid=1210437)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(RayTrainWorker pid=1210437)\u001b[0m Getting checkpoint from /home/jupyter/AgenticADMET/notebooks/../output/artifacts/mol_mlm_roberta_zinc/last.ckpt...\n",
            "\u001b[36m(RayTrainWorker pid=1210437)\u001b[0m Loading checkpoint from roberta to roberta...\n",
            "\u001b[36m(RayTrainWorker pid=1210437)\u001b[0m <All keys matched successfully>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(RayTrainWorker pid=1210437)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/ray/train/lightning/_lightning_utils.py:262: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
            "\u001b[36m(RayTrainWorker pid=1210437)\u001b[0m `get_trial_name` is deprecated because the concept of a `Trial` will soon be removed in Ray Train.Ray Train will no longer assume that it's running within a Ray Tune `Trial` in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
            "\u001b[36m(RayTrainWorker pid=1210437)\u001b[0m GPU available: True (cuda), used: True\n",
            "\u001b[36m(RayTrainWorker pid=1210437)\u001b[0m TPU available: False, using: 0 TPU cores\n",
            "\u001b[36m(RayTrainWorker pid=1210437)\u001b[0m HPU available: False, using: 0 HPUs\n",
            "\u001b[36m(RayTrainWorker pid=1210437)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
            "\u001b[36m(RayTrainWorker pid=1210437)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\u001b[36m(RayTrainWorker pid=1210437)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=1210437)\u001b[0m   | Name      | Type         | Params | Mode \n",
            "\u001b[36m(RayTrainWorker pid=1210437)\u001b[0m ---------------------------------------------------\n",
            "\u001b[36m(RayTrainWorker pid=1210437)\u001b[0m 0 | roberta   | RobertaModel | 8.7 M  | train\n",
            "\u001b[36m(RayTrainWorker pid=1210437)\u001b[0m 1 | predictor | MLP          | 462 K  | train\n",
            "\u001b[36m(RayTrainWorker pid=1210437)\u001b[0m 2 | criterion | MSE          | 0      | train\n",
            "\u001b[36m(RayTrainWorker pid=1210437)\u001b[0m 3 | metrics   | ModuleList   | 0      | train\n",
            "\u001b[36m(RayTrainWorker pid=1210437)\u001b[0m ---------------------------------------------------\n",
            "\u001b[36m(RayTrainWorker pid=1210437)\u001b[0m 9.1 M     Trainable params\n",
            "\u001b[36m(RayTrainWorker pid=1210437)\u001b[0m 0         Non-trainable params\n",
            "\u001b[36m(RayTrainWorker pid=1210437)\u001b[0m 9.1 M     Total params\n",
            "\u001b[36m(RayTrainWorker pid=1210437)\u001b[0m 36.548    Total estimated model params size (MB)\n",
            "\u001b[36m(RayTrainWorker pid=1210437)\u001b[0m 132       Modules in train mode\n",
            "\u001b[36m(RayTrainWorker pid=1210437)\u001b[0m 0         Modules in eval mode\n",
            "\u001b[36m(RayTrainWorker pid=1210437)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
            "\u001b[36m(RayTrainWorker pid=1210437)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1210437)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
            "\u001b[36m(RayTrainWorker pid=1210437)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "\u001b[36m(RayTrainWorker pid=1210437)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('lr', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1210437)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('train_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1210437)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/137d3681/checkpoint_000000)\n",
            "2025-03-12 23:45:33,408\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1210437)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/137d3681/checkpoint_000001)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:45:33,933 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1664 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:45:34,496\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1210437)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/137d3681/checkpoint_000002)\n",
            "\u001b[36m(RayTrainWorker pid=1210437)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/137d3681/checkpoint_000003)\n",
            "2025-03-12 23:45:35,657\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:45:36,749\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1210437)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/137d3681/checkpoint_000004)\n",
            "2025-03-12 23:45:37,852\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1210437)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/137d3681/checkpoint_000005)\n",
            "\u001b[36m(RayTrainWorker pid=1210437)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/137d3681/checkpoint_000006)\n",
            "2025-03-12 23:45:38,944\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1210437)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/137d3681/checkpoint_000007)\n",
            "2025-03-12 23:45:39,986\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:45:41,035\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1210437)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/137d3681/checkpoint_000008)\n",
            "\u001b[36m(RayTrainWorker pid=1210437)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/137d3681/checkpoint_000009)\n",
            "2025-03-12 23:45:42,047\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1210437)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/137d3681/checkpoint_000010)\n",
            "2025-03-12 23:45:43,066\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:45:43,942 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.064 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:45:44,064\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1210437)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/137d3681/checkpoint_000011)\n",
            "\u001b[36m(RayTrainWorker pid=1210437)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/137d3681/checkpoint_000012)\n",
            "2025-03-12 23:45:45,248\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:45:46,365\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1210437)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/137d3681/checkpoint_000013)\n",
            "2025-03-12 23:45:47,535\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1210437)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/137d3681/checkpoint_000014)\n",
            "2025-03-12 23:45:48,700\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1210437)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/137d3681/checkpoint_000015)\n",
            "\u001b[36m(RayTrainWorker pid=1210437)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/137d3681/checkpoint_000016)\n",
            "2025-03-12 23:45:49,771\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:45:50,945\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1210437)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/137d3681/checkpoint_000017)\n",
            "2025-03-12 23:45:52,076\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1210437)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/137d3681/checkpoint_000018)\n",
            "2025-03-12 23:45:53,217\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1210437)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/137d3681/checkpoint_000019)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:45:53,951 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1662 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:45:54,291\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1210437)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/137d3681/checkpoint_000020)\n",
            "2025-03-12 23:45:55,427\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1210437)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/137d3681/checkpoint_000021)\n",
            "2025-03-12 23:45:56,561\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1210437)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/137d3681/checkpoint_000022)\n",
            "\u001b[36m(RayTrainWorker pid=1210437)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/137d3681/checkpoint_000023)\n",
            "2025-03-12 23:45:57,705\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:45:58,818\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1210437)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/137d3681/checkpoint_000024)\n",
            "\u001b[36m(RayTrainWorker pid=1210437)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/137d3681/checkpoint_000025)\n",
            "2025-03-12 23:45:59,967\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:46:01,056\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1210437)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/137d3681/checkpoint_000026)\n",
            "2025-03-12 23:46:02,222\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1210437)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/137d3681/checkpoint_000027)\n",
            "2025-03-12 23:46:03,455\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1210437)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/137d3681/checkpoint_000028)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:46:03,960 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1661 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:46:04,599\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1210437)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/137d3681/checkpoint_000029)\n",
            "2025-03-12 23:46:05,747\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1210437)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/137d3681/checkpoint_000030)\n",
            "2025-03-12 23:46:06,832\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1210437)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/137d3681/checkpoint_000031)\n",
            "\u001b[36m(RayTrainWorker pid=1210437)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/137d3681/checkpoint_000032)\n",
            "2025-03-12 23:46:07,945\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1210437)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/137d3681/checkpoint_000033)\n",
            "2025-03-12 23:46:09,071\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:46:10,162\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1210437)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/137d3681/checkpoint_000034)\n",
            "\u001b[36m(RayTrainWorker pid=1210437)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/137d3681/checkpoint_000035)\n",
            "2025-03-12 23:46:11,318\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:46:12,388\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1210437)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/137d3681/checkpoint_000036)\n",
            "2025-03-12 23:46:13,559\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1210437)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/137d3681/checkpoint_000037)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:46:13,969 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.166 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:46:14,700\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1210437)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/137d3681/checkpoint_000038)\n",
            "2025-03-12 23:46:15,832\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1210437)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/137d3681/checkpoint_000039)\n",
            "2025-03-12 23:46:16,959\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1210437)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/137d3681/checkpoint_000040)\n",
            "\u001b[36m(RayTrainWorker pid=1210437)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/137d3681/checkpoint_000041)\n",
            "2025-03-12 23:46:18,037\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1210437)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/137d3681/checkpoint_000042)\n",
            "2025-03-12 23:46:19,168\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1210437)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/137d3681/checkpoint_000043)\n",
            "2025-03-12 23:46:20,305\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1210437)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/137d3681/checkpoint_000044)\n",
            "2025-03-12 23:46:21,453\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1210437)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/137d3681/checkpoint_000045)\n",
            "2025-03-12 23:46:22,578\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1210437)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/137d3681/checkpoint_000046)\n",
            "2025-03-12 23:46:23,708\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:46:23,977 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.166 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:46:24,816\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1210437)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/137d3681/checkpoint_000047)\n",
            "2025-03-12 23:46:25,942\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1210437)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/137d3681/checkpoint_000048)\n",
            "2025-03-12 23:46:27,079\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1210437)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/137d3681/checkpoint_000049)\n",
            "\u001b[36m(RayTrainWorker pid=1210437)\u001b[0m `Trainer.fit` stopped: `max_epochs=50` reached.\n",
            "\u001b[36m(RayTrainWorker pid=1210437)\u001b[0m [rank0]:[W312 23:46:29.780851749 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:46:33,986 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.166 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(TorchTrainer pid=1211915)\u001b[0m Started distributed worker processes: \n",
            "\u001b[36m(TorchTrainer pid=1211915)\u001b[0m - (node_id=6cf2562c5de19587d3dc355f8ebead9f15bbc1bb9651b7563d063b82, ip=10.128.0.3, pid=1212072) world_rank=0, local_rank=0, node_rank=0\n",
            "\u001b[36m(RayTrainWorker pid=1212072)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
            "\u001b[36m(RayTrainWorker pid=1212072)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/ray/train/lightning/_lightning_utils.py:262: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
            "\u001b[36m(RayTrainWorker pid=1212072)\u001b[0m `get_trial_name` is deprecated because the concept of a `Trial` will soon be removed in Ray Train.Ray Train will no longer assume that it's running within a Ray Tune `Trial` in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
            "\u001b[36m(RayTrainWorker pid=1212072)\u001b[0m GPU available: True (cuda), used: True\n",
            "\u001b[36m(RayTrainWorker pid=1212072)\u001b[0m TPU available: False, using: 0 TPU cores\n",
            "\u001b[36m(RayTrainWorker pid=1212072)\u001b[0m HPU available: False, using: 0 HPUs\n",
            "\u001b[36m(RayTrainWorker pid=1212072)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(RayTrainWorker pid=1212072)\u001b[0m Getting checkpoint from /home/jupyter/AgenticADMET/notebooks/../output/artifacts/mol_mlm_roberta_zinc/last.ckpt...\n",
            "\u001b[36m(RayTrainWorker pid=1212072)\u001b[0m Loading checkpoint from roberta to roberta...\n",
            "\u001b[36m(RayTrainWorker pid=1212072)\u001b[0m <All keys matched successfully>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:46:43,994 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1613 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1212072)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\u001b[36m(RayTrainWorker pid=1212072)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=1212072)\u001b[0m   | Name      | Type         | Params | Mode \n",
            "\u001b[36m(RayTrainWorker pid=1212072)\u001b[0m ---------------------------------------------------\n",
            "\u001b[36m(RayTrainWorker pid=1212072)\u001b[0m 0 | roberta   | RobertaModel | 8.7 M  | train\n",
            "\u001b[36m(RayTrainWorker pid=1212072)\u001b[0m 1 | predictor | MLP          | 890 K  | train\n",
            "\u001b[36m(RayTrainWorker pid=1212072)\u001b[0m 2 | criterion | MSE          | 0      | train\n",
            "\u001b[36m(RayTrainWorker pid=1212072)\u001b[0m 3 | metrics   | ModuleList   | 0      | train\n",
            "\u001b[36m(RayTrainWorker pid=1212072)\u001b[0m ---------------------------------------------------\n",
            "\u001b[36m(RayTrainWorker pid=1212072)\u001b[0m 9.6 M     Trainable params\n",
            "\u001b[36m(RayTrainWorker pid=1212072)\u001b[0m 0         Non-trainable params\n",
            "\u001b[36m(RayTrainWorker pid=1212072)\u001b[0m 9.6 M     Total params\n",
            "\u001b[36m(RayTrainWorker pid=1212072)\u001b[0m 38.259    Total estimated model params size (MB)\n",
            "\u001b[36m(RayTrainWorker pid=1212072)\u001b[0m 132       Modules in train mode\n",
            "\u001b[36m(RayTrainWorker pid=1212072)\u001b[0m 0         Modules in eval mode\n",
            "\u001b[36m(RayTrainWorker pid=1212072)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
            "\u001b[36m(RayTrainWorker pid=1212072)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1212072)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
            "\u001b[36m(RayTrainWorker pid=1212072)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (11) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "\u001b[36m(RayTrainWorker pid=1212072)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('lr', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1212072)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('train_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1212072)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/70578d83/checkpoint_000000)\n",
            "2025-03-12 23:46:47,482\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1212072)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/70578d83/checkpoint_000001)\n",
            "\u001b[36m(RayTrainWorker pid=1212072)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/70578d83/checkpoint_000002)\n",
            "2025-03-12 23:46:48,655\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:46:49,839\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1212072)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/70578d83/checkpoint_000003)\n",
            "2025-03-12 23:46:51,000\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1212072)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/70578d83/checkpoint_000004)\n",
            "2025-03-12 23:46:52,136\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1212072)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/70578d83/checkpoint_000005)\n",
            "2025-03-12 23:46:53,342\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1212072)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/70578d83/checkpoint_000006)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:46:54,003 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1658 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1212072)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/70578d83/checkpoint_000007)\n",
            "2025-03-12 23:46:54,518\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:46:55,719\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1212072)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/70578d83/checkpoint_000008)\n",
            "2025-03-12 23:46:56,946\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1212072)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/70578d83/checkpoint_000009)\n",
            "\u001b[36m(RayTrainWorker pid=1212072)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/70578d83/checkpoint_000010)\n",
            "2025-03-12 23:46:58,148\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:46:59,307\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1212072)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/70578d83/checkpoint_000011)\n",
            "2025-03-12 23:47:00,485\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1212072)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/70578d83/checkpoint_000012)\n",
            "\u001b[36m(RayTrainWorker pid=1212072)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/70578d83/checkpoint_000013)\n",
            "2025-03-12 23:47:01,682\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:47:02,821\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1212072)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/70578d83/checkpoint_000014)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:47:04,012 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.0586 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1212072)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/70578d83/checkpoint_000015)\n",
            "2025-03-12 23:47:04,042\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1212072)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/70578d83/checkpoint_000016)\n",
            "2025-03-12 23:47:05,272\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:47:06,455\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1212072)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/70578d83/checkpoint_000017)\n",
            "\u001b[36m(RayTrainWorker pid=1212072)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/70578d83/checkpoint_000018)\n",
            "2025-03-12 23:47:07,660\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:47:08,873\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1212072)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/70578d83/checkpoint_000019)\n",
            "2025-03-12 23:47:10,063\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1212072)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/70578d83/checkpoint_000020)\n",
            "2025-03-12 23:47:11,227\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1212072)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/70578d83/checkpoint_000021)\n",
            "2025-03-12 23:47:12,345\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1212072)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/70578d83/checkpoint_000022)\n",
            "2025-03-12 23:47:13,462\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1212072)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/70578d83/checkpoint_000023)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:47:14,021 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1656 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:47:14,612\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1212072)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/70578d83/checkpoint_000024)\n",
            "\u001b[36m(RayTrainWorker pid=1212072)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/70578d83/checkpoint_000025)\n",
            "2025-03-12 23:47:15,807\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1212072)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/70578d83/checkpoint_000026)\n",
            "2025-03-12 23:47:17,034\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:47:18,227\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1212072)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/70578d83/checkpoint_000027)\n",
            "2025-03-12 23:47:19,463\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1212072)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/70578d83/checkpoint_000028)\n",
            "2025-03-12 23:47:20,608\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1212072)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/70578d83/checkpoint_000029)\n",
            "2025-03-12 23:47:21,820\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1212072)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/70578d83/checkpoint_000030)\n",
            "\u001b[36m(RayTrainWorker pid=1212072)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/70578d83/checkpoint_000031)\n",
            "2025-03-12 23:47:23,048\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:47:24,030 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.0586 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:47:24,205\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1212072)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/70578d83/checkpoint_000032)\n",
            "2025-03-12 23:47:25,427\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1212072)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/70578d83/checkpoint_000033)\n",
            "\u001b[36m(RayTrainWorker pid=1212072)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/70578d83/checkpoint_000034)\n",
            "2025-03-12 23:47:26,627\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:47:27,780\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1212072)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/70578d83/checkpoint_000035)\n",
            "\u001b[36m(RayTrainWorker pid=1212072)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/70578d83/checkpoint_000036)\n",
            "2025-03-12 23:47:29,005\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:47:30,172\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1212072)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/70578d83/checkpoint_000037)\n",
            "2025-03-12 23:47:31,351\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1212072)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/70578d83/checkpoint_000038)\n",
            "\u001b[36m(RayTrainWorker pid=1212072)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/70578d83/checkpoint_000039)\n",
            "2025-03-12 23:47:32,533\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1212072)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/70578d83/checkpoint_000040)\n",
            "2025-03-12 23:47:33,758\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:47:34,039 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1656 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:47:34,941\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1212072)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/70578d83/checkpoint_000041)\n",
            "2025-03-12 23:47:36,089\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1212072)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/70578d83/checkpoint_000042)\n",
            "2025-03-12 23:47:37,279\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1212072)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/70578d83/checkpoint_000043)\n",
            "2025-03-12 23:47:38,468\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1212072)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/70578d83/checkpoint_000044)\n",
            "2025-03-12 23:47:39,651\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1212072)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/70578d83/checkpoint_000045)\n",
            "\u001b[36m(RayTrainWorker pid=1212072)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/70578d83/checkpoint_000046)\n",
            "2025-03-12 23:47:40,864\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:47:42,048\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1212072)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/70578d83/checkpoint_000047)\n",
            "\u001b[36m(RayTrainWorker pid=1212072)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/70578d83/checkpoint_000048)\n",
            "2025-03-12 23:47:43,207\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:47:44,048 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1656 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:47:44,438\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1212072)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/70578d83/checkpoint_000049)\n",
            "\u001b[36m(RayTrainWorker pid=1212072)\u001b[0m `Trainer.fit` stopped: `max_epochs=50` reached.\n",
            "\u001b[36m(RayTrainWorker pid=1212072)\u001b[0m [rank0]:[W312 23:47:46.138221714 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:47:54,056 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1655 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(TorchTrainer pid=1213545)\u001b[0m Started distributed worker processes: \n",
            "\u001b[36m(TorchTrainer pid=1213545)\u001b[0m - (node_id=6cf2562c5de19587d3dc355f8ebead9f15bbc1bb9651b7563d063b82, ip=10.128.0.3, pid=1213698) world_rank=0, local_rank=0, node_rank=0\n",
            "\u001b[36m(RayTrainWorker pid=1213698)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(RayTrainWorker pid=1213698)\u001b[0m Getting checkpoint from /home/jupyter/AgenticADMET/notebooks/../output/artifacts/mol_mlm_roberta_zinc/last.ckpt...\n",
            "\u001b[36m(RayTrainWorker pid=1213698)\u001b[0m Loading checkpoint from roberta to roberta...\n",
            "\u001b[36m(RayTrainWorker pid=1213698)\u001b[0m <All keys matched successfully>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(RayTrainWorker pid=1213698)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/ray/train/lightning/_lightning_utils.py:262: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
            "\u001b[36m(RayTrainWorker pid=1213698)\u001b[0m `get_trial_name` is deprecated because the concept of a `Trial` will soon be removed in Ray Train.Ray Train will no longer assume that it's running within a Ray Tune `Trial` in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
            "\u001b[36m(RayTrainWorker pid=1213698)\u001b[0m GPU available: True (cuda), used: True\n",
            "\u001b[36m(RayTrainWorker pid=1213698)\u001b[0m TPU available: False, using: 0 TPU cores\n",
            "\u001b[36m(RayTrainWorker pid=1213698)\u001b[0m HPU available: False, using: 0 HPUs\n",
            "\u001b[36m(RayTrainWorker pid=1213698)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
            "\u001b[36m(RayTrainWorker pid=1213698)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\u001b[36m(RayTrainWorker pid=1213698)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=1213698)\u001b[0m   | Name      | Type         | Params | Mode \n",
            "\u001b[36m(RayTrainWorker pid=1213698)\u001b[0m ---------------------------------------------------\n",
            "\u001b[36m(RayTrainWorker pid=1213698)\u001b[0m 0 | roberta   | RobertaModel | 8.7 M  | train\n",
            "\u001b[36m(RayTrainWorker pid=1213698)\u001b[0m 1 | predictor | MLP          | 297 K  | train\n",
            "\u001b[36m(RayTrainWorker pid=1213698)\u001b[0m 2 | criterion | MSE          | 0      | train\n",
            "\u001b[36m(RayTrainWorker pid=1213698)\u001b[0m 3 | metrics   | ModuleList   | 0      | train\n",
            "\u001b[36m(RayTrainWorker pid=1213698)\u001b[0m ---------------------------------------------------\n",
            "\u001b[36m(RayTrainWorker pid=1213698)\u001b[0m 9.0 M     Trainable params\n",
            "\u001b[36m(RayTrainWorker pid=1213698)\u001b[0m 0         Non-trainable params\n",
            "\u001b[36m(RayTrainWorker pid=1213698)\u001b[0m 9.0 M     Total params\n",
            "\u001b[36m(RayTrainWorker pid=1213698)\u001b[0m 35.889    Total estimated model params size (MB)\n",
            "\u001b[36m(RayTrainWorker pid=1213698)\u001b[0m 132       Modules in train mode\n",
            "\u001b[36m(RayTrainWorker pid=1213698)\u001b[0m 0         Modules in eval mode\n",
            "\u001b[36m(RayTrainWorker pid=1213698)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
            "\u001b[36m(RayTrainWorker pid=1213698)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1213698)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
            "\u001b[36m(RayTrainWorker pid=1213698)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (21) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "\u001b[36m(RayTrainWorker pid=1213698)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('lr', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1213698)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('train_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1213698)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5834913a/checkpoint_000000)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:48:04,064 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1653 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:48:04,936\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1213698)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5834913a/checkpoint_000001)\n",
            "\u001b[36m(RayTrainWorker pid=1213698)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5834913a/checkpoint_000002)\n",
            "2025-03-12 23:48:06,209\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:48:07,490\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1213698)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5834913a/checkpoint_000003)\n",
            "\u001b[36m(RayTrainWorker pid=1213698)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5834913a/checkpoint_000004)\n",
            "2025-03-12 23:48:08,794\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1213698)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5834913a/checkpoint_000005)\n",
            "2025-03-12 23:48:10,053\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:48:11,306\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1213698)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5834913a/checkpoint_000006)\n",
            "\u001b[36m(RayTrainWorker pid=1213698)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5834913a/checkpoint_000007)\n",
            "2025-03-12 23:48:12,625\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:48:13,854\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1213698)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5834913a/checkpoint_000008)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:48:14,073 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1652 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:48:15,196\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1213698)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5834913a/checkpoint_000009)\n",
            "2025-03-12 23:48:16,389\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1213698)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5834913a/checkpoint_000010)\n",
            "2025-03-12 23:48:17,649\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1213698)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5834913a/checkpoint_000011)\n",
            "2025-03-12 23:48:18,914\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1213698)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5834913a/checkpoint_000012)\n",
            "\u001b[36m(RayTrainWorker pid=1213698)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5834913a/checkpoint_000013)\n",
            "2025-03-12 23:48:20,171\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:48:21,458\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1213698)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5834913a/checkpoint_000014)\n",
            "2025-03-12 23:48:22,730\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1213698)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5834913a/checkpoint_000015)\n",
            "2025-03-12 23:48:24,028\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:48:24,081 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1652 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1213698)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5834913a/checkpoint_000016)\n",
            "2025-03-12 23:48:25,313\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1213698)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5834913a/checkpoint_000017)\n",
            "2025-03-12 23:48:26,615\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1213698)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5834913a/checkpoint_000018)\n",
            "\u001b[36m(RayTrainWorker pid=1213698)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5834913a/checkpoint_000019)\n",
            "2025-03-12 23:48:27,914\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:48:29,187\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1213698)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5834913a/checkpoint_000020)\n",
            "2025-03-12 23:48:30,481\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1213698)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5834913a/checkpoint_000021)\n",
            "\u001b[36m(RayTrainWorker pid=1213698)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5834913a/checkpoint_000022)\n",
            "2025-03-12 23:48:31,734\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:48:33,024\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1213698)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5834913a/checkpoint_000023)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:48:34,090 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.0648 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:48:34,277\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1213698)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5834913a/checkpoint_000024)\n",
            "2025-03-12 23:48:35,578\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1213698)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5834913a/checkpoint_000025)\n",
            "2025-03-12 23:48:36,840\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1213698)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5834913a/checkpoint_000026)\n",
            "2025-03-12 23:48:38,094\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1213698)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5834913a/checkpoint_000027)\n",
            "\u001b[36m(RayTrainWorker pid=1213698)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5834913a/checkpoint_000028)\n",
            "2025-03-12 23:48:39,376\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:48:40,579\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1213698)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5834913a/checkpoint_000029)\n",
            "\u001b[36m(RayTrainWorker pid=1213698)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5834913a/checkpoint_000030)\n",
            "2025-03-12 23:48:41,841\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:48:43,119\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1213698)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5834913a/checkpoint_000031)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:48:44,099 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1652 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1213698)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5834913a/checkpoint_000032)\n",
            "2025-03-12 23:48:44,457\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1213698)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5834913a/checkpoint_000033)\n",
            "2025-03-12 23:48:45,768\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1213698)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5834913a/checkpoint_000034)\n",
            "2025-03-12 23:48:47,007\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:48:48,287\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1213698)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5834913a/checkpoint_000035)\n",
            "\u001b[36m(RayTrainWorker pid=1213698)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5834913a/checkpoint_000036)\n",
            "2025-03-12 23:48:49,591\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:48:50,890\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1213698)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5834913a/checkpoint_000037)\n",
            "2025-03-12 23:48:52,150\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1213698)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5834913a/checkpoint_000038)\n",
            "\u001b[36m(RayTrainWorker pid=1213698)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5834913a/checkpoint_000039)\n",
            "2025-03-12 23:48:53,408\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:48:54,108 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1651 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1213698)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5834913a/checkpoint_000040)\n",
            "2025-03-12 23:48:54,676\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1213698)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5834913a/checkpoint_000041)\n",
            "2025-03-12 23:48:55,907\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:48:57,166\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1213698)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5834913a/checkpoint_000042)\n",
            "2025-03-12 23:48:58,432\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1213698)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5834913a/checkpoint_000043)\n",
            "2025-03-12 23:48:59,690\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1213698)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5834913a/checkpoint_000044)\n",
            "2025-03-12 23:49:01,051\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1213698)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5834913a/checkpoint_000045)\n",
            "2025-03-12 23:49:02,370\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1213698)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5834913a/checkpoint_000046)\n",
            "\u001b[36m(RayTrainWorker pid=1213698)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5834913a/checkpoint_000047)\n",
            "2025-03-12 23:49:03,644\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:49:04,118 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.165 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:49:04,979\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1213698)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5834913a/checkpoint_000048)\n",
            "\u001b[36m(RayTrainWorker pid=1213698)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5834913a/checkpoint_000049)\n",
            "2025-03-12 23:49:06,248\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1213698)\u001b[0m `Trainer.fit` stopped: `max_epochs=50` reached.\n",
            "\u001b[36m(RayTrainWorker pid=1213698)\u001b[0m [rank0]:[W312 23:49:08.865297819 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:49:14,126 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1649 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(TorchTrainer pid=1215256)\u001b[0m Started distributed worker processes: \n",
            "\u001b[36m(TorchTrainer pid=1215256)\u001b[0m - (node_id=6cf2562c5de19587d3dc355f8ebead9f15bbc1bb9651b7563d063b82, ip=10.128.0.3, pid=1215479) world_rank=0, local_rank=0, node_rank=0\n",
            "\u001b[36m(RayTrainWorker pid=1215479)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
            "\u001b[36m(RayTrainWorker pid=1215479)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/ray/train/lightning/_lightning_utils.py:262: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
            "\u001b[36m(RayTrainWorker pid=1215479)\u001b[0m `get_trial_name` is deprecated because the concept of a `Trial` will soon be removed in Ray Train.Ray Train will no longer assume that it's running within a Ray Tune `Trial` in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
            "\u001b[36m(RayTrainWorker pid=1215479)\u001b[0m GPU available: True (cuda), used: True\n",
            "\u001b[36m(RayTrainWorker pid=1215479)\u001b[0m TPU available: False, using: 0 TPU cores\n",
            "\u001b[36m(RayTrainWorker pid=1215479)\u001b[0m HPU available: False, using: 0 HPUs\n",
            "\u001b[36m(RayTrainWorker pid=1215479)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(RayTrainWorker pid=1215479)\u001b[0m Getting checkpoint from /home/jupyter/AgenticADMET/notebooks/../output/artifacts/mol_mlm_roberta_zinc/last.ckpt...\n",
            "\u001b[36m(RayTrainWorker pid=1215479)\u001b[0m Loading checkpoint from roberta to roberta...\n",
            "\u001b[36m(RayTrainWorker pid=1215479)\u001b[0m <All keys matched successfully>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(RayTrainWorker pid=1215479)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\u001b[36m(RayTrainWorker pid=1215479)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=1215479)\u001b[0m   | Name      | Type         | Params | Mode \n",
            "\u001b[36m(RayTrainWorker pid=1215479)\u001b[0m ---------------------------------------------------\n",
            "\u001b[36m(RayTrainWorker pid=1215479)\u001b[0m 0 | roberta   | RobertaModel | 8.7 M  | train\n",
            "\u001b[36m(RayTrainWorker pid=1215479)\u001b[0m 1 | predictor | MLP          | 659 K  | train\n",
            "\u001b[36m(RayTrainWorker pid=1215479)\u001b[0m 2 | criterion | MSE          | 0      | train\n",
            "\u001b[36m(RayTrainWorker pid=1215479)\u001b[0m 3 | metrics   | ModuleList   | 0      | train\n",
            "\u001b[36m(RayTrainWorker pid=1215479)\u001b[0m ---------------------------------------------------\n",
            "\u001b[36m(RayTrainWorker pid=1215479)\u001b[0m 9.3 M     Trainable params\n",
            "\u001b[36m(RayTrainWorker pid=1215479)\u001b[0m 0         Non-trainable params\n",
            "\u001b[36m(RayTrainWorker pid=1215479)\u001b[0m 9.3 M     Total params\n",
            "\u001b[36m(RayTrainWorker pid=1215479)\u001b[0m 37.338    Total estimated model params size (MB)\n",
            "\u001b[36m(RayTrainWorker pid=1215479)\u001b[0m 132       Modules in train mode\n",
            "\u001b[36m(RayTrainWorker pid=1215479)\u001b[0m 0         Modules in eval mode\n",
            "\u001b[36m(RayTrainWorker pid=1215479)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
            "\u001b[36m(RayTrainWorker pid=1215479)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1215479)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
            "\u001b[36m(RayTrainWorker pid=1215479)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:49:24,135 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1648 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1215479)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('lr', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1215479)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('train_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1215479)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/ede8484b/checkpoint_000000)\n",
            "2025-03-12 23:49:26,267\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1215479)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/ede8484b/checkpoint_000001)\n",
            "2025-03-12 23:49:27,417\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1215479)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/ede8484b/checkpoint_000002)\n",
            "2025-03-12 23:49:28,546\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1215479)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/ede8484b/checkpoint_000003)\n",
            "2025-03-12 23:49:29,700\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1215479)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/ede8484b/checkpoint_000004)\n",
            "2025-03-12 23:49:30,822\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1215479)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/ede8484b/checkpoint_000005)\n",
            "2025-03-12 23:49:31,942\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1215479)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/ede8484b/checkpoint_000006)\n",
            "2025-03-12 23:49:33,047\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1215479)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/ede8484b/checkpoint_000007)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:49:34,143 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.0603 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1215479)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/ede8484b/checkpoint_000008)\n",
            "2025-03-12 23:49:34,206\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:49:35,289\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1215479)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/ede8484b/checkpoint_000009)\n",
            "\u001b[36m(RayTrainWorker pid=1215479)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/ede8484b/checkpoint_000010)\n",
            "2025-03-12 23:49:36,476\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:49:37,560\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1215479)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/ede8484b/checkpoint_000011)\n",
            "2025-03-12 23:49:38,737\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1215479)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/ede8484b/checkpoint_000012)\n",
            "2025-03-12 23:49:39,828\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1215479)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/ede8484b/checkpoint_000013)\n",
            "2025-03-12 23:49:41,007\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1215479)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/ede8484b/checkpoint_000014)\n",
            "\u001b[36m(RayTrainWorker pid=1215479)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/ede8484b/checkpoint_000015)\n",
            "2025-03-12 23:49:42,170\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:49:43,369\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1215479)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/ede8484b/checkpoint_000016)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:49:44,152 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1648 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:49:44,537\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1215479)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/ede8484b/checkpoint_000017)\n",
            "2025-03-12 23:49:45,742\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1215479)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/ede8484b/checkpoint_000018)\n",
            "\u001b[36m(RayTrainWorker pid=1215479)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/ede8484b/checkpoint_000019)\n",
            "2025-03-12 23:49:46,935\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1215479)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/ede8484b/checkpoint_000020)\n",
            "2025-03-12 23:49:48,079\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:49:49,293\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1215479)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/ede8484b/checkpoint_000021)\n",
            "\u001b[36m(RayTrainWorker pid=1215479)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/ede8484b/checkpoint_000022)\n",
            "2025-03-12 23:49:50,498\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:49:51,607\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1215479)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/ede8484b/checkpoint_000023)\n",
            "2025-03-12 23:49:52,786\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1215479)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/ede8484b/checkpoint_000024)\n",
            "2025-03-12 23:49:53,859\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1215479)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/ede8484b/checkpoint_000025)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:49:54,161 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1647 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1215479)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/ede8484b/checkpoint_000026)\n",
            "2025-03-12 23:49:55,019\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1215479)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/ede8484b/checkpoint_000027)\n",
            "2025-03-12 23:49:56,173\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:49:57,374\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1215479)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/ede8484b/checkpoint_000028)\n",
            "\u001b[36m(RayTrainWorker pid=1215479)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/ede8484b/checkpoint_000029)\n",
            "2025-03-12 23:49:58,538\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:49:59,706\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1215479)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/ede8484b/checkpoint_000030)\n",
            "2025-03-12 23:50:00,879\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1215479)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/ede8484b/checkpoint_000031)\n",
            "\u001b[36m(RayTrainWorker pid=1215479)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/ede8484b/checkpoint_000032)\n",
            "2025-03-12 23:50:02,074\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1215479)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/ede8484b/checkpoint_000033)\n",
            "2025-03-12 23:50:03,212\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:50:04,171 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.0601 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1215479)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/ede8484b/checkpoint_000034)\n",
            "2025-03-12 23:50:04,336\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:50:05,493\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1215479)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/ede8484b/checkpoint_000035)\n",
            "2025-03-12 23:50:06,592\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1215479)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/ede8484b/checkpoint_000036)\n",
            "2025-03-12 23:50:07,733\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1215479)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/ede8484b/checkpoint_000037)\n",
            "2025-03-12 23:50:08,873\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1215479)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/ede8484b/checkpoint_000038)\n",
            "\u001b[36m(RayTrainWorker pid=1215479)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/ede8484b/checkpoint_000039)\n",
            "2025-03-12 23:50:09,921\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:50:11,049\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1215479)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/ede8484b/checkpoint_000040)\n",
            "2025-03-12 23:50:12,144\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1215479)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/ede8484b/checkpoint_000041)\n",
            "2025-03-12 23:50:13,272\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1215479)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/ede8484b/checkpoint_000042)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:50:14,180 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.0656 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1215479)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/ede8484b/checkpoint_000043)\n",
            "2025-03-12 23:50:14,394\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:50:15,560\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1215479)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/ede8484b/checkpoint_000044)\n",
            "2025-03-12 23:50:16,722\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1215479)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/ede8484b/checkpoint_000045)\n",
            "2025-03-12 23:50:17,854\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1215479)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/ede8484b/checkpoint_000046)\n",
            "2025-03-12 23:50:18,972\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1215479)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/ede8484b/checkpoint_000047)\n",
            "2025-03-12 23:50:20,103\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1215479)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/ede8484b/checkpoint_000048)\n",
            "\u001b[36m(RayTrainWorker pid=1215479)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/ede8484b/checkpoint_000049)\n",
            "\u001b[36m(RayTrainWorker pid=1215479)\u001b[0m `Trainer.fit` stopped: `max_epochs=50` reached.\n",
            "2025-03-12 23:50:21,287\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1215479)\u001b[0m [rank0]:[W312 23:50:23.917223583 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:50:24,190 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1645 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:50:34,199 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1643 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(TorchTrainer pid=1216885)\u001b[0m Started distributed worker processes: \n",
            "\u001b[36m(TorchTrainer pid=1216885)\u001b[0m - (node_id=6cf2562c5de19587d3dc355f8ebead9f15bbc1bb9651b7563d063b82, ip=10.128.0.3, pid=1217098) world_rank=0, local_rank=0, node_rank=0\n",
            "\u001b[36m(RayTrainWorker pid=1217098)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(RayTrainWorker pid=1217098)\u001b[0m Loading checkpoint from roberta to roberta...\n",
            "\u001b[36m(RayTrainWorker pid=1217098)\u001b[0m <All keys matched successfully>\n",
            "\u001b[36m(RayTrainWorker pid=1217098)\u001b[0m Getting checkpoint from /home/jupyter/AgenticADMET/notebooks/../output/artifacts/mol_mlm_roberta_zinc/last.ckpt...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(RayTrainWorker pid=1217098)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/ray/train/lightning/_lightning_utils.py:262: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
            "\u001b[36m(RayTrainWorker pid=1217098)\u001b[0m `get_trial_name` is deprecated because the concept of a `Trial` will soon be removed in Ray Train.Ray Train will no longer assume that it's running within a Ray Tune `Trial` in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
            "\u001b[36m(RayTrainWorker pid=1217098)\u001b[0m GPU available: True (cuda), used: True\n",
            "\u001b[36m(RayTrainWorker pid=1217098)\u001b[0m TPU available: False, using: 0 TPU cores\n",
            "\u001b[36m(RayTrainWorker pid=1217098)\u001b[0m HPU available: False, using: 0 HPUs\n",
            "\u001b[36m(RayTrainWorker pid=1217098)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
            "\u001b[36m(RayTrainWorker pid=1217098)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\u001b[36m(RayTrainWorker pid=1217098)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=1217098)\u001b[0m   | Name      | Type         | Params | Mode \n",
            "\u001b[36m(RayTrainWorker pid=1217098)\u001b[0m ---------------------------------------------------\n",
            "\u001b[36m(RayTrainWorker pid=1217098)\u001b[0m 0 | roberta   | RobertaModel | 8.7 M  | train\n",
            "\u001b[36m(RayTrainWorker pid=1217098)\u001b[0m 1 | predictor | MLP          | 399 K  | train\n",
            "\u001b[36m(RayTrainWorker pid=1217098)\u001b[0m 2 | criterion | MSE          | 0      | train\n",
            "\u001b[36m(RayTrainWorker pid=1217098)\u001b[0m 3 | metrics   | ModuleList   | 0      | train\n",
            "\u001b[36m(RayTrainWorker pid=1217098)\u001b[0m ---------------------------------------------------\n",
            "\u001b[36m(RayTrainWorker pid=1217098)\u001b[0m 9.1 M     Trainable params\n",
            "\u001b[36m(RayTrainWorker pid=1217098)\u001b[0m 0         Non-trainable params\n",
            "\u001b[36m(RayTrainWorker pid=1217098)\u001b[0m 9.1 M     Total params\n",
            "\u001b[36m(RayTrainWorker pid=1217098)\u001b[0m 36.296    Total estimated model params size (MB)\n",
            "\u001b[36m(RayTrainWorker pid=1217098)\u001b[0m 128       Modules in train mode\n",
            "\u001b[36m(RayTrainWorker pid=1217098)\u001b[0m 0         Modules in eval mode\n",
            "\u001b[36m(RayTrainWorker pid=1217098)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
            "\u001b[36m(RayTrainWorker pid=1217098)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1217098)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
            "\u001b[36m(RayTrainWorker pid=1217098)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (11) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "\u001b[36m(RayTrainWorker pid=1217098)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('lr', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1217098)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('train_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1217098)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/3cee4eec/checkpoint_000000)\n",
            "2025-03-12 23:50:41,602\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1217098)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/3cee4eec/checkpoint_000001)\n",
            "2025-03-12 23:50:42,772\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1217098)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/3cee4eec/checkpoint_000002)\n",
            "2025-03-12 23:50:43,948\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1217098)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/3cee4eec/checkpoint_000003)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:50:44,208 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1644 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:50:45,162\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1217098)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/3cee4eec/checkpoint_000004)\n",
            "2025-03-12 23:50:46,279\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1217098)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/3cee4eec/checkpoint_000005)\n",
            "2025-03-12 23:50:47,433\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1217098)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/3cee4eec/checkpoint_000006)\n",
            "\u001b[36m(RayTrainWorker pid=1217098)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/3cee4eec/checkpoint_000007)\n",
            "2025-03-12 23:50:48,624\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:50:49,762\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1217098)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/3cee4eec/checkpoint_000008)\n",
            "2025-03-12 23:50:50,891\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1217098)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/3cee4eec/checkpoint_000009)\n",
            "2025-03-12 23:50:52,073\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1217098)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/3cee4eec/checkpoint_000010)\n",
            "\u001b[36m(RayTrainWorker pid=1217098)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/3cee4eec/checkpoint_000011)\n",
            "2025-03-12 23:50:53,251\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:50:54,217 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.0628 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:50:54,441\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1217098)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/3cee4eec/checkpoint_000012)\n",
            "2025-03-12 23:50:55,571\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1217098)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/3cee4eec/checkpoint_000013)\n",
            "\u001b[36m(RayTrainWorker pid=1217098)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/3cee4eec/checkpoint_000014)\n",
            "2025-03-12 23:50:56,679\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:50:57,853\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1217098)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/3cee4eec/checkpoint_000015)\n",
            "2025-03-12 23:50:59,024\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1217098)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/3cee4eec/checkpoint_000016)\n",
            "\u001b[36m(RayTrainWorker pid=1217098)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/3cee4eec/checkpoint_000017)\n",
            "2025-03-12 23:51:00,206\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:51:01,400\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1217098)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/3cee4eec/checkpoint_000018)\n",
            "2025-03-12 23:51:02,562\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1217098)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/3cee4eec/checkpoint_000019)\n",
            "\u001b[36m(RayTrainWorker pid=1217098)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/3cee4eec/checkpoint_000020)\n",
            "2025-03-12 23:51:03,731\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:51:04,227 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1642 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1217098)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/3cee4eec/checkpoint_000021)\n",
            "2025-03-12 23:51:04,888\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1217098)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/3cee4eec/checkpoint_000022)\n",
            "2025-03-12 23:51:06,034\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:51:07,197\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1217098)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/3cee4eec/checkpoint_000023)\n",
            "2025-03-12 23:51:08,348\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1217098)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/3cee4eec/checkpoint_000024)\n",
            "2025-03-12 23:51:09,510\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1217098)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/3cee4eec/checkpoint_000025)\n",
            "\u001b[36m(RayTrainWorker pid=1217098)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/3cee4eec/checkpoint_000026)\n",
            "2025-03-12 23:51:10,704\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1217098)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/3cee4eec/checkpoint_000027)\n",
            "2025-03-12 23:51:11,819\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:51:13,036\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1217098)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/3cee4eec/checkpoint_000028)\n",
            "2025-03-12 23:51:14,235\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:51:14,236 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1642 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1217098)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/3cee4eec/checkpoint_000029)\n",
            "2025-03-12 23:51:15,374\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1217098)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/3cee4eec/checkpoint_000030)\n",
            "\u001b[36m(RayTrainWorker pid=1217098)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/3cee4eec/checkpoint_000031)\n",
            "2025-03-12 23:51:16,566\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1217098)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/3cee4eec/checkpoint_000032)\n",
            "2025-03-12 23:51:17,722\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1217098)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/3cee4eec/checkpoint_000033)\n",
            "2025-03-12 23:51:18,837\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:51:19,967\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1217098)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/3cee4eec/checkpoint_000034)\n",
            "\u001b[36m(RayTrainWorker pid=1217098)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/3cee4eec/checkpoint_000035)\n",
            "2025-03-12 23:51:21,140\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:51:22,321\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1217098)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/3cee4eec/checkpoint_000036)\n",
            "\u001b[36m(RayTrainWorker pid=1217098)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/3cee4eec/checkpoint_000037)\n",
            "2025-03-12 23:51:23,506\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:51:24,245 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1641 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:51:24,702\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1217098)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/3cee4eec/checkpoint_000038)\n",
            "2025-03-12 23:51:25,851\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1217098)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/3cee4eec/checkpoint_000039)\n",
            "2025-03-12 23:51:27,075\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1217098)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/3cee4eec/checkpoint_000040)\n",
            "\u001b[36m(RayTrainWorker pid=1217098)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/3cee4eec/checkpoint_000041)\n",
            "2025-03-12 23:51:28,247\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1217098)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/3cee4eec/checkpoint_000042)\n",
            "2025-03-12 23:51:29,414\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:51:30,595\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1217098)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/3cee4eec/checkpoint_000043)\n",
            "2025-03-12 23:51:31,903\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1217098)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/3cee4eec/checkpoint_000044)\n",
            "2025-03-12 23:51:33,092\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1217098)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/3cee4eec/checkpoint_000045)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:51:34,254 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.0625 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:51:34,319\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1217098)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/3cee4eec/checkpoint_000046)\n",
            "2025-03-12 23:51:35,539\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1217098)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/3cee4eec/checkpoint_000047)\n",
            "2025-03-12 23:51:36,780\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1217098)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/3cee4eec/checkpoint_000048)\n",
            "\u001b[36m(RayTrainWorker pid=1217098)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/3cee4eec/checkpoint_000049)\n",
            "2025-03-12 23:51:37,911\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1217098)\u001b[0m `Trainer.fit` stopped: `max_epochs=50` reached.\n",
            "\u001b[36m(RayTrainWorker pid=1217098)\u001b[0m [rank0]:[W312 23:51:39.558692253 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:51:44,264 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.164 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(TorchTrainer pid=1218490)\u001b[0m Started distributed worker processes: \n",
            "\u001b[36m(TorchTrainer pid=1218490)\u001b[0m - (node_id=6cf2562c5de19587d3dc355f8ebead9f15bbc1bb9651b7563d063b82, ip=10.128.0.3, pid=1218729) world_rank=0, local_rank=0, node_rank=0\n",
            "\u001b[36m(RayTrainWorker pid=1218729)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(RayTrainWorker pid=1218729)\u001b[0m Getting checkpoint from /home/jupyter/AgenticADMET/notebooks/../output/artifacts/mol_mlm_roberta_zinc/last.ckpt...\n",
            "\u001b[36m(RayTrainWorker pid=1218729)\u001b[0m Loading checkpoint from roberta to roberta...\n",
            "\u001b[36m(RayTrainWorker pid=1218729)\u001b[0m <All keys matched successfully>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(RayTrainWorker pid=1218729)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/ray/train/lightning/_lightning_utils.py:262: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
            "\u001b[36m(RayTrainWorker pid=1218729)\u001b[0m `get_trial_name` is deprecated because the concept of a `Trial` will soon be removed in Ray Train.Ray Train will no longer assume that it's running within a Ray Tune `Trial` in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:51:54,274 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1639 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1218729)\u001b[0m GPU available: True (cuda), used: True\n",
            "\u001b[36m(RayTrainWorker pid=1218729)\u001b[0m TPU available: False, using: 0 TPU cores\n",
            "\u001b[36m(RayTrainWorker pid=1218729)\u001b[0m HPU available: False, using: 0 HPUs\n",
            "\u001b[36m(RayTrainWorker pid=1218729)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
            "\u001b[36m(RayTrainWorker pid=1218729)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\u001b[36m(RayTrainWorker pid=1218729)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=1218729)\u001b[0m   | Name      | Type         | Params | Mode \n",
            "\u001b[36m(RayTrainWorker pid=1218729)\u001b[0m ---------------------------------------------------\n",
            "\u001b[36m(RayTrainWorker pid=1218729)\u001b[0m 0 | roberta   | RobertaModel | 8.7 M  | train\n",
            "\u001b[36m(RayTrainWorker pid=1218729)\u001b[0m 1 | predictor | MLP          | 299 K  | train\n",
            "\u001b[36m(RayTrainWorker pid=1218729)\u001b[0m 2 | criterion | MSE          | 0      | train\n",
            "\u001b[36m(RayTrainWorker pid=1218729)\u001b[0m 3 | metrics   | ModuleList   | 0      | train\n",
            "\u001b[36m(RayTrainWorker pid=1218729)\u001b[0m ---------------------------------------------------\n",
            "\u001b[36m(RayTrainWorker pid=1218729)\u001b[0m 9.0 M     Trainable params\n",
            "\u001b[36m(RayTrainWorker pid=1218729)\u001b[0m 0         Non-trainable params\n",
            "\u001b[36m(RayTrainWorker pid=1218729)\u001b[0m 9.0 M     Total params\n",
            "\u001b[36m(RayTrainWorker pid=1218729)\u001b[0m 35.896    Total estimated model params size (MB)\n",
            "\u001b[36m(RayTrainWorker pid=1218729)\u001b[0m 128       Modules in train mode\n",
            "\u001b[36m(RayTrainWorker pid=1218729)\u001b[0m 0         Modules in eval mode\n",
            "\u001b[36m(RayTrainWorker pid=1218729)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
            "\u001b[36m(RayTrainWorker pid=1218729)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1218729)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
            "\u001b[36m(RayTrainWorker pid=1218729)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (21) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "\u001b[36m(RayTrainWorker pid=1218729)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('lr', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1218729)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('train_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1218729)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/1bb12d79/checkpoint_000000)\n",
            "\u001b[36m(RayTrainWorker pid=1218729)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/1bb12d79/checkpoint_000001)\n",
            "2025-03-12 23:51:58,063\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:51:59,336\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1218729)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/1bb12d79/checkpoint_000002)\n",
            "\u001b[36m(RayTrainWorker pid=1218729)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/1bb12d79/checkpoint_000003)\n",
            "2025-03-12 23:52:00,561\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1218729)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/1bb12d79/checkpoint_000004)\n",
            "2025-03-12 23:52:01,821\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:52:03,106\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1218729)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/1bb12d79/checkpoint_000005)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:52:04,284 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.0633 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1218729)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/1bb12d79/checkpoint_000006)\n",
            "2025-03-12 23:52:04,420\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:52:05,684\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1218729)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/1bb12d79/checkpoint_000007)\n",
            "\u001b[36m(RayTrainWorker pid=1218729)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/1bb12d79/checkpoint_000008)\n",
            "2025-03-12 23:52:06,979\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:52:08,263\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1218729)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/1bb12d79/checkpoint_000009)\n",
            "\u001b[36m(RayTrainWorker pid=1218729)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/1bb12d79/checkpoint_000010)\n",
            "2025-03-12 23:52:09,487\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1218729)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/1bb12d79/checkpoint_000011)\n",
            "2025-03-12 23:52:10,749\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:52:12,064\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1218729)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/1bb12d79/checkpoint_000012)\n",
            "\u001b[36m(RayTrainWorker pid=1218729)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/1bb12d79/checkpoint_000013)\n",
            "2025-03-12 23:52:13,315\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:52:14,293 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1637 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1218729)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/1bb12d79/checkpoint_000014)\n",
            "2025-03-12 23:52:14,680\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1218729)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/1bb12d79/checkpoint_000015)\n",
            "2025-03-12 23:52:15,945\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1218729)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/1bb12d79/checkpoint_000016)\n",
            "2025-03-12 23:52:17,205\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:52:18,435\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1218729)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/1bb12d79/checkpoint_000017)\n",
            "2025-03-12 23:52:19,760\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1218729)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/1bb12d79/checkpoint_000018)\n",
            "2025-03-12 23:52:21,099\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1218729)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/1bb12d79/checkpoint_000019)\n",
            "2025-03-12 23:52:22,356\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1218729)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/1bb12d79/checkpoint_000020)\n",
            "2025-03-12 23:52:23,700\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1218729)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/1bb12d79/checkpoint_000021)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:52:24,302 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1637 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1218729)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/1bb12d79/checkpoint_000022)\n",
            "2025-03-12 23:52:24,961\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:52:26,242\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1218729)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/1bb12d79/checkpoint_000023)\n",
            "2025-03-12 23:52:27,579\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1218729)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/1bb12d79/checkpoint_000024)\n",
            "\u001b[36m(RayTrainWorker pid=1218729)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/1bb12d79/checkpoint_000025)\n",
            "2025-03-12 23:52:28,820\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:52:30,113\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1218729)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/1bb12d79/checkpoint_000026)\n",
            "2025-03-12 23:52:31,441\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1218729)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/1bb12d79/checkpoint_000027)\n",
            "2025-03-12 23:52:32,696\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1218729)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/1bb12d79/checkpoint_000028)\n",
            "2025-03-12 23:52:34,013\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1218729)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/1bb12d79/checkpoint_000029)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:52:34,311 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.159 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:52:35,374\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1218729)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/1bb12d79/checkpoint_000030)\n",
            "2025-03-12 23:52:36,652\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1218729)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/1bb12d79/checkpoint_000031)\n",
            "2025-03-12 23:52:37,899\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1218729)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/1bb12d79/checkpoint_000032)\n",
            "2025-03-12 23:52:39,231\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1218729)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/1bb12d79/checkpoint_000033)\n",
            "2025-03-12 23:52:40,481\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1218729)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/1bb12d79/checkpoint_000034)\n",
            "\u001b[36m(RayTrainWorker pid=1218729)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/1bb12d79/checkpoint_000035)\n",
            "2025-03-12 23:52:41,761\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:52:43,055\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1218729)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/1bb12d79/checkpoint_000036)\n",
            "2025-03-12 23:52:44,293\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:52:44,320 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1636 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1218729)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/1bb12d79/checkpoint_000037)\n",
            "2025-03-12 23:52:45,578\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1218729)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/1bb12d79/checkpoint_000038)\n",
            "2025-03-12 23:52:46,806\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1218729)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/1bb12d79/checkpoint_000039)\n",
            "2025-03-12 23:52:48,142\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1218729)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/1bb12d79/checkpoint_000040)\n",
            "\u001b[36m(RayTrainWorker pid=1218729)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/1bb12d79/checkpoint_000041)\n",
            "2025-03-12 23:52:49,427\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:52:50,709\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1218729)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/1bb12d79/checkpoint_000042)\n",
            "2025-03-12 23:52:51,972\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1218729)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/1bb12d79/checkpoint_000043)\n",
            "\u001b[36m(RayTrainWorker pid=1218729)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/1bb12d79/checkpoint_000044)\n",
            "2025-03-12 23:52:53,349\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:52:54,329 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1635 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1218729)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/1bb12d79/checkpoint_000045)\n",
            "2025-03-12 23:52:54,604\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:52:55,883\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1218729)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/1bb12d79/checkpoint_000046)\n",
            "\u001b[36m(RayTrainWorker pid=1218729)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/1bb12d79/checkpoint_000047)\n",
            "2025-03-12 23:52:57,200\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:52:58,474\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1218729)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/1bb12d79/checkpoint_000048)\n",
            "2025-03-12 23:52:59,737\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1218729)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/1bb12d79/checkpoint_000049)\n",
            "\u001b[36m(RayTrainWorker pid=1218729)\u001b[0m `Trainer.fit` stopped: `max_epochs=50` reached.\n",
            "\u001b[36m(RayTrainWorker pid=1218729)\u001b[0m [rank0]:[W312 23:53:01.393664841 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:53:04,339 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1634 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(TorchTrainer pid=1220227)\u001b[0m Started distributed worker processes: \n",
            "\u001b[36m(TorchTrainer pid=1220227)\u001b[0m - (node_id=6cf2562c5de19587d3dc355f8ebead9f15bbc1bb9651b7563d063b82, ip=10.128.0.3, pid=1220550) world_rank=0, local_rank=0, node_rank=0\n",
            "\u001b[36m(RayTrainWorker pid=1220550)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:53:14,348 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1634 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1220550)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/ray/train/lightning/_lightning_utils.py:262: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
            "\u001b[36m(RayTrainWorker pid=1220550)\u001b[0m `get_trial_name` is deprecated because the concept of a `Trial` will soon be removed in Ray Train.Ray Train will no longer assume that it's running within a Ray Tune `Trial` in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
            "\u001b[36m(RayTrainWorker pid=1220550)\u001b[0m GPU available: True (cuda), used: True\n",
            "\u001b[36m(RayTrainWorker pid=1220550)\u001b[0m TPU available: False, using: 0 TPU cores\n",
            "\u001b[36m(RayTrainWorker pid=1220550)\u001b[0m HPU available: False, using: 0 HPUs\n",
            "\u001b[36m(RayTrainWorker pid=1220550)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(RayTrainWorker pid=1220550)\u001b[0m Loading checkpoint from roberta to roberta...\n",
            "\u001b[36m(RayTrainWorker pid=1220550)\u001b[0m <All keys matched successfully>\n",
            "\u001b[36m(RayTrainWorker pid=1220550)\u001b[0m Getting checkpoint from /home/jupyter/AgenticADMET/notebooks/../output/artifacts/mol_mlm_roberta_zinc/last.ckpt...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(RayTrainWorker pid=1220550)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\u001b[36m(RayTrainWorker pid=1220550)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=1220550)\u001b[0m   | Name      | Type         | Params | Mode \n",
            "\u001b[36m(RayTrainWorker pid=1220550)\u001b[0m ---------------------------------------------------\n",
            "\u001b[36m(RayTrainWorker pid=1220550)\u001b[0m 0 | roberta   | RobertaModel | 8.7 M  | train\n",
            "\u001b[36m(RayTrainWorker pid=1220550)\u001b[0m 1 | predictor | MLP          | 165 K  | train\n",
            "\u001b[36m(RayTrainWorker pid=1220550)\u001b[0m 2 | criterion | MSE          | 0      | train\n",
            "\u001b[36m(RayTrainWorker pid=1220550)\u001b[0m 3 | metrics   | ModuleList   | 0      | train\n",
            "\u001b[36m(RayTrainWorker pid=1220550)\u001b[0m ---------------------------------------------------\n",
            "\u001b[36m(RayTrainWorker pid=1220550)\u001b[0m 8.8 M     Trainable params\n",
            "\u001b[36m(RayTrainWorker pid=1220550)\u001b[0m 0         Non-trainable params\n",
            "\u001b[36m(RayTrainWorker pid=1220550)\u001b[0m 8.8 M     Total params\n",
            "\u001b[36m(RayTrainWorker pid=1220550)\u001b[0m 35.361    Total estimated model params size (MB)\n",
            "\u001b[36m(RayTrainWorker pid=1220550)\u001b[0m 132       Modules in train mode\n",
            "\u001b[36m(RayTrainWorker pid=1220550)\u001b[0m 0         Modules in eval mode\n",
            "\u001b[36m(RayTrainWorker pid=1220550)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
            "\u001b[36m(RayTrainWorker pid=1220550)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1220550)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
            "\u001b[36m(RayTrainWorker pid=1220550)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "\u001b[36m(RayTrainWorker pid=1220550)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('lr', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1220550)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('train_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1220550)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/7c654a8e/checkpoint_000000)\n",
            "\u001b[36m(RayTrainWorker pid=1220550)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/7c654a8e/checkpoint_000001)\n",
            "2025-03-12 23:53:19,757\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:53:20,882\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1220550)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/7c654a8e/checkpoint_000002)\n",
            "2025-03-12 23:53:22,095\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1220550)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/7c654a8e/checkpoint_000003)\n",
            "\u001b[36m(RayTrainWorker pid=1220550)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/7c654a8e/checkpoint_000004)\n",
            "2025-03-12 23:53:23,216\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:53:24,375\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:53:24,357 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.0644 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1220550)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/7c654a8e/checkpoint_000005)\n",
            "2025-03-12 23:53:25,552\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1220550)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/7c654a8e/checkpoint_000006)\n",
            "2025-03-12 23:53:26,690\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1220550)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/7c654a8e/checkpoint_000007)\n",
            "\u001b[36m(RayTrainWorker pid=1220550)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/7c654a8e/checkpoint_000008)\n",
            "2025-03-12 23:53:27,881\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1220550)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/7c654a8e/checkpoint_000009)\n",
            "2025-03-12 23:53:28,999\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1220550)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/7c654a8e/checkpoint_000010)\n",
            "2025-03-12 23:53:30,165\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:53:31,291\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1220550)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/7c654a8e/checkpoint_000011)\n",
            "2025-03-12 23:53:32,519\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1220550)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/7c654a8e/checkpoint_000012)\n",
            "2025-03-12 23:53:33,652\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1220550)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/7c654a8e/checkpoint_000013)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:53:34,366 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1633 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1220550)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/7c654a8e/checkpoint_000014)\n",
            "2025-03-12 23:53:34,820\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:53:35,922\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1220550)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/7c654a8e/checkpoint_000015)\n",
            "\u001b[36m(RayTrainWorker pid=1220550)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/7c654a8e/checkpoint_000016)\n",
            "2025-03-12 23:53:37,109\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:53:38,201\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1220550)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/7c654a8e/checkpoint_000017)\n",
            "\u001b[36m(RayTrainWorker pid=1220550)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/7c654a8e/checkpoint_000018)\n",
            "2025-03-12 23:53:39,420\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:53:40,558\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1220550)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/7c654a8e/checkpoint_000019)\n",
            "2025-03-12 23:53:41,674\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1220550)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/7c654a8e/checkpoint_000020)\n",
            "2025-03-12 23:53:42,848\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1220550)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/7c654a8e/checkpoint_000021)\n",
            "\u001b[36m(RayTrainWorker pid=1220550)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/7c654a8e/checkpoint_000022)\n",
            "2025-03-12 23:53:43,992\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:53:44,375 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1632 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:53:45,138\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1220550)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/7c654a8e/checkpoint_000023)\n",
            "2025-03-12 23:53:46,264\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1220550)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/7c654a8e/checkpoint_000024)\n",
            "\u001b[36m(RayTrainWorker pid=1220550)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/7c654a8e/checkpoint_000025)\n",
            "2025-03-12 23:53:47,404\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:53:48,562\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1220550)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/7c654a8e/checkpoint_000026)\n",
            "2025-03-12 23:53:49,709\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1220550)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/7c654a8e/checkpoint_000027)\n",
            "\u001b[36m(RayTrainWorker pid=1220550)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/7c654a8e/checkpoint_000028)\n",
            "2025-03-12 23:53:50,897\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:53:52,064\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1220550)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/7c654a8e/checkpoint_000029)\n",
            "2025-03-12 23:53:53,229\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1220550)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/7c654a8e/checkpoint_000030)\n",
            "2025-03-12 23:53:54,380\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:53:54,384 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1632 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1220550)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/7c654a8e/checkpoint_000031)\n",
            "2025-03-12 23:53:55,553\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1220550)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/7c654a8e/checkpoint_000032)\n",
            "\u001b[36m(RayTrainWorker pid=1220550)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/7c654a8e/checkpoint_000033)\n",
            "2025-03-12 23:53:56,745\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:53:57,840\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1220550)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/7c654a8e/checkpoint_000034)\n",
            "\u001b[36m(RayTrainWorker pid=1220550)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/7c654a8e/checkpoint_000035)\n",
            "2025-03-12 23:53:59,042\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1220550)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/7c654a8e/checkpoint_000036)\n",
            "2025-03-12 23:54:00,198\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:54:01,365\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1220550)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/7c654a8e/checkpoint_000037)\n",
            "\u001b[36m(RayTrainWorker pid=1220550)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/7c654a8e/checkpoint_000038)\n",
            "2025-03-12 23:54:02,589\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1220550)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/7c654a8e/checkpoint_000039)\n",
            "2025-03-12 23:54:03,839\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:54:04,393 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1584 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:54:05,001\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1220550)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/7c654a8e/checkpoint_000040)\n",
            "2025-03-12 23:54:06,166\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1220550)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/7c654a8e/checkpoint_000041)\n",
            "2025-03-12 23:54:07,329\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1220550)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/7c654a8e/checkpoint_000042)\n",
            "\u001b[36m(RayTrainWorker pid=1220550)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/7c654a8e/checkpoint_000043)\n",
            "2025-03-12 23:54:08,503\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:54:09,709\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1220550)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/7c654a8e/checkpoint_000044)\n",
            "2025-03-12 23:54:10,854\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1220550)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/7c654a8e/checkpoint_000045)\n",
            "2025-03-12 23:54:11,986\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1220550)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/7c654a8e/checkpoint_000046)\n",
            "\u001b[36m(RayTrainWorker pid=1220550)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/7c654a8e/checkpoint_000047)\n",
            "2025-03-12 23:54:13,159\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1220550)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/7c654a8e/checkpoint_000048)\n",
            "2025-03-12 23:54:14,317\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:54:14,402 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.163 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:54:15,521\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1220550)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/7c654a8e/checkpoint_000049)\n",
            "\u001b[36m(RayTrainWorker pid=1220550)\u001b[0m `Trainer.fit` stopped: `max_epochs=50` reached.\n",
            "\u001b[36m(RayTrainWorker pid=1220550)\u001b[0m [rank0]:[W312 23:54:17.437524217 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:54:24,412 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1608 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(TorchTrainer pid=1221989)\u001b[0m Started distributed worker processes: \n",
            "\u001b[36m(TorchTrainer pid=1221989)\u001b[0m - (node_id=6cf2562c5de19587d3dc355f8ebead9f15bbc1bb9651b7563d063b82, ip=10.128.0.3, pid=1222149) world_rank=0, local_rank=0, node_rank=0\n",
            "\u001b[36m(RayTrainWorker pid=1222149)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(RayTrainWorker pid=1222149)\u001b[0m Getting checkpoint from /home/jupyter/AgenticADMET/notebooks/../output/artifacts/mol_mlm_roberta_zinc/last.ckpt...\n",
            "\u001b[36m(RayTrainWorker pid=1222149)\u001b[0m Loading checkpoint from roberta to roberta...\n",
            "\u001b[36m(RayTrainWorker pid=1222149)\u001b[0m <All keys matched successfully>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(RayTrainWorker pid=1222149)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/ray/train/lightning/_lightning_utils.py:262: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
            "\u001b[36m(RayTrainWorker pid=1222149)\u001b[0m `get_trial_name` is deprecated because the concept of a `Trial` will soon be removed in Ray Train.Ray Train will no longer assume that it's running within a Ray Tune `Trial` in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
            "\u001b[36m(RayTrainWorker pid=1222149)\u001b[0m GPU available: True (cuda), used: True\n",
            "\u001b[36m(RayTrainWorker pid=1222149)\u001b[0m TPU available: False, using: 0 TPU cores\n",
            "\u001b[36m(RayTrainWorker pid=1222149)\u001b[0m HPU available: False, using: 0 HPUs\n",
            "\u001b[36m(RayTrainWorker pid=1222149)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
            "\u001b[36m(RayTrainWorker pid=1222149)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\u001b[36m(RayTrainWorker pid=1222149)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=1222149)\u001b[0m   | Name      | Type         | Params | Mode \n",
            "\u001b[36m(RayTrainWorker pid=1222149)\u001b[0m ---------------------------------------------------\n",
            "\u001b[36m(RayTrainWorker pid=1222149)\u001b[0m 0 | roberta   | RobertaModel | 8.7 M  | train\n",
            "\u001b[36m(RayTrainWorker pid=1222149)\u001b[0m 1 | predictor | MLP          | 462 K  | train\n",
            "\u001b[36m(RayTrainWorker pid=1222149)\u001b[0m 2 | criterion | MSE          | 0      | train\n",
            "\u001b[36m(RayTrainWorker pid=1222149)\u001b[0m 3 | metrics   | ModuleList   | 0      | train\n",
            "\u001b[36m(RayTrainWorker pid=1222149)\u001b[0m ---------------------------------------------------\n",
            "\u001b[36m(RayTrainWorker pid=1222149)\u001b[0m 9.1 M     Trainable params\n",
            "\u001b[36m(RayTrainWorker pid=1222149)\u001b[0m 0         Non-trainable params\n",
            "\u001b[36m(RayTrainWorker pid=1222149)\u001b[0m 9.1 M     Total params\n",
            "\u001b[36m(RayTrainWorker pid=1222149)\u001b[0m 36.548    Total estimated model params size (MB)\n",
            "\u001b[36m(RayTrainWorker pid=1222149)\u001b[0m 132       Modules in train mode\n",
            "\u001b[36m(RayTrainWorker pid=1222149)\u001b[0m 0         Modules in eval mode\n",
            "\u001b[36m(RayTrainWorker pid=1222149)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
            "\u001b[36m(RayTrainWorker pid=1222149)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1222149)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
            "\u001b[36m(RayTrainWorker pid=1222149)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:54:34,421 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1628 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1222149)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('lr', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1222149)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('train_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1222149)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e1067dd5/checkpoint_000000)\n",
            "2025-03-12 23:54:36,110\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1222149)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e1067dd5/checkpoint_000001)\n",
            "2025-03-12 23:54:37,306\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1222149)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e1067dd5/checkpoint_000002)\n",
            "2025-03-12 23:54:38,454\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1222149)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e1067dd5/checkpoint_000003)\n",
            "2025-03-12 23:54:39,626\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1222149)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e1067dd5/checkpoint_000004)\n",
            "2025-03-12 23:54:40,775\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1222149)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e1067dd5/checkpoint_000005)\n",
            "2025-03-12 23:54:41,968\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1222149)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e1067dd5/checkpoint_000006)\n",
            "2025-03-12 23:54:43,169\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1222149)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e1067dd5/checkpoint_000007)\n",
            "\u001b[36m(RayTrainWorker pid=1222149)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e1067dd5/checkpoint_000008)\n",
            "2025-03-12 23:54:44,334\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:54:44,430 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1628 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:54:45,528\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1222149)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e1067dd5/checkpoint_000009)\n",
            "2025-03-12 23:54:46,713\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1222149)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e1067dd5/checkpoint_000010)\n",
            "\u001b[36m(RayTrainWorker pid=1222149)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e1067dd5/checkpoint_000011)\n",
            "2025-03-12 23:54:47,988\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:54:49,151\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1222149)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e1067dd5/checkpoint_000012)\n",
            "2025-03-12 23:54:50,319\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1222149)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e1067dd5/checkpoint_000013)\n",
            "2025-03-12 23:54:51,476\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1222149)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e1067dd5/checkpoint_000014)\n",
            "2025-03-12 23:54:52,679\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1222149)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e1067dd5/checkpoint_000015)\n",
            "2025-03-12 23:54:53,826\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1222149)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e1067dd5/checkpoint_000016)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:54:54,439 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1627 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1222149)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e1067dd5/checkpoint_000017)\n",
            "2025-03-12 23:54:55,081\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:54:56,250\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1222149)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e1067dd5/checkpoint_000018)\n",
            "2025-03-12 23:54:57,484\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1222149)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e1067dd5/checkpoint_000019)\n",
            "2025-03-12 23:54:58,662\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1222149)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e1067dd5/checkpoint_000020)\n",
            "2025-03-12 23:54:59,806\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1222149)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e1067dd5/checkpoint_000021)\n",
            "2025-03-12 23:55:01,016\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1222149)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e1067dd5/checkpoint_000022)\n",
            "\u001b[36m(RayTrainWorker pid=1222149)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e1067dd5/checkpoint_000023)\n",
            "2025-03-12 23:55:02,218\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1222149)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e1067dd5/checkpoint_000024)\n",
            "2025-03-12 23:55:03,392\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:55:04,447 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.0604 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:55:04,560\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1222149)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e1067dd5/checkpoint_000025)\n",
            "\u001b[36m(RayTrainWorker pid=1222149)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e1067dd5/checkpoint_000026)\n",
            "2025-03-12 23:55:05,753\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1222149)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e1067dd5/checkpoint_000027)\n",
            "2025-03-12 23:55:06,900\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:55:08,095\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1222149)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e1067dd5/checkpoint_000028)\n",
            "2025-03-12 23:55:09,323\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1222149)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e1067dd5/checkpoint_000029)\n",
            "2025-03-12 23:55:10,581\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1222149)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e1067dd5/checkpoint_000030)\n",
            "2025-03-12 23:55:11,790\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1222149)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e1067dd5/checkpoint_000031)\n",
            "2025-03-12 23:55:12,973\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1222149)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e1067dd5/checkpoint_000032)\n",
            "\u001b[36m(RayTrainWorker pid=1222149)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e1067dd5/checkpoint_000033)\n",
            "2025-03-12 23:55:14,163\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:55:14,456 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1626 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:55:15,277\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1222149)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e1067dd5/checkpoint_000034)\n",
            "2025-03-12 23:55:16,507\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1222149)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e1067dd5/checkpoint_000035)\n",
            "2025-03-12 23:55:17,726\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1222149)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e1067dd5/checkpoint_000036)\n",
            "2025-03-12 23:55:18,904\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1222149)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e1067dd5/checkpoint_000037)\n",
            "\u001b[36m(RayTrainWorker pid=1222149)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e1067dd5/checkpoint_000038)\n",
            "2025-03-12 23:55:20,060\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:55:21,251\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1222149)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e1067dd5/checkpoint_000039)\n",
            "\u001b[36m(RayTrainWorker pid=1222149)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e1067dd5/checkpoint_000040)\n",
            "2025-03-12 23:55:22,449\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:55:23,627\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1222149)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e1067dd5/checkpoint_000041)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:55:24,466 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1625 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1222149)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e1067dd5/checkpoint_000042)\n",
            "2025-03-12 23:55:24,837\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:55:26,064\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1222149)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e1067dd5/checkpoint_000043)\n",
            "2025-03-12 23:55:27,304\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1222149)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e1067dd5/checkpoint_000044)\n",
            "2025-03-12 23:55:28,517\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1222149)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e1067dd5/checkpoint_000045)\n",
            "2025-03-12 23:55:29,683\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1222149)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e1067dd5/checkpoint_000046)\n",
            "\u001b[36m(RayTrainWorker pid=1222149)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e1067dd5/checkpoint_000047)\n",
            "2025-03-12 23:55:30,865\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:55:32,053\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1222149)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e1067dd5/checkpoint_000048)\n",
            "2025-03-12 23:55:33,216\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1222149)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/e1067dd5/checkpoint_000049)\n",
            "\u001b[36m(RayTrainWorker pid=1222149)\u001b[0m `Trainer.fit` stopped: `max_epochs=50` reached.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:55:34,476 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1625 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1222149)\u001b[0m [rank0]:[W312 23:55:35.960025821 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:55:44,486 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1624 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1224012)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
            "\u001b[36m(TorchTrainer pid=1223743)\u001b[0m Started distributed worker processes: \n",
            "\u001b[36m(TorchTrainer pid=1223743)\u001b[0m - (node_id=6cf2562c5de19587d3dc355f8ebead9f15bbc1bb9651b7563d063b82, ip=10.128.0.3, pid=1224012) world_rank=0, local_rank=0, node_rank=0\n",
            "\u001b[36m(RayTrainWorker pid=1224012)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/ray/train/lightning/_lightning_utils.py:262: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
            "\u001b[36m(RayTrainWorker pid=1224012)\u001b[0m `get_trial_name` is deprecated because the concept of a `Trial` will soon be removed in Ray Train.Ray Train will no longer assume that it's running within a Ray Tune `Trial` in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
            "\u001b[36m(RayTrainWorker pid=1224012)\u001b[0m GPU available: True (cuda), used: True\n",
            "\u001b[36m(RayTrainWorker pid=1224012)\u001b[0m TPU available: False, using: 0 TPU cores\n",
            "\u001b[36m(RayTrainWorker pid=1224012)\u001b[0m HPU available: False, using: 0 HPUs\n",
            "\u001b[36m(RayTrainWorker pid=1224012)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(RayTrainWorker pid=1224012)\u001b[0m Getting checkpoint from /home/jupyter/AgenticADMET/notebooks/../output/artifacts/mol_mlm_roberta_zinc/last.ckpt...\n",
            "\u001b[36m(RayTrainWorker pid=1224012)\u001b[0m Loading checkpoint from roberta to roberta...\n",
            "\u001b[36m(RayTrainWorker pid=1224012)\u001b[0m <All keys matched successfully>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(RayTrainWorker pid=1224012)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\u001b[36m(RayTrainWorker pid=1224012)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=1224012)\u001b[0m   | Name      | Type         | Params | Mode \n",
            "\u001b[36m(RayTrainWorker pid=1224012)\u001b[0m ---------------------------------------------------\n",
            "\u001b[36m(RayTrainWorker pid=1224012)\u001b[0m 0 | roberta   | RobertaModel | 8.7 M  | train\n",
            "\u001b[36m(RayTrainWorker pid=1224012)\u001b[0m 1 | predictor | MLP          | 149 K  | train\n",
            "\u001b[36m(RayTrainWorker pid=1224012)\u001b[0m 2 | criterion | MSE          | 0      | train\n",
            "\u001b[36m(RayTrainWorker pid=1224012)\u001b[0m 3 | metrics   | ModuleList   | 0      | train\n",
            "\u001b[36m(RayTrainWorker pid=1224012)\u001b[0m ---------------------------------------------------\n",
            "\u001b[36m(RayTrainWorker pid=1224012)\u001b[0m 8.8 M     Trainable params\n",
            "\u001b[36m(RayTrainWorker pid=1224012)\u001b[0m 0         Non-trainable params\n",
            "\u001b[36m(RayTrainWorker pid=1224012)\u001b[0m 8.8 M     Total params\n",
            "\u001b[36m(RayTrainWorker pid=1224012)\u001b[0m 35.297    Total estimated model params size (MB)\n",
            "\u001b[36m(RayTrainWorker pid=1224012)\u001b[0m 128       Modules in train mode\n",
            "\u001b[36m(RayTrainWorker pid=1224012)\u001b[0m 0         Modules in eval mode\n",
            "\u001b[36m(RayTrainWorker pid=1224012)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
            "\u001b[36m(RayTrainWorker pid=1224012)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1224012)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
            "\u001b[36m(RayTrainWorker pid=1224012)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (11) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "\u001b[36m(RayTrainWorker pid=1224012)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('lr', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1224012)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('train_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1224012)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d71d2247/checkpoint_000000)\n",
            "\u001b[36m(RayTrainWorker pid=1224012)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d71d2247/checkpoint_000001)\n",
            "2025-03-12 23:55:53,837\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:55:54,496 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1622 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:55:54,948\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1224012)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d71d2247/checkpoint_000002)\n",
            "2025-03-12 23:55:56,076\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1224012)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d71d2247/checkpoint_000003)\n",
            "2025-03-12 23:55:57,207\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1224012)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d71d2247/checkpoint_000004)\n",
            "\u001b[36m(RayTrainWorker pid=1224012)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d71d2247/checkpoint_000005)\n",
            "2025-03-12 23:55:58,307\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1224012)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d71d2247/checkpoint_000006)\n",
            "2025-03-12 23:55:59,478\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:56:00,564\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1224012)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d71d2247/checkpoint_000007)\n",
            "\u001b[36m(RayTrainWorker pid=1224012)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d71d2247/checkpoint_000008)\n",
            "2025-03-12 23:56:01,764\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1224012)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d71d2247/checkpoint_000009)\n",
            "2025-03-12 23:56:02,889\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1224012)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d71d2247/checkpoint_000010)\n",
            "2025-03-12 23:56:04,063\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:56:04,505 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1622 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:56:05,151\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1224012)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d71d2247/checkpoint_000011)\n",
            "2025-03-12 23:56:06,283\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1224012)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d71d2247/checkpoint_000012)\n",
            "2025-03-12 23:56:07,432\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1224012)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d71d2247/checkpoint_000013)\n",
            "2025-03-12 23:56:08,598\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1224012)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d71d2247/checkpoint_000014)\n",
            "2025-03-12 23:56:09,717\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1224012)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d71d2247/checkpoint_000015)\n",
            "2025-03-12 23:56:10,855\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1224012)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d71d2247/checkpoint_000016)\n",
            "2025-03-12 23:56:11,984\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1224012)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d71d2247/checkpoint_000017)\n",
            "2025-03-12 23:56:13,120\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1224012)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d71d2247/checkpoint_000018)\n",
            "2025-03-12 23:56:14,258\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1224012)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d71d2247/checkpoint_000019)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:56:14,514 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1621 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:56:15,449\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1224012)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d71d2247/checkpoint_000020)\n",
            "\u001b[36m(RayTrainWorker pid=1224012)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d71d2247/checkpoint_000021)\n",
            "2025-03-12 23:56:16,625\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:56:17,711\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1224012)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d71d2247/checkpoint_000022)\n",
            "\u001b[36m(RayTrainWorker pid=1224012)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d71d2247/checkpoint_000023)\n",
            "2025-03-12 23:56:18,886\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1224012)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d71d2247/checkpoint_000024)\n",
            "2025-03-12 23:56:20,049\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:56:21,206\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1224012)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d71d2247/checkpoint_000025)\n",
            "2025-03-12 23:56:22,364\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1224012)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d71d2247/checkpoint_000026)\n",
            "2025-03-12 23:56:23,516\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1224012)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d71d2247/checkpoint_000027)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:56:24,523 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.0634 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:56:24,660\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1224012)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d71d2247/checkpoint_000028)\n",
            "2025-03-12 23:56:25,789\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1224012)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d71d2247/checkpoint_000029)\n",
            "2025-03-12 23:56:26,909\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1224012)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d71d2247/checkpoint_000030)\n",
            "2025-03-12 23:56:28,084\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1224012)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d71d2247/checkpoint_000031)\n",
            "2025-03-12 23:56:29,220\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1224012)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d71d2247/checkpoint_000032)\n",
            "\u001b[36m(RayTrainWorker pid=1224012)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d71d2247/checkpoint_000033)\n",
            "2025-03-12 23:56:30,414\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1224012)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d71d2247/checkpoint_000034)\n",
            "2025-03-12 23:56:31,529\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:56:32,728\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1224012)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d71d2247/checkpoint_000035)\n",
            "2025-03-12 23:56:33,884\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1224012)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d71d2247/checkpoint_000036)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:56:34,533 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1621 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:56:35,059\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1224012)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d71d2247/checkpoint_000037)\n",
            "2025-03-12 23:56:36,199\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1224012)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d71d2247/checkpoint_000038)\n",
            "\u001b[36m(RayTrainWorker pid=1224012)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d71d2247/checkpoint_000039)\n",
            "2025-03-12 23:56:37,364\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:56:38,525\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1224012)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d71d2247/checkpoint_000040)\n",
            "2025-03-12 23:56:39,672\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1224012)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d71d2247/checkpoint_000041)\n",
            "2025-03-12 23:56:40,831\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1224012)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d71d2247/checkpoint_000042)\n",
            "2025-03-12 23:56:41,980\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1224012)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d71d2247/checkpoint_000043)\n",
            "2025-03-12 23:56:43,138\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1224012)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d71d2247/checkpoint_000044)\n",
            "2025-03-12 23:56:44,280\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1224012)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d71d2247/checkpoint_000045)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:56:44,543 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.162 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:56:45,417\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1224012)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d71d2247/checkpoint_000046)\n",
            "2025-03-12 23:56:46,580\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1224012)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d71d2247/checkpoint_000047)\n",
            "2025-03-12 23:56:47,732\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1224012)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d71d2247/checkpoint_000048)\n",
            "2025-03-12 23:56:48,925\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1224012)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/d71d2247/checkpoint_000049)\n",
            "\u001b[36m(RayTrainWorker pid=1224012)\u001b[0m `Trainer.fit` stopped: `max_epochs=50` reached.\n",
            "\u001b[36m(RayTrainWorker pid=1224012)\u001b[0m [rank0]:[W312 23:56:50.543613081 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:56:54,553 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1573 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(TorchTrainer pid=1225445)\u001b[0m Started distributed worker processes: \n",
            "\u001b[36m(TorchTrainer pid=1225445)\u001b[0m - (node_id=6cf2562c5de19587d3dc355f8ebead9f15bbc1bb9651b7563d063b82, ip=10.128.0.3, pid=1225601) world_rank=0, local_rank=0, node_rank=0\n",
            "\u001b[36m(RayTrainWorker pid=1225601)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:57:04,563 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1618 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(RayTrainWorker pid=1225601)\u001b[0m Getting checkpoint from /home/jupyter/AgenticADMET/notebooks/../output/artifacts/mol_mlm_roberta_zinc/last.ckpt...\n",
            "\u001b[36m(RayTrainWorker pid=1225601)\u001b[0m Loading checkpoint from roberta to roberta...\n",
            "\u001b[36m(RayTrainWorker pid=1225601)\u001b[0m <All keys matched successfully>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(RayTrainWorker pid=1225601)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/ray/train/lightning/_lightning_utils.py:262: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
            "\u001b[36m(RayTrainWorker pid=1225601)\u001b[0m `get_trial_name` is deprecated because the concept of a `Trial` will soon be removed in Ray Train.Ray Train will no longer assume that it's running within a Ray Tune `Trial` in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
            "\u001b[36m(RayTrainWorker pid=1225601)\u001b[0m GPU available: True (cuda), used: True\n",
            "\u001b[36m(RayTrainWorker pid=1225601)\u001b[0m TPU available: False, using: 0 TPU cores\n",
            "\u001b[36m(RayTrainWorker pid=1225601)\u001b[0m HPU available: False, using: 0 HPUs\n",
            "\u001b[36m(RayTrainWorker pid=1225601)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
            "\u001b[36m(RayTrainWorker pid=1225601)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\u001b[36m(RayTrainWorker pid=1225601)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=1225601)\u001b[0m   | Name      | Type         | Params | Mode \n",
            "\u001b[36m(RayTrainWorker pid=1225601)\u001b[0m ---------------------------------------------------\n",
            "\u001b[36m(RayTrainWorker pid=1225601)\u001b[0m 0 | roberta   | RobertaModel | 8.7 M  | train\n",
            "\u001b[36m(RayTrainWorker pid=1225601)\u001b[0m 1 | predictor | MLP          | 66.4 K | train\n",
            "\u001b[36m(RayTrainWorker pid=1225601)\u001b[0m 2 | criterion | MSE          | 0      | train\n",
            "\u001b[36m(RayTrainWorker pid=1225601)\u001b[0m 3 | metrics   | ModuleList   | 0      | train\n",
            "\u001b[36m(RayTrainWorker pid=1225601)\u001b[0m ---------------------------------------------------\n",
            "\u001b[36m(RayTrainWorker pid=1225601)\u001b[0m 8.7 M     Trainable params\n",
            "\u001b[36m(RayTrainWorker pid=1225601)\u001b[0m 0         Non-trainable params\n",
            "\u001b[36m(RayTrainWorker pid=1225601)\u001b[0m 8.7 M     Total params\n",
            "\u001b[36m(RayTrainWorker pid=1225601)\u001b[0m 34.964    Total estimated model params size (MB)\n",
            "\u001b[36m(RayTrainWorker pid=1225601)\u001b[0m 132       Modules in train mode\n",
            "\u001b[36m(RayTrainWorker pid=1225601)\u001b[0m 0         Modules in eval mode\n",
            "\u001b[36m(RayTrainWorker pid=1225601)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
            "\u001b[36m(RayTrainWorker pid=1225601)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1225601)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
            "\u001b[36m(RayTrainWorker pid=1225601)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "\u001b[36m(RayTrainWorker pid=1225601)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('lr', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1225601)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('train_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1225601)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/0d3d2f70/checkpoint_000000)\n",
            "2025-03-12 23:57:08,773\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1225601)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/0d3d2f70/checkpoint_000001)\n",
            "2025-03-12 23:57:09,816\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1225601)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/0d3d2f70/checkpoint_000002)\n",
            "2025-03-12 23:57:10,887\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1225601)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/0d3d2f70/checkpoint_000003)\n",
            "\u001b[36m(RayTrainWorker pid=1225601)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/0d3d2f70/checkpoint_000004)\n",
            "2025-03-12 23:57:11,988\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:57:13,089\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1225601)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/0d3d2f70/checkpoint_000005)\n",
            "2025-03-12 23:57:14,240\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1225601)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/0d3d2f70/checkpoint_000006)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:57:14,572 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1617 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1225601)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/0d3d2f70/checkpoint_000007)\n",
            "2025-03-12 23:57:15,320\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1225601)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/0d3d2f70/checkpoint_000008)\n",
            "2025-03-12 23:57:16,454\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:57:17,575\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1225601)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/0d3d2f70/checkpoint_000009)\n",
            "\u001b[36m(RayTrainWorker pid=1225601)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/0d3d2f70/checkpoint_000010)\n",
            "2025-03-12 23:57:18,649\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1225601)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/0d3d2f70/checkpoint_000011)\n",
            "2025-03-12 23:57:19,771\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:57:20,876\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1225601)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/0d3d2f70/checkpoint_000012)\n",
            "2025-03-12 23:57:21,981\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1225601)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/0d3d2f70/checkpoint_000013)\n",
            "\u001b[36m(RayTrainWorker pid=1225601)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/0d3d2f70/checkpoint_000014)\n",
            "2025-03-12 23:57:23,132\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1225601)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/0d3d2f70/checkpoint_000015)\n",
            "2025-03-12 23:57:24,271\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:57:24,582 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1571 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:57:25,363\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1225601)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/0d3d2f70/checkpoint_000016)\n",
            "2025-03-12 23:57:26,486\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1225601)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/0d3d2f70/checkpoint_000017)\n",
            "2025-03-12 23:57:27,634\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1225601)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/0d3d2f70/checkpoint_000018)\n",
            "2025-03-12 23:57:28,768\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1225601)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/0d3d2f70/checkpoint_000019)\n",
            "\u001b[36m(RayTrainWorker pid=1225601)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/0d3d2f70/checkpoint_000020)\n",
            "2025-03-12 23:57:29,906\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:57:31,058\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1225601)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/0d3d2f70/checkpoint_000021)\n",
            "\u001b[36m(RayTrainWorker pid=1225601)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/0d3d2f70/checkpoint_000022)\n",
            "2025-03-12 23:57:32,171\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:57:33,281\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1225601)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/0d3d2f70/checkpoint_000023)\n",
            "\u001b[36m(RayTrainWorker pid=1225601)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/0d3d2f70/checkpoint_000024)\n",
            "2025-03-12 23:57:34,382\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:57:34,591 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1617 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:57:35,479\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1225601)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/0d3d2f70/checkpoint_000025)\n",
            "2025-03-12 23:57:36,617\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1225601)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/0d3d2f70/checkpoint_000026)\n",
            "2025-03-12 23:57:37,772\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1225601)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/0d3d2f70/checkpoint_000027)\n",
            "2025-03-12 23:57:38,871\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1225601)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/0d3d2f70/checkpoint_000028)\n",
            "\u001b[36m(RayTrainWorker pid=1225601)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/0d3d2f70/checkpoint_000029)\n",
            "2025-03-12 23:57:40,004\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:57:41,108\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1225601)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/0d3d2f70/checkpoint_000030)\n",
            "\u001b[36m(RayTrainWorker pid=1225601)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/0d3d2f70/checkpoint_000031)\n",
            "2025-03-12 23:57:42,266\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:57:43,391\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1225601)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/0d3d2f70/checkpoint_000032)\n",
            "2025-03-12 23:57:44,503\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1225601)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/0d3d2f70/checkpoint_000033)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:57:44,602 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1617 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:57:45,584\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1225601)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/0d3d2f70/checkpoint_000034)\n",
            "2025-03-12 23:57:46,697\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1225601)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/0d3d2f70/checkpoint_000035)\n",
            "\u001b[36m(RayTrainWorker pid=1225601)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/0d3d2f70/checkpoint_000036)\n",
            "2025-03-12 23:57:47,787\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1225601)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/0d3d2f70/checkpoint_000037)\n",
            "2025-03-12 23:57:48,932\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1225601)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/0d3d2f70/checkpoint_000038)\n",
            "2025-03-12 23:57:50,066\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1225601)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/0d3d2f70/checkpoint_000039)\n",
            "2025-03-12 23:57:51,236\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1225601)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/0d3d2f70/checkpoint_000040)\n",
            "2025-03-12 23:57:52,390\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:57:53,477\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1225601)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/0d3d2f70/checkpoint_000041)\n",
            "2025-03-12 23:57:54,523\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1225601)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/0d3d2f70/checkpoint_000042)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:57:54,611 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1615 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:57:55,658\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1225601)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/0d3d2f70/checkpoint_000043)\n",
            "2025-03-12 23:57:56,774\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1225601)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/0d3d2f70/checkpoint_000044)\n",
            "2025-03-12 23:57:57,917\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1225601)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/0d3d2f70/checkpoint_000045)\n",
            "2025-03-12 23:57:59,009\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1225601)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/0d3d2f70/checkpoint_000046)\n",
            "\u001b[36m(RayTrainWorker pid=1225601)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/0d3d2f70/checkpoint_000047)\n",
            "2025-03-12 23:58:00,083\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:58:01,169\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1225601)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/0d3d2f70/checkpoint_000048)\n",
            "2025-03-12 23:58:02,295\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1225601)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/0d3d2f70/checkpoint_000049)\n",
            "\u001b[36m(RayTrainWorker pid=1225601)\u001b[0m `Trainer.fit` stopped: `max_epochs=50` reached.\n",
            "\u001b[36m(RayTrainWorker pid=1225601)\u001b[0m [rank0]:[W312 23:58:04.953529796 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:58:04,621 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1614 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:58:14,642 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1613 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1227228)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
            "\u001b[36m(TorchTrainer pid=1226968)\u001b[0m Started distributed worker processes: \n",
            "\u001b[36m(TorchTrainer pid=1226968)\u001b[0m - (node_id=6cf2562c5de19587d3dc355f8ebead9f15bbc1bb9651b7563d063b82, ip=10.128.0.3, pid=1227228) world_rank=0, local_rank=0, node_rank=0\n",
            "\u001b[36m(RayTrainWorker pid=1227228)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/ray/train/lightning/_lightning_utils.py:262: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
            "\u001b[36m(RayTrainWorker pid=1227228)\u001b[0m `get_trial_name` is deprecated because the concept of a `Trial` will soon be removed in Ray Train.Ray Train will no longer assume that it's running within a Ray Tune `Trial` in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
            "\u001b[36m(RayTrainWorker pid=1227228)\u001b[0m GPU available: True (cuda), used: True\n",
            "\u001b[36m(RayTrainWorker pid=1227228)\u001b[0m TPU available: False, using: 0 TPU cores\n",
            "\u001b[36m(RayTrainWorker pid=1227228)\u001b[0m HPU available: False, using: 0 HPUs\n",
            "\u001b[36m(RayTrainWorker pid=1227228)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(RayTrainWorker pid=1227228)\u001b[0m Getting checkpoint from /home/jupyter/AgenticADMET/notebooks/../output/artifacts/mol_mlm_roberta_zinc/last.ckpt...\n",
            "\u001b[36m(RayTrainWorker pid=1227228)\u001b[0m Loading checkpoint from roberta to roberta...\n",
            "\u001b[36m(RayTrainWorker pid=1227228)\u001b[0m <All keys matched successfully>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(RayTrainWorker pid=1227228)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\u001b[36m(RayTrainWorker pid=1227228)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=1227228)\u001b[0m   | Name      | Type         | Params | Mode \n",
            "\u001b[36m(RayTrainWorker pid=1227228)\u001b[0m ---------------------------------------------------\n",
            "\u001b[36m(RayTrainWorker pid=1227228)\u001b[0m 0 | roberta   | RobertaModel | 8.7 M  | train\n",
            "\u001b[36m(RayTrainWorker pid=1227228)\u001b[0m 1 | predictor | MLP          | 399 K  | train\n",
            "\u001b[36m(RayTrainWorker pid=1227228)\u001b[0m 2 | criterion | MSE          | 0      | train\n",
            "\u001b[36m(RayTrainWorker pid=1227228)\u001b[0m 3 | metrics   | ModuleList   | 0      | train\n",
            "\u001b[36m(RayTrainWorker pid=1227228)\u001b[0m ---------------------------------------------------\n",
            "\u001b[36m(RayTrainWorker pid=1227228)\u001b[0m 9.1 M     Trainable params\n",
            "\u001b[36m(RayTrainWorker pid=1227228)\u001b[0m 0         Non-trainable params\n",
            "\u001b[36m(RayTrainWorker pid=1227228)\u001b[0m 9.1 M     Total params\n",
            "\u001b[36m(RayTrainWorker pid=1227228)\u001b[0m 36.296    Total estimated model params size (MB)\n",
            "\u001b[36m(RayTrainWorker pid=1227228)\u001b[0m 128       Modules in train mode\n",
            "\u001b[36m(RayTrainWorker pid=1227228)\u001b[0m 0         Modules in eval mode\n",
            "\u001b[36m(RayTrainWorker pid=1227228)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
            "\u001b[36m(RayTrainWorker pid=1227228)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1227228)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
            "\u001b[36m(RayTrainWorker pid=1227228)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (21) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "\u001b[36m(RayTrainWorker pid=1227228)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('lr', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "\u001b[36m(RayTrainWorker pid=1227228)\u001b[0m /opt/conda/envs/admet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('train_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "2025-03-12 23:58:22,398\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1227228)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/f780394e/checkpoint_000000)\n",
            "2025-03-12 23:58:23,765\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1227228)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/f780394e/checkpoint_000001)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:58:24,651 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1613 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:58:25,036\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1227228)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/f780394e/checkpoint_000002)\n",
            "2025-03-12 23:58:26,359\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1227228)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/f780394e/checkpoint_000003)\n",
            "2025-03-12 23:58:27,627\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1227228)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/f780394e/checkpoint_000004)\n",
            "2025-03-12 23:58:28,912\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1227228)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/f780394e/checkpoint_000005)\n",
            "\u001b[36m(RayTrainWorker pid=1227228)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/f780394e/checkpoint_000006)\n",
            "2025-03-12 23:58:30,198\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1227228)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/f780394e/checkpoint_000007)\n",
            "2025-03-12 23:58:31,446\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:58:32,724\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1227228)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/f780394e/checkpoint_000008)\n",
            "\u001b[36m(RayTrainWorker pid=1227228)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/f780394e/checkpoint_000009)\n",
            "2025-03-12 23:58:34,041\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:58:34,660 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1612 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:58:35,338\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1227228)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/f780394e/checkpoint_000010)\n",
            "2025-03-12 23:58:36,678\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1227228)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/f780394e/checkpoint_000011)\n",
            "2025-03-12 23:58:37,945\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1227228)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/f780394e/checkpoint_000012)\n",
            "\u001b[36m(RayTrainWorker pid=1227228)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/f780394e/checkpoint_000013)\n",
            "2025-03-12 23:58:39,243\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1227228)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/f780394e/checkpoint_000014)\n",
            "2025-03-12 23:58:40,513\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1227228)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/f780394e/checkpoint_000015)\n",
            "2025-03-12 23:58:41,849\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:58:43,170\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1227228)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/f780394e/checkpoint_000016)\n",
            "2025-03-12 23:58:44,487\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1227228)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/f780394e/checkpoint_000017)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:58:44,670 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1612 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1227228)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/f780394e/checkpoint_000018)\n",
            "2025-03-12 23:58:45,902\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:58:47,162\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1227228)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/f780394e/checkpoint_000019)\n",
            "2025-03-12 23:58:48,447\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1227228)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/f780394e/checkpoint_000020)\n",
            "\u001b[36m(RayTrainWorker pid=1227228)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/f780394e/checkpoint_000021)\n",
            "2025-03-12 23:58:49,738\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:58:51,056\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1227228)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/f780394e/checkpoint_000022)\n",
            "\u001b[36m(RayTrainWorker pid=1227228)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/f780394e/checkpoint_000023)\n",
            "2025-03-12 23:58:52,360\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:58:53,661\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1227228)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/f780394e/checkpoint_000024)\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:58:54,679 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.1557 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1227228)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/f780394e/checkpoint_000025)\n",
            "2025-03-12 23:58:54,966\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:58:56,266\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1227228)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/f780394e/checkpoint_000026)\n",
            "2025-03-12 23:58:57,650\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1227228)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/f780394e/checkpoint_000027)\n",
            "2025-03-12 23:58:58,944\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1227228)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/f780394e/checkpoint_000028)\n",
            "2025-03-12 23:59:00,232\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1227228)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/f780394e/checkpoint_000029)\n",
            "2025-03-12 23:59:01,571\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1227228)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/f780394e/checkpoint_000030)\n",
            "2025-03-12 23:59:02,907\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1227228)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/f780394e/checkpoint_000031)\n",
            "\u001b[36m(RayTrainWorker pid=1227228)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/f780394e/checkpoint_000032)\n",
            "2025-03-12 23:59:04,244\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:59:04,688 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.161 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1227228)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/f780394e/checkpoint_000033)\n",
            "2025-03-12 23:59:05,572\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1227228)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/f780394e/checkpoint_000034)\n",
            "2025-03-12 23:59:06,844\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:59:08,148\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1227228)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/f780394e/checkpoint_000035)\n",
            "\u001b[36m(RayTrainWorker pid=1227228)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/f780394e/checkpoint_000036)\n",
            "2025-03-12 23:59:09,420\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:59:10,763\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1227228)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/f780394e/checkpoint_000037)\n",
            "2025-03-12 23:59:12,112\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1227228)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/f780394e/checkpoint_000038)\n",
            "\u001b[36m(RayTrainWorker pid=1227228)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/f780394e/checkpoint_000039)\n",
            "2025-03-12 23:59:13,389\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:59:14,673\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:59:14,698 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.161 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "\u001b[36m(RayTrainWorker pid=1227228)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/f780394e/checkpoint_000040)\n",
            "2025-03-12 23:59:15,937\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1227228)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/f780394e/checkpoint_000041)\n",
            "\u001b[36m(RayTrainWorker pid=1227228)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/f780394e/checkpoint_000042)\n",
            "2025-03-12 23:59:17,231\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:59:18,528\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1227228)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/f780394e/checkpoint_000043)\n",
            "2025-03-12 23:59:19,815\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1227228)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/f780394e/checkpoint_000044)\n",
            "\u001b[36m(RayTrainWorker pid=1227228)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/f780394e/checkpoint_000045)\n",
            "2025-03-12 23:59:21,088\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:59:22,309\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1227228)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/f780394e/checkpoint_000046)\n",
            "\u001b[36m(RayTrainWorker pid=1227228)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/f780394e/checkpoint_000047)\n",
            "2025-03-12 23:59:23,593\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-03-12 23:59:24,708 E 1176943 1176979] (raylet) file_system_monitor.cc:116: /var/tmp/ray/session_2025-03-12_23-20-50_987198_1174294 is over 95% full, available space: 13.0594 GB; capacity: 295.046 GB. Object creation will fail if spilling is required.\n",
            "2025-03-12 23:59:24,868\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=1227228)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/f780394e/checkpoint_000048)\n",
            "\u001b[36m(RayTrainWorker pid=1227228)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/f780394e/checkpoint_000049)\n",
            "\u001b[36m(RayTrainWorker pid=1227228)\u001b[0m `Trainer.fit` stopped: `max_epochs=50` reached.\n",
            "2025-03-12 23:59:26,193\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "2025-03-12 23:59:27,421\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54' in 0.0134s.\n",
            "2025-03-12 23:59:27,432\tINFO tune.py:1041 -- Total run time: 2312.70 seconds (2312.66 seconds for the tuning loop).\n"
          ]
        }
      ],
      "source": [
        "ray.shutdown()\n",
        "ray.init(include_dashboard=False)\n",
        "\n",
        "scheduler = FIFOScheduler()\n",
        "\n",
        "# Scaling config controls the resources used by Ray\n",
        "scaling_config = ScalingConfig(\n",
        "    num_workers=1,\n",
        "    use_gpu=True, # set to True if you want to use GPU\n",
        ")\n",
        "\n",
        "# Checkpoint config controls the checkpointing behavior of Ray\n",
        "checkpoint_config = CheckpointConfig(\n",
        "    num_to_keep=1, # number of checkpoints to keep\n",
        "    checkpoint_score_attribute=\"val_loss\", # Save the checkpoint based on this metric\n",
        "    checkpoint_score_order=\"min\", # Save the checkpoint with the lowest metric value\n",
        "    checkpoint_frequency=0,        # Do not checkpoint during training\n",
        ")\n",
        "\n",
        "run_config = RunConfig(\n",
        "    checkpoint_config=checkpoint_config,\n",
        "    storage_path=(hpopt_save_dir / \"ray_results\").absolute(), # directory to save the results\n",
        ")\n",
        "\n",
        "ray_trainer = TorchTrainer(\n",
        "    lambda config: train_model(config, train_dset, val_dset),\n",
        "    scaling_config=scaling_config,\n",
        "    run_config=run_config,\n",
        ")\n",
        "\n",
        "search_alg = HyperOptSearch(\n",
        "    n_initial_points=10, # number of random evaluations before tree parzen estimators\n",
        "    random_state_seed=RANDOM_SEED,\n",
        ")\n",
        "\n",
        "# OptunaSearch is another search algorithm that can be used\n",
        "# search_alg = OptunaSearch()\n",
        "\n",
        "tune_config = tune.TuneConfig(\n",
        "    metric=\"val/mae\",\n",
        "    mode=\"min\",\n",
        "    num_samples=30, # number of trials to run\n",
        "    scheduler=scheduler,\n",
        "    search_alg=search_alg,\n",
        "    trial_dirname_creator=lambda trial: str(trial.trial_id), # shorten filepaths\n",
        ")\n",
        "\n",
        "tuner = tune.Tuner(\n",
        "    ray_trainer,\n",
        "    param_space={\n",
        "        \"train_loop_config\": search_space,\n",
        "    },\n",
        "    tune_config=tune_config,\n",
        ")\n",
        "\n",
        "# Start the hyperparameter search\n",
        "results = tuner.fit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17UhIjgvOqhJ"
      },
      "source": [
        "## Hyperparameter optimization results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "AxfLJQAtOqhJ",
        "outputId": "93992d74-00ec-4e20-86f4-bba18ff0b83f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>train_loss</th>\n",
              "      <th>train_loss_step</th>\n",
              "      <th>val/mae</th>\n",
              "      <th>val/r2</th>\n",
              "      <th>val_loss</th>\n",
              "      <th>lr</th>\n",
              "      <th>train_loss_epoch</th>\n",
              "      <th>epoch</th>\n",
              "      <th>step</th>\n",
              "      <th>timestamp</th>\n",
              "      <th>...</th>\n",
              "      <th>hostname</th>\n",
              "      <th>node_ip</th>\n",
              "      <th>time_since_restore</th>\n",
              "      <th>iterations_since_restore</th>\n",
              "      <th>config/train_loop_config/hidden_dim</th>\n",
              "      <th>config/train_loop_config/num_layers</th>\n",
              "      <th>config/train_loop_config/batch_size</th>\n",
              "      <th>config/train_loop_config/weight_decay</th>\n",
              "      <th>config/train_loop_config/dropout</th>\n",
              "      <th>logdir</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.092170</td>\n",
              "      <td>0.092536</td>\n",
              "      <td>0.386482</td>\n",
              "      <td>0.669224</td>\n",
              "      <td>0.316027</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.092170</td>\n",
              "      <td>49</td>\n",
              "      <td>550</td>\n",
              "      <td>1741822100</td>\n",
              "      <td>...</td>\n",
              "      <td>dl-vladvin-1</td>\n",
              "      <td>10.128.0.3</td>\n",
              "      <td>63.350655</td>\n",
              "      <td>50</td>\n",
              "      <td>384</td>\n",
              "      <td>1</td>\n",
              "      <td>32</td>\n",
              "      <td>0.000810</td>\n",
              "      <td>0.034136</td>\n",
              "      <td>5a576c82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.077947</td>\n",
              "      <td>0.057792</td>\n",
              "      <td>0.396328</td>\n",
              "      <td>0.657834</td>\n",
              "      <td>0.322932</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.077947</td>\n",
              "      <td>49</td>\n",
              "      <td>1050</td>\n",
              "      <td>1741822332</td>\n",
              "      <td>...</td>\n",
              "      <td>dl-vladvin-1</td>\n",
              "      <td>10.128.0.3</td>\n",
              "      <td>73.132622</td>\n",
              "      <td>50</td>\n",
              "      <td>1024</td>\n",
              "      <td>2</td>\n",
              "      <td>16</td>\n",
              "      <td>0.000121</td>\n",
              "      <td>0.043038</td>\n",
              "      <td>5f4597d7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.095285</td>\n",
              "      <td>0.085481</td>\n",
              "      <td>0.399819</td>\n",
              "      <td>0.650773</td>\n",
              "      <td>0.327451</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.095285</td>\n",
              "      <td>49</td>\n",
              "      <td>350</td>\n",
              "      <td>1741823882</td>\n",
              "      <td>...</td>\n",
              "      <td>dl-vladvin-1</td>\n",
              "      <td>10.128.0.3</td>\n",
              "      <td>63.111884</td>\n",
              "      <td>50</td>\n",
              "      <td>128</td>\n",
              "      <td>2</td>\n",
              "      <td>48</td>\n",
              "      <td>0.000020</td>\n",
              "      <td>0.015735</td>\n",
              "      <td>0d3d2f70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.099747</td>\n",
              "      <td>0.026480</td>\n",
              "      <td>0.400149</td>\n",
              "      <td>0.641377</td>\n",
              "      <td>0.346764</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.099747</td>\n",
              "      <td>49</td>\n",
              "      <td>1050</td>\n",
              "      <td>1741823031</td>\n",
              "      <td>...</td>\n",
              "      <td>dl-vladvin-1</td>\n",
              "      <td>10.128.0.3</td>\n",
              "      <td>75.221304</td>\n",
              "      <td>50</td>\n",
              "      <td>1024</td>\n",
              "      <td>2</td>\n",
              "      <td>16</td>\n",
              "      <td>0.000154</td>\n",
              "      <td>0.041954</td>\n",
              "      <td>995662df</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.112690</td>\n",
              "      <td>0.096280</td>\n",
              "      <td>0.404122</td>\n",
              "      <td>0.661672</td>\n",
              "      <td>0.317830</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.112690</td>\n",
              "      <td>49</td>\n",
              "      <td>350</td>\n",
              "      <td>1741822173</td>\n",
              "      <td>...</td>\n",
              "      <td>dl-vladvin-1</td>\n",
              "      <td>10.128.0.3</td>\n",
              "      <td>62.279430</td>\n",
              "      <td>50</td>\n",
              "      <td>256</td>\n",
              "      <td>2</td>\n",
              "      <td>48</td>\n",
              "      <td>0.003525</td>\n",
              "      <td>0.114586</td>\n",
              "      <td>08c9cf27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.077075</td>\n",
              "      <td>0.063354</td>\n",
              "      <td>0.405421</td>\n",
              "      <td>0.651078</td>\n",
              "      <td>0.329478</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.077075</td>\n",
              "      <td>49</td>\n",
              "      <td>350</td>\n",
              "      <td>1741823186</td>\n",
              "      <td>...</td>\n",
              "      <td>dl-vladvin-1</td>\n",
              "      <td>10.128.0.3</td>\n",
              "      <td>63.248566</td>\n",
              "      <td>50</td>\n",
              "      <td>512</td>\n",
              "      <td>2</td>\n",
              "      <td>48</td>\n",
              "      <td>0.000430</td>\n",
              "      <td>0.027994</td>\n",
              "      <td>137d3681</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.091107</td>\n",
              "      <td>0.131095</td>\n",
              "      <td>0.406214</td>\n",
              "      <td>0.659257</td>\n",
              "      <td>0.317610</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.091107</td>\n",
              "      <td>49</td>\n",
              "      <td>350</td>\n",
              "      <td>1741823421</td>\n",
              "      <td>...</td>\n",
              "      <td>dl-vladvin-1</td>\n",
              "      <td>10.128.0.3</td>\n",
              "      <td>64.408439</td>\n",
              "      <td>50</td>\n",
              "      <td>640</td>\n",
              "      <td>2</td>\n",
              "      <td>48</td>\n",
              "      <td>0.000031</td>\n",
              "      <td>0.003652</td>\n",
              "      <td>ede8484b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.129780</td>\n",
              "      <td>0.161829</td>\n",
              "      <td>0.407775</td>\n",
              "      <td>0.666119</td>\n",
              "      <td>0.310546</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.129780</td>\n",
              "      <td>49</td>\n",
              "      <td>150</td>\n",
              "      <td>1741822634</td>\n",
              "      <td>...</td>\n",
              "      <td>dl-vladvin-1</td>\n",
              "      <td>10.128.0.3</td>\n",
              "      <td>66.980362</td>\n",
              "      <td>50</td>\n",
              "      <td>384</td>\n",
              "      <td>2</td>\n",
              "      <td>128</td>\n",
              "      <td>0.001676</td>\n",
              "      <td>0.067281</td>\n",
              "      <td>8eaba676</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.092790</td>\n",
              "      <td>0.080546</td>\n",
              "      <td>0.407854</td>\n",
              "      <td>0.673837</td>\n",
              "      <td>0.313061</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.092790</td>\n",
              "      <td>49</td>\n",
              "      <td>1050</td>\n",
              "      <td>1741823966</td>\n",
              "      <td>...</td>\n",
              "      <td>dl-vladvin-1</td>\n",
              "      <td>10.128.0.3</td>\n",
              "      <td>72.617448</td>\n",
              "      <td>50</td>\n",
              "      <td>1024</td>\n",
              "      <td>1</td>\n",
              "      <td>16</td>\n",
              "      <td>0.000209</td>\n",
              "      <td>0.034451</td>\n",
              "      <td>f780394e</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.090692</td>\n",
              "      <td>0.063619</td>\n",
              "      <td>0.407999</td>\n",
              "      <td>0.666391</td>\n",
              "      <td>0.310292</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.090692</td>\n",
              "      <td>49</td>\n",
              "      <td>200</td>\n",
              "      <td>1741821729</td>\n",
              "      <td>...</td>\n",
              "      <td>dl-vladvin-1</td>\n",
              "      <td>10.128.0.3</td>\n",
              "      <td>66.358428</td>\n",
              "      <td>50</td>\n",
              "      <td>896</td>\n",
              "      <td>2</td>\n",
              "      <td>96</td>\n",
              "      <td>0.000028</td>\n",
              "      <td>0.018611</td>\n",
              "      <td>56fc16e6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.128798</td>\n",
              "      <td>0.052107</td>\n",
              "      <td>0.408013</td>\n",
              "      <td>0.647400</td>\n",
              "      <td>0.327737</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.128798</td>\n",
              "      <td>49</td>\n",
              "      <td>300</td>\n",
              "      <td>1741821878</td>\n",
              "      <td>...</td>\n",
              "      <td>dl-vladvin-1</td>\n",
              "      <td>10.128.0.3</td>\n",
              "      <td>63.515885</td>\n",
              "      <td>50</td>\n",
              "      <td>128</td>\n",
              "      <td>1</td>\n",
              "      <td>64</td>\n",
              "      <td>0.000013</td>\n",
              "      <td>0.025711</td>\n",
              "      <td>bd4b22c9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.076417</td>\n",
              "      <td>0.033835</td>\n",
              "      <td>0.411848</td>\n",
              "      <td>0.664666</td>\n",
              "      <td>0.315712</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.076417</td>\n",
              "      <td>49</td>\n",
              "      <td>1050</td>\n",
              "      <td>1741823579</td>\n",
              "      <td>...</td>\n",
              "      <td>dl-vladvin-1</td>\n",
              "      <td>10.128.0.3</td>\n",
              "      <td>71.590104</td>\n",
              "      <td>50</td>\n",
              "      <td>768</td>\n",
              "      <td>1</td>\n",
              "      <td>16</td>\n",
              "      <td>0.000988</td>\n",
              "      <td>0.055379</td>\n",
              "      <td>1bb12d79</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.085354</td>\n",
              "      <td>0.072953</td>\n",
              "      <td>0.412268</td>\n",
              "      <td>0.642638</td>\n",
              "      <td>0.337387</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.085354</td>\n",
              "      <td>49</td>\n",
              "      <td>550</td>\n",
              "      <td>1741823497</td>\n",
              "      <td>...</td>\n",
              "      <td>dl-vladvin-1</td>\n",
              "      <td>10.128.0.3</td>\n",
              "      <td>65.948513</td>\n",
              "      <td>50</td>\n",
              "      <td>1024</td>\n",
              "      <td>1</td>\n",
              "      <td>32</td>\n",
              "      <td>0.000283</td>\n",
              "      <td>0.036214</td>\n",
              "      <td>3cee4eec</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.090405</td>\n",
              "      <td>0.176685</td>\n",
              "      <td>0.415268</td>\n",
              "      <td>0.615975</td>\n",
              "      <td>0.369317</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.090405</td>\n",
              "      <td>49</td>\n",
              "      <td>550</td>\n",
              "      <td>1741823264</td>\n",
              "      <td>...</td>\n",
              "      <td>dl-vladvin-1</td>\n",
              "      <td>10.128.0.3</td>\n",
              "      <td>66.485056</td>\n",
              "      <td>50</td>\n",
              "      <td>768</td>\n",
              "      <td>2</td>\n",
              "      <td>32</td>\n",
              "      <td>0.000155</td>\n",
              "      <td>0.064471</td>\n",
              "      <td>70578d83</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.084255</td>\n",
              "      <td>0.043339</td>\n",
              "      <td>0.419707</td>\n",
              "      <td>0.609386</td>\n",
              "      <td>0.378770</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.084255</td>\n",
              "      <td>49</td>\n",
              "      <td>1050</td>\n",
              "      <td>1741823113</td>\n",
              "      <td>...</td>\n",
              "      <td>dl-vladvin-1</td>\n",
              "      <td>10.128.0.3</td>\n",
              "      <td>71.719339</td>\n",
              "      <td>50</td>\n",
              "      <td>384</td>\n",
              "      <td>2</td>\n",
              "      <td>16</td>\n",
              "      <td>0.000061</td>\n",
              "      <td>0.047795</td>\n",
              "      <td>774d392e</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.078577</td>\n",
              "      <td>0.048802</td>\n",
              "      <td>0.421358</td>\n",
              "      <td>0.636165</td>\n",
              "      <td>0.341291</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.078577</td>\n",
              "      <td>49</td>\n",
              "      <td>350</td>\n",
              "      <td>1741822406</td>\n",
              "      <td>...</td>\n",
              "      <td>dl-vladvin-1</td>\n",
              "      <td>10.128.0.3</td>\n",
              "      <td>62.507959</td>\n",
              "      <td>50</td>\n",
              "      <td>512</td>\n",
              "      <td>1</td>\n",
              "      <td>48</td>\n",
              "      <td>0.000419</td>\n",
              "      <td>0.049347</td>\n",
              "      <td>481a02c2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.122182</td>\n",
              "      <td>0.096444</td>\n",
              "      <td>0.421762</td>\n",
              "      <td>0.647431</td>\n",
              "      <td>0.339310</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.122182</td>\n",
              "      <td>49</td>\n",
              "      <td>1050</td>\n",
              "      <td>1741822716</td>\n",
              "      <td>...</td>\n",
              "      <td>dl-vladvin-1</td>\n",
              "      <td>10.128.0.3</td>\n",
              "      <td>71.055779</td>\n",
              "      <td>50</td>\n",
              "      <td>640</td>\n",
              "      <td>1</td>\n",
              "      <td>16</td>\n",
              "      <td>0.092593</td>\n",
              "      <td>0.008781</td>\n",
              "      <td>fe376ce8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.179719</td>\n",
              "      <td>0.201831</td>\n",
              "      <td>0.422031</td>\n",
              "      <td>0.660143</td>\n",
              "      <td>0.316104</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.179719</td>\n",
              "      <td>49</td>\n",
              "      <td>150</td>\n",
              "      <td>1741822870</td>\n",
              "      <td>...</td>\n",
              "      <td>dl-vladvin-1</td>\n",
              "      <td>10.128.0.3</td>\n",
              "      <td>66.325545</td>\n",
              "      <td>50</td>\n",
              "      <td>384</td>\n",
              "      <td>2</td>\n",
              "      <td>112</td>\n",
              "      <td>0.000012</td>\n",
              "      <td>0.138805</td>\n",
              "      <td>a1a7455a</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.099124</td>\n",
              "      <td>0.056838</td>\n",
              "      <td>0.422184</td>\n",
              "      <td>0.644191</td>\n",
              "      <td>0.336550</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.099124</td>\n",
              "      <td>49</td>\n",
              "      <td>550</td>\n",
              "      <td>1741822026</td>\n",
              "      <td>...</td>\n",
              "      <td>dl-vladvin-1</td>\n",
              "      <td>10.128.0.3</td>\n",
              "      <td>64.266496</td>\n",
              "      <td>50</td>\n",
              "      <td>896</td>\n",
              "      <td>1</td>\n",
              "      <td>32</td>\n",
              "      <td>0.021232</td>\n",
              "      <td>0.156513</td>\n",
              "      <td>e0d70199</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.122485</td>\n",
              "      <td>0.094938</td>\n",
              "      <td>0.422941</td>\n",
              "      <td>0.619162</td>\n",
              "      <td>0.354221</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.122485</td>\n",
              "      <td>49</td>\n",
              "      <td>200</td>\n",
              "      <td>1741822480</td>\n",
              "      <td>...</td>\n",
              "      <td>dl-vladvin-1</td>\n",
              "      <td>10.128.0.3</td>\n",
              "      <td>64.191614</td>\n",
              "      <td>50</td>\n",
              "      <td>512</td>\n",
              "      <td>1</td>\n",
              "      <td>96</td>\n",
              "      <td>0.000570</td>\n",
              "      <td>0.131638</td>\n",
              "      <td>d61f6592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.103626</td>\n",
              "      <td>0.078887</td>\n",
              "      <td>0.423131</td>\n",
              "      <td>0.625669</td>\n",
              "      <td>0.360542</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.103626</td>\n",
              "      <td>49</td>\n",
              "      <td>1050</td>\n",
              "      <td>1741823346</td>\n",
              "      <td>...</td>\n",
              "      <td>dl-vladvin-1</td>\n",
              "      <td>10.128.0.3</td>\n",
              "      <td>71.213969</td>\n",
              "      <td>50</td>\n",
              "      <td>384</td>\n",
              "      <td>2</td>\n",
              "      <td>16</td>\n",
              "      <td>0.000913</td>\n",
              "      <td>0.086560</td>\n",
              "      <td>5834913a</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.099836</td>\n",
              "      <td>0.037196</td>\n",
              "      <td>0.426872</td>\n",
              "      <td>0.604044</td>\n",
              "      <td>0.375238</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.099836</td>\n",
              "      <td>49</td>\n",
              "      <td>550</td>\n",
              "      <td>1741822944</td>\n",
              "      <td>...</td>\n",
              "      <td>dl-vladvin-1</td>\n",
              "      <td>10.128.0.3</td>\n",
              "      <td>64.246681</td>\n",
              "      <td>50</td>\n",
              "      <td>256</td>\n",
              "      <td>1</td>\n",
              "      <td>32</td>\n",
              "      <td>0.001959</td>\n",
              "      <td>0.000715</td>\n",
              "      <td>6d6626b0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.128456</td>\n",
              "      <td>0.088383</td>\n",
              "      <td>0.428697</td>\n",
              "      <td>0.629483</td>\n",
              "      <td>0.344621</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.128456</td>\n",
              "      <td>49</td>\n",
              "      <td>200</td>\n",
              "      <td>1741822556</td>\n",
              "      <td>...</td>\n",
              "      <td>dl-vladvin-1</td>\n",
              "      <td>10.128.0.3</td>\n",
              "      <td>65.180606</td>\n",
              "      <td>50</td>\n",
              "      <td>640</td>\n",
              "      <td>1</td>\n",
              "      <td>96</td>\n",
              "      <td>0.000079</td>\n",
              "      <td>0.199139</td>\n",
              "      <td>d0f8f668</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.146676</td>\n",
              "      <td>0.178818</td>\n",
              "      <td>0.430102</td>\n",
              "      <td>0.616070</td>\n",
              "      <td>0.357299</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.146676</td>\n",
              "      <td>49</td>\n",
              "      <td>250</td>\n",
              "      <td>1741823733</td>\n",
              "      <td>...</td>\n",
              "      <td>dl-vladvin-1</td>\n",
              "      <td>10.128.0.3</td>\n",
              "      <td>66.944036</td>\n",
              "      <td>50</td>\n",
              "      <td>512</td>\n",
              "      <td>2</td>\n",
              "      <td>80</td>\n",
              "      <td>0.003619</td>\n",
              "      <td>0.016692</td>\n",
              "      <td>e1067dd5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.137496</td>\n",
              "      <td>0.155992</td>\n",
              "      <td>0.434657</td>\n",
              "      <td>0.591216</td>\n",
              "      <td>0.380214</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.137496</td>\n",
              "      <td>49</td>\n",
              "      <td>150</td>\n",
              "      <td>1741822249</td>\n",
              "      <td>...</td>\n",
              "      <td>dl-vladvin-1</td>\n",
              "      <td>10.128.0.3</td>\n",
              "      <td>66.045544</td>\n",
              "      <td>50</td>\n",
              "      <td>896</td>\n",
              "      <td>1</td>\n",
              "      <td>128</td>\n",
              "      <td>0.006943</td>\n",
              "      <td>0.082633</td>\n",
              "      <td>599a7bc7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.130427</td>\n",
              "      <td>0.082609</td>\n",
              "      <td>0.440786</td>\n",
              "      <td>0.604849</td>\n",
              "      <td>0.371668</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.130427</td>\n",
              "      <td>49</td>\n",
              "      <td>550</td>\n",
              "      <td>1741821804</td>\n",
              "      <td>...</td>\n",
              "      <td>dl-vladvin-1</td>\n",
              "      <td>10.128.0.3</td>\n",
              "      <td>65.196376</td>\n",
              "      <td>50</td>\n",
              "      <td>768</td>\n",
              "      <td>2</td>\n",
              "      <td>32</td>\n",
              "      <td>0.007773</td>\n",
              "      <td>0.173402</td>\n",
              "      <td>b2eb9de9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.141451</td>\n",
              "      <td>0.099026</td>\n",
              "      <td>0.448804</td>\n",
              "      <td>0.603299</td>\n",
              "      <td>0.370231</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.141451</td>\n",
              "      <td>49</td>\n",
              "      <td>300</td>\n",
              "      <td>1741821952</td>\n",
              "      <td>...</td>\n",
              "      <td>dl-vladvin-1</td>\n",
              "      <td>10.128.0.3</td>\n",
              "      <td>63.650551</td>\n",
              "      <td>50</td>\n",
              "      <td>256</td>\n",
              "      <td>1</td>\n",
              "      <td>64</td>\n",
              "      <td>0.043976</td>\n",
              "      <td>0.078642</td>\n",
              "      <td>439b693d</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.111356</td>\n",
              "      <td>0.067257</td>\n",
              "      <td>0.458665</td>\n",
              "      <td>0.600252</td>\n",
              "      <td>0.373409</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.111356</td>\n",
              "      <td>49</td>\n",
              "      <td>550</td>\n",
              "      <td>1741823808</td>\n",
              "      <td>...</td>\n",
              "      <td>dl-vladvin-1</td>\n",
              "      <td>10.128.0.3</td>\n",
              "      <td>64.808590</td>\n",
              "      <td>50</td>\n",
              "      <td>384</td>\n",
              "      <td>1</td>\n",
              "      <td>32</td>\n",
              "      <td>0.000042</td>\n",
              "      <td>0.069335</td>\n",
              "      <td>d71d2247</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.198289</td>\n",
              "      <td>0.197974</td>\n",
              "      <td>0.460946</td>\n",
              "      <td>0.598104</td>\n",
              "      <td>0.375146</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.198289</td>\n",
              "      <td>49</td>\n",
              "      <td>250</td>\n",
              "      <td>1741822793</td>\n",
              "      <td>...</td>\n",
              "      <td>dl-vladvin-1</td>\n",
              "      <td>10.128.0.3</td>\n",
              "      <td>65.480602</td>\n",
              "      <td>50</td>\n",
              "      <td>128</td>\n",
              "      <td>1</td>\n",
              "      <td>80</td>\n",
              "      <td>0.000169</td>\n",
              "      <td>0.106793</td>\n",
              "      <td>5fa0d29c</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.169902</td>\n",
              "      <td>0.127426</td>\n",
              "      <td>0.464765</td>\n",
              "      <td>0.580917</td>\n",
              "      <td>0.389617</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.169902</td>\n",
              "      <td>49</td>\n",
              "      <td>300</td>\n",
              "      <td>1741823655</td>\n",
              "      <td>...</td>\n",
              "      <td>dl-vladvin-1</td>\n",
              "      <td>10.128.0.3</td>\n",
              "      <td>65.446462</td>\n",
              "      <td>50</td>\n",
              "      <td>256</td>\n",
              "      <td>2</td>\n",
              "      <td>64</td>\n",
              "      <td>0.000068</td>\n",
              "      <td>0.096551</td>\n",
              "      <td>7c654a8e</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>30 rows × 29 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    train_loss  train_loss_step   val/mae    val/r2  val_loss      lr  \\\n",
              "5     0.092170         0.092536  0.386482  0.669224  0.316027  0.0001   \n",
              "8     0.077947         0.057792  0.396328  0.657834  0.322932  0.0001   \n",
              "28    0.095285         0.085481  0.399819  0.650773  0.327451  0.0001   \n",
              "17    0.099747         0.026480  0.400149  0.641377  0.346764  0.0001   \n",
              "6     0.112690         0.096280  0.404122  0.661672  0.317830  0.0001   \n",
              "19    0.077075         0.063354  0.405421  0.651078  0.329478  0.0001   \n",
              "22    0.091107         0.131095  0.406214  0.659257  0.317610  0.0001   \n",
              "12    0.129780         0.161829  0.407775  0.666119  0.310546  0.0001   \n",
              "29    0.092790         0.080546  0.407854  0.673837  0.313061  0.0001   \n",
              "0     0.090692         0.063619  0.407999  0.666391  0.310292  0.0001   \n",
              "2     0.128798         0.052107  0.408013  0.647400  0.327737  0.0001   \n",
              "24    0.076417         0.033835  0.411848  0.664666  0.315712  0.0001   \n",
              "23    0.085354         0.072953  0.412268  0.642638  0.337387  0.0001   \n",
              "20    0.090405         0.176685  0.415268  0.615975  0.369317  0.0001   \n",
              "18    0.084255         0.043339  0.419707  0.609386  0.378770  0.0001   \n",
              "9     0.078577         0.048802  0.421358  0.636165  0.341291  0.0001   \n",
              "13    0.122182         0.096444  0.421762  0.647431  0.339310  0.0001   \n",
              "15    0.179719         0.201831  0.422031  0.660143  0.316104  0.0001   \n",
              "4     0.099124         0.056838  0.422184  0.644191  0.336550  0.0001   \n",
              "10    0.122485         0.094938  0.422941  0.619162  0.354221  0.0001   \n",
              "21    0.103626         0.078887  0.423131  0.625669  0.360542  0.0001   \n",
              "16    0.099836         0.037196  0.426872  0.604044  0.375238  0.0001   \n",
              "11    0.128456         0.088383  0.428697  0.629483  0.344621  0.0001   \n",
              "26    0.146676         0.178818  0.430102  0.616070  0.357299  0.0001   \n",
              "7     0.137496         0.155992  0.434657  0.591216  0.380214  0.0001   \n",
              "1     0.130427         0.082609  0.440786  0.604849  0.371668  0.0001   \n",
              "3     0.141451         0.099026  0.448804  0.603299  0.370231  0.0001   \n",
              "27    0.111356         0.067257  0.458665  0.600252  0.373409  0.0001   \n",
              "14    0.198289         0.197974  0.460946  0.598104  0.375146  0.0001   \n",
              "25    0.169902         0.127426  0.464765  0.580917  0.389617  0.0001   \n",
              "\n",
              "    train_loss_epoch  epoch  step   timestamp  ...      hostname     node_ip  \\\n",
              "5           0.092170     49   550  1741822100  ...  dl-vladvin-1  10.128.0.3   \n",
              "8           0.077947     49  1050  1741822332  ...  dl-vladvin-1  10.128.0.3   \n",
              "28          0.095285     49   350  1741823882  ...  dl-vladvin-1  10.128.0.3   \n",
              "17          0.099747     49  1050  1741823031  ...  dl-vladvin-1  10.128.0.3   \n",
              "6           0.112690     49   350  1741822173  ...  dl-vladvin-1  10.128.0.3   \n",
              "19          0.077075     49   350  1741823186  ...  dl-vladvin-1  10.128.0.3   \n",
              "22          0.091107     49   350  1741823421  ...  dl-vladvin-1  10.128.0.3   \n",
              "12          0.129780     49   150  1741822634  ...  dl-vladvin-1  10.128.0.3   \n",
              "29          0.092790     49  1050  1741823966  ...  dl-vladvin-1  10.128.0.3   \n",
              "0           0.090692     49   200  1741821729  ...  dl-vladvin-1  10.128.0.3   \n",
              "2           0.128798     49   300  1741821878  ...  dl-vladvin-1  10.128.0.3   \n",
              "24          0.076417     49  1050  1741823579  ...  dl-vladvin-1  10.128.0.3   \n",
              "23          0.085354     49   550  1741823497  ...  dl-vladvin-1  10.128.0.3   \n",
              "20          0.090405     49   550  1741823264  ...  dl-vladvin-1  10.128.0.3   \n",
              "18          0.084255     49  1050  1741823113  ...  dl-vladvin-1  10.128.0.3   \n",
              "9           0.078577     49   350  1741822406  ...  dl-vladvin-1  10.128.0.3   \n",
              "13          0.122182     49  1050  1741822716  ...  dl-vladvin-1  10.128.0.3   \n",
              "15          0.179719     49   150  1741822870  ...  dl-vladvin-1  10.128.0.3   \n",
              "4           0.099124     49   550  1741822026  ...  dl-vladvin-1  10.128.0.3   \n",
              "10          0.122485     49   200  1741822480  ...  dl-vladvin-1  10.128.0.3   \n",
              "21          0.103626     49  1050  1741823346  ...  dl-vladvin-1  10.128.0.3   \n",
              "16          0.099836     49   550  1741822944  ...  dl-vladvin-1  10.128.0.3   \n",
              "11          0.128456     49   200  1741822556  ...  dl-vladvin-1  10.128.0.3   \n",
              "26          0.146676     49   250  1741823733  ...  dl-vladvin-1  10.128.0.3   \n",
              "7           0.137496     49   150  1741822249  ...  dl-vladvin-1  10.128.0.3   \n",
              "1           0.130427     49   550  1741821804  ...  dl-vladvin-1  10.128.0.3   \n",
              "3           0.141451     49   300  1741821952  ...  dl-vladvin-1  10.128.0.3   \n",
              "27          0.111356     49   550  1741823808  ...  dl-vladvin-1  10.128.0.3   \n",
              "14          0.198289     49   250  1741822793  ...  dl-vladvin-1  10.128.0.3   \n",
              "25          0.169902     49   300  1741823655  ...  dl-vladvin-1  10.128.0.3   \n",
              "\n",
              "    time_since_restore  iterations_since_restore  \\\n",
              "5            63.350655                        50   \n",
              "8            73.132622                        50   \n",
              "28           63.111884                        50   \n",
              "17           75.221304                        50   \n",
              "6            62.279430                        50   \n",
              "19           63.248566                        50   \n",
              "22           64.408439                        50   \n",
              "12           66.980362                        50   \n",
              "29           72.617448                        50   \n",
              "0            66.358428                        50   \n",
              "2            63.515885                        50   \n",
              "24           71.590104                        50   \n",
              "23           65.948513                        50   \n",
              "20           66.485056                        50   \n",
              "18           71.719339                        50   \n",
              "9            62.507959                        50   \n",
              "13           71.055779                        50   \n",
              "15           66.325545                        50   \n",
              "4            64.266496                        50   \n",
              "10           64.191614                        50   \n",
              "21           71.213969                        50   \n",
              "16           64.246681                        50   \n",
              "11           65.180606                        50   \n",
              "26           66.944036                        50   \n",
              "7            66.045544                        50   \n",
              "1            65.196376                        50   \n",
              "3            63.650551                        50   \n",
              "27           64.808590                        50   \n",
              "14           65.480602                        50   \n",
              "25           65.446462                        50   \n",
              "\n",
              "   config/train_loop_config/hidden_dim config/train_loop_config/num_layers  \\\n",
              "5                                  384                                   1   \n",
              "8                                 1024                                   2   \n",
              "28                                 128                                   2   \n",
              "17                                1024                                   2   \n",
              "6                                  256                                   2   \n",
              "19                                 512                                   2   \n",
              "22                                 640                                   2   \n",
              "12                                 384                                   2   \n",
              "29                                1024                                   1   \n",
              "0                                  896                                   2   \n",
              "2                                  128                                   1   \n",
              "24                                 768                                   1   \n",
              "23                                1024                                   1   \n",
              "20                                 768                                   2   \n",
              "18                                 384                                   2   \n",
              "9                                  512                                   1   \n",
              "13                                 640                                   1   \n",
              "15                                 384                                   2   \n",
              "4                                  896                                   1   \n",
              "10                                 512                                   1   \n",
              "21                                 384                                   2   \n",
              "16                                 256                                   1   \n",
              "11                                 640                                   1   \n",
              "26                                 512                                   2   \n",
              "7                                  896                                   1   \n",
              "1                                  768                                   2   \n",
              "3                                  256                                   1   \n",
              "27                                 384                                   1   \n",
              "14                                 128                                   1   \n",
              "25                                 256                                   2   \n",
              "\n",
              "    config/train_loop_config/batch_size  \\\n",
              "5                                    32   \n",
              "8                                    16   \n",
              "28                                   48   \n",
              "17                                   16   \n",
              "6                                    48   \n",
              "19                                   48   \n",
              "22                                   48   \n",
              "12                                  128   \n",
              "29                                   16   \n",
              "0                                    96   \n",
              "2                                    64   \n",
              "24                                   16   \n",
              "23                                   32   \n",
              "20                                   32   \n",
              "18                                   16   \n",
              "9                                    48   \n",
              "13                                   16   \n",
              "15                                  112   \n",
              "4                                    32   \n",
              "10                                   96   \n",
              "21                                   16   \n",
              "16                                   32   \n",
              "11                                   96   \n",
              "26                                   80   \n",
              "7                                   128   \n",
              "1                                    32   \n",
              "3                                    64   \n",
              "27                                   32   \n",
              "14                                   80   \n",
              "25                                   64   \n",
              "\n",
              "    config/train_loop_config/weight_decay  config/train_loop_config/dropout  \\\n",
              "5                                0.000810                          0.034136   \n",
              "8                                0.000121                          0.043038   \n",
              "28                               0.000020                          0.015735   \n",
              "17                               0.000154                          0.041954   \n",
              "6                                0.003525                          0.114586   \n",
              "19                               0.000430                          0.027994   \n",
              "22                               0.000031                          0.003652   \n",
              "12                               0.001676                          0.067281   \n",
              "29                               0.000209                          0.034451   \n",
              "0                                0.000028                          0.018611   \n",
              "2                                0.000013                          0.025711   \n",
              "24                               0.000988                          0.055379   \n",
              "23                               0.000283                          0.036214   \n",
              "20                               0.000155                          0.064471   \n",
              "18                               0.000061                          0.047795   \n",
              "9                                0.000419                          0.049347   \n",
              "13                               0.092593                          0.008781   \n",
              "15                               0.000012                          0.138805   \n",
              "4                                0.021232                          0.156513   \n",
              "10                               0.000570                          0.131638   \n",
              "21                               0.000913                          0.086560   \n",
              "16                               0.001959                          0.000715   \n",
              "11                               0.000079                          0.199139   \n",
              "26                               0.003619                          0.016692   \n",
              "7                                0.006943                          0.082633   \n",
              "1                                0.007773                          0.173402   \n",
              "3                                0.043976                          0.078642   \n",
              "27                               0.000042                          0.069335   \n",
              "14                               0.000169                          0.106793   \n",
              "25                               0.000068                          0.096551   \n",
              "\n",
              "      logdir  \n",
              "5   5a576c82  \n",
              "8   5f4597d7  \n",
              "28  0d3d2f70  \n",
              "17  995662df  \n",
              "6   08c9cf27  \n",
              "19  137d3681  \n",
              "22  ede8484b  \n",
              "12  8eaba676  \n",
              "29  f780394e  \n",
              "0   56fc16e6  \n",
              "2   bd4b22c9  \n",
              "24  1bb12d79  \n",
              "23  3cee4eec  \n",
              "20  70578d83  \n",
              "18  774d392e  \n",
              "9   481a02c2  \n",
              "13  fe376ce8  \n",
              "15  a1a7455a  \n",
              "4   e0d70199  \n",
              "10  d61f6592  \n",
              "21  5834913a  \n",
              "16  6d6626b0  \n",
              "11  d0f8f668  \n",
              "26  e1067dd5  \n",
              "7   599a7bc7  \n",
              "1   b2eb9de9  \n",
              "3   439b693d  \n",
              "27  d71d2247  \n",
              "14  5fa0d29c  \n",
              "25  7c654a8e  \n",
              "\n",
              "[30 rows x 29 columns]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# results of all trials\n",
        "result_df = results.get_dataframe()\n",
        "results_df = result_df.sort_values('val/mae')\n",
        "results_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Result(\n",
              "  metrics={'train_loss': 0.09216958284378052, 'train_loss_step': 0.092535600066185, 'val/mae': 0.38648226857185364, 'val/r2': 0.6692236661911011, 'val_loss': 0.31602737307548523, 'lr': 9.999999747378752e-05, 'train_loss_epoch': 0.09216958284378052, 'epoch': 49, 'step': 550},\n",
              "  path='/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5a576c82',\n",
              "  filesystem='local',\n",
              "  checkpoint=Checkpoint(filesystem=local, path=/home/jupyter/AgenticADMET/notebooks/../output/asap/rnd_splits/roberta/run_0/split_0/hpopt/ray_results/TorchTrainer_2025-03-12_23-20-54/5a576c82/checkpoint_000049)\n",
              ")"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results.get_best_result(metric=\"val/mae\", mode=\"min\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_df.to_csv(hpopt_save_dir / 'results.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "0cuYAo1XOqhJ"
      },
      "outputs": [],
      "source": [
        "ray.shutdown()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "admet",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
